diff --git a/docs/autopyfactory-factory.conf.5 b/docs/autopyfactory-factory.conf.5
deleted file mode 100644
index 295bf02..0000000
--- a/docs/autopyfactory-factory.conf.5
+++ /dev/null
@@ -1,179 +0,0 @@
-.\" Process this file with
- autopyfactory-factory.conf.5
-.\"
-.TH AUTOPYFACTORY FACTORY.CONF 5 "JUNE 2013" Linux "User Manuals"
-.SH NAME
-AutoPyFactory factory.conf \- factory.conf configuration file for autopyfactory
-.SH DESCRIPTION
-.B factory.conf
-
-/etc/apf/factory.conf  Configuration file for the factories of AutoPyFactory.
-
-.B baseLogDir
-
-
-where outputs from pilots are stored
-
-.I NOTE: No trailing '/'!!!
-
-
-
-.B baseLogDirUrl
-
-
-where outputs from pilots are available via http
-
-.I NOTE: No trailing '/'!!!
-
-
-
-.B baseLogHttpPort
-
-
-What port to run the HTTP server to export logs. Make sure this matchesthe value in the baseLogDirUrl.
-
-
-
-.B batchstatus.condor.sleep
-
-
-time the Condor BatchStatus Plugin waits between cycles Value is in seconds.
-
-
-
-.B batchstatus.maxtime
-
-
-maximum time while the info is considered reasonable. If info stored is older than that, is considered not valid, and some NULL output will be returned.
-
-
-
-.B cycles
-
-
-maximum number of times the queues will loop. None means forever.
-
-
-
-.B cleanlogs.keepdays
-
-
-maximum number of days the condor logs will be kept, in case they are placed in a subdirectory for an APFQueue that is not being currently managed by AutoPyFactory.  For example, an apfqueue that has been created and used for a short amount of time, and it does not exist anymore. Still the created logs have to be cleaned at some point...
-
-
-
-.B factoryId
-
-
-Name that the factory instance will have in the APF web monitor.  Make factoryId something descriptive and unique for your factory, for example <site>-<host>-<admin> (e.g. BNL-gridui11-jhover)
-
-
-.B factoryAdminEmail
-
-
-Email of the local admin to contact in case of a problem with an specific APF instance.
-
-
-.B factorySMTPServer  
-
-
-Server to use to send alert emails to admin. 
-
-
-.B factory.sleep
-
-
-sleep time between cycles in mainLoop in Factory object Value is in seconds.
-
-
-
-.B factoryUser
-
-
-account under which APF will run
-
-.B maxperfactory.maximum
-
-
-maximum number of condor jobs to be running at the same time per Factory.  It is a global number, used by all APFQueues submitting pilots with condor.  The value will be used by MaxPerFactorySchedPlugin plugin
-
-
-.B monitorURL
-
-
-URL for the web monitor
-
-
-.B logserver.enabled
-
-
-determines if batch logs are exported via HTTP.  Valid values are True|False
-
-
-.B logserver.index
-
-
-determines if automatic directory indexing is allowed when log directories are browsed.  Valid values are True|False
-
-
-.B logserver.allowrobots
-
-if false, creates a robots.txt file in the docroot.  Valid valudes are True|False
-
-
-.B proxyConf
-
-local path to the configuration file for automatic proxy management.  
-.I NOTE: must be a local path, not a URI.
-
-
-.B proxymanager.enabled
-
-to determine if automatic proxy management is used or not.  Accepted values are True|False
-
-.B proxymanager.sleep
-
-Sleep interval for proxymanager thread. 
-
-
-.B queueConf
-
-URI plus path to the configuration file for APF queues.
-
-.I NOTE: Must be expressed as a URI (file:// or http://)
-.I NOTE: Cannot be used at the same time that queueDirConf
-
-
-.B queueDirConf
-
-directory with a set of configuration files, all of them to be used at the same time. 
-i.e. /etc/apf/queues.d/
-
-.I NOTE: Cannot be used at the same time that queueConf
-
-
-
-.B monitorConf
-
-
-local path to the configuration file for Monitor plugins.
-
-
-.B versionTag
-
-
-APF version number as it will be displayed in the web monitor
-
-
-.B wmsstatus.maximum
-
-
-maximum time while the info is considered reasonable.  If info stored is older than that, is considered not valid, and some NULL output will be returned.
-
-
-
-.B wmsstatus.panda.sleep
-
-
-time the WMSStatus Plugin waits between cycles Value is in seconds.
-
diff --git a/docs/autopyfactory-monitor.conf.5 b/docs/autopyfactory-monitor.conf.5
deleted file mode 100644
index a9838e8..0000000
--- a/docs/autopyfactory-monitor.conf.5
+++ /dev/null
@@ -1,20 +0,0 @@
-.\" Process this file with
- autopyfactory-proxy.conf.5
-.\"
-.TH AUTOPYFACTORY MONITOR.CONF 5 "JUNE 2013" Linux "User Manuals"
-.SH NAME
-AutoPyFactory monitor.conf \- monitor.conf configuration file for autopyfactory
-.SH DESCRIPTION
-.B monitor.conf
-
-/etc/apf/monitor.conf  Configuration file for the monitor setup for AutoPyFactory.
-
-.B monitorplugin 
-
-the type of plugin to handle this monitor instance 
-
-.B monitorURL   
-
-URL for the web monitor 
-
-
diff --git a/docs/autopyfactory-proxy.conf.5 b/docs/autopyfactory-proxy.conf.5
deleted file mode 100644
index e35e149..0000000
--- a/docs/autopyfactory-proxy.conf.5
+++ /dev/null
@@ -1,151 +0,0 @@
-.\" Process this file with
- autopyfactory-proxy.conf.5
-.\"
-.TH AUTOPYFACTORY PROXY.CONF 5 "JUNE 2013" Linux "User Manuals"
-.SH NAME
-AutoPyFactory proxy.conf \- proxy.conf configuration file for autopyfactory
-.SH DESCRIPTION
-.B proxy.conf
-
-/etc/apf/proxy.conf  Configuration file for the proxies management for AutoPyFactory.
-
-.B baseproxy
-
-
-if used, create a very long-lived proxy, e.g.
-
-    grid-proxy-init -valid 720:0 -out /tmp/plainProxy
-
-Note that maintenance of this proxy must occur completely outside of APF. 
-
-
-
-.B proxyfile
-
-
-path to the user grid proxy file.
-
-
-
-.B checktime
-
-
-How often to check proxy validity, in seconds
-
-
-
-.B interruptcheck
-
-
-Frequency to check for keyboard/signal interrupts, in seconds
-
-
-
-.B lifetime
-
-
-initial voms lifetime, in seconds (604800 = 7 days).  345600 is ATLAS VOMS maximum
-
-
-
-.B minlife
-
-
-Minimum lifetime of proxy (renew if less) in seconds
-
-
-
-.B renew
-
-
-If you do not want to use ProxyManager to renew proxies, set this  False and only define 'proxyfile'.  If renew is set to false, then no grid client setup is necessary. 
-
-
-
-.B usercert
-
-
-path to the user grid certificate file
-
-
-
-.B userkey
-
-
-path to the user grid key file
-
-
-
-.B vorole
-
-
-user VO role
-
-
-.B flavor
-
-
-voms or myproxy. voms directly generates proxy using cert or baseproxy myproxy retrieves a proxy from myproxy, then generates the target proxy against voms using it as baseproxy.
-
-
-.B myproxy_hostname
-
-
-Myproxy server host.
-
-
-.B myproxy_username
-
-
-User name to be used on MyProxy service
-
-
-.B myproxy_passphrase
-
-
-Passphrase for proxy retrieval from MyProxy
-
-
-.B retriever_profile
-
-
-A list of other proxymanager profiles to be used to authorize proxy retrieval from MyProxy.
-
-
-.B initdelay
-
-
-In seconds, how long to wait before generating. Needed for MyProxy when using cert authentication--we need to allow time for the auth credential to be generated (by another proxymanager profile).
-
-
-.B owner
-
-
-If running standalone (as root) and you want the proxy to be owned by another account.
-
-
-.B remote_host
-
-
-If defined, copy proxyfile to same path on remote host
-
-
-.B remote_user
-
-
-User to connect as?
-
-
-.B remote_owner
-
-
-If connect user is root, what account should own the file?
-
-
-.B remote_group
-
-
-If connect user is root, what group should own the file?
-
-
-
diff --git a/docs/autopyfactory-queues.conf.5 b/docs/autopyfactory-queues.conf.5
deleted file mode 100644
index 8b2990b..0000000
--- a/docs/autopyfactory-queues.conf.5
+++ /dev/null
@@ -1,1153 +0,0 @@
-.\" Process this file with
- autopyfactory-queues.conf.5
-.\"
-.TH AUTOPYFACTORY QUEUES.CONF 5 "JUNE 2013" Linux "User Manuals"
-.SH NAME
-AutoPyFactory quues.conf \- queues.conf configuration file for autopyfactory 
-.SH DESCRIPTION
-.B queues.conf
-
-
-/etc/apf/queues.conf  Configuration file for APFQueue component of AutoPyFactory.
-
-Defaults for queues - these values are set when there is not an explicit value
-If you don't set them here the factory takes sensible default values, so nothing is mandatory
-see ConfigLoader._configurationDefaults() for these values. 
-
-Some of these values may be in the process of deprecation, especially submission parameters 
-which are now handled by the submit plugins. 
-
-.SH GENERIC VARIABLES
-
-.B override
-
-determines if values from this config file have precedence over
-the same values comming from different sources of information.
-If True then schedconfig does not clobber configuration file values.
-Valid values are True|False.
-
-
-.B cloud
-
-is the cloud this queue is in. You should set this to suppress pilot
-submission when the cloud goes offline
-N.B. Panda clouds are UPPER CASE, e.g., UK
-
-
-.B vo
-
-Virtual Organization
-
-
-.B grid
-
-Grid middleware flavor at the site. (e.g. OSG, EGI, NorduGrid)
-
-
-.B batchqueue
-
-the Batch system related queue name.
-E.g. the PanDA queue name (formerly called nickname)
-
-
-.B wmsqueue
-
-the WMS system queue name.
-E.g. the PanDA siteid name
-
-
-.B enabled
-
-determines if each queue section must be used by AutoPyFactory
-or not. Allows to disable a queue without commenting out all the values. 
-Valid values are True|False.
-
-
-.B status
-
-can be "test", "offline" or "online"
-
-
-.B apfqueue.sleep
-
-sleep time between cycles in APFQueue object.
-Value is in seconds.   
-
-
-.B autofill
-
-says if the info from this filled should be completed
-with info from a ConfigPlugin object
-
-
-.B cleanlogs.keepdays
-
-maximum number of days the condor logs
-will be kept
-
-
-
-.SH WMS STATUS PLUGIN VARIABLES
-
-
-.B wmsstatusplugin
-
-WMS Status Plugin.
-
-
-
-.SH CONFIG PLUGIN VARIABLES
-
-
-.B configplugin
-
-Config Plugin.
-Optional.
-E.g. Panda.
-
-
-
-.SH BATCH STATUS PLUGIN VARIABLES
-
-
-.B batchstatusplugin
-
-Batch Status Plugin.
-
-
-.B batchstatus.condor.queryargs
-
-list of command line input options
-to be included in the query command *verbatim*. E.g. 
-batchstatus.condor.query = -name <schedd_name> -pool <centralmanagerhostname[:portnumber]>
-
-
-
-.SH SCHED PLUGIN VARIABLES
-
-
-.B schedplugin
-
-specific Scheduler Plugin implementing
-the algorithm deciding how many new pilots
-to submit next cycle.
-The value can be a single Plugin or a split by comma
-list of Plugins.
-In the case of more than one plugin, 
-each one will acts as a filter with respect to the
-value returned by the previous one.
-By selecting the right combination of Plugins in a given order,
-a complex algorithm can be built.
-E.g., the algorithm can start by using Activated Plugin,
-which will determine the number of pilots based on 
-the number of activated jobs in the WMS queue and 
-the number of already submitted pilots.
-After that, this number can be filtered to 
-a maximum (MaxPerCycleSchedPlugin) or a minimum (MinPerCycleSchedPlugin)
-number of pilots.
-Or even can be filtered to a maximum number of pilots
-per factory (MaxPerFactorySchedPlugin)
-Also it can be filtered depending on the status of the wmsqueue 
-(StatusTestSchedPlugin, StatusOfflineSchedPlugin).
-
-
-
-.SH CONFIGURATION WHEN SCHEDPLUGIN IS ACTIVATED
-
-
-.I IMPORTANT NOTE: Deprecated. Activated Plugin is not maintained anymore. Instead, suggested option is to use Ready Plugin plus a chain of limiting plugins (MaxPerCycle, MinPerCycle...)
-
-.B sched.activated.default
-
-default number of pilots to be submitted
-when the context information 
-does not exist is not reliable 
-To be used in Activated Scheduler Plugin.
-
-
-.B sched.activated.max_jobs_torun
-
-maximum number of jobs running
-simoultaneously. 
-To be used in Activated Scheduler Plugin.
-
-
-.B sched.activated.max_pilots_per_cycle
-
-maximum number of pilots
-to be submitted per cycle.
-To be used in Activated Scheduler Plugin.
-
-
-.B sched.activated.min_pilots_per_cycle
-
-minimum number of pilots
-to be submitted per cycle.
-To be used in Activated Scheduler Plugin.
-
-
-.B sched.activated.min_pilots_pending
-
-minimum number of pilots
-to be idle on queue waiting to start execution.
-To be used in Activated Scheduler Plugin.
-
-
-.B sched.activated.max_pilots_pending
-
-maximum number of pilots
-to be idle on queue waiting to start execution.
-To be used in Activated Scheduler Plugin.
-
-
-.B sched.activated.testmode.allowed
-
-Boolean variable to trigger
-special mode of operation when the wmsqueue is in
-in status = test
-
-
-.B sched.activated.testmode.pilots
-
-number of pilots to submit
-when the wmsqueue is in status = test
-and sched.activated.testmode.allowed is True
-
-
-
-.SH CONFIGURATION WHEN SCHEDPLUGIN IS READY
-
-
-.B sched.ready.offset
-
-
-the minimum value in the number of ready jobs to trigger submission.
-
-
-
-.SH CONFIGURATION WHEN SCHEDPLUGIN IS FIXED
-
-
-.B sched.fixed.pilotspercycle
-
-fixed number of pilots to be submitted
-each cycle, when using the Fixed Scheduler Plugin.
-
-
-
-.SH CONFIGURATION WHEN SCHEDPLUGIN IS MAXPERCYCLE
-
-
-.B sched.maxpercycle.maximum
-
-maximum number of pilots to be submitted
-per cycle
-
-
-
-.SH CONFIGURATION WHEN SCHEDPLUGIN IS MINPERCYCLE
-
-
-.B sched.minpercycle.minimum
-
-minimum number of pilots to be submitted
-per cycle
-
-
-
-.SH CONFIGURATION WHEN SCHEDPLUGIN IS MAXPENDING
-
-
-.B sched.maxpending.maximum
-
-maximum number of pilots to be pending
-
-
-
-.SH CONFIGURATION WHEN SCHEDPLUGIN IS MINPENDING
-
-
-.B sched.minpending.minimum
-
-minimum number of pilots to be pending
-
-
-
-.SH CONFIGURATION WHEN SCHEDPLUGIN IS MAXTORUN
-
-
-.B sched.maxtorun.maximum
-
-maximum number of pilots allowed to, potentially,
-be running at a time. 
-
-
-
-.SH CONFIGURATION WHEN SCHEDPLUGIN IS STATUSTEST
-
-
-.B sched.statustest.pilots
-
-number of pilots to submit
-when the wmsqueue is in status = test
-
-
-.SH CONFIGURATION WHEN SCHEDPLUGIN IS STATUSOFFLINE
-
-
-.B sched.statusoffline.pilots
-
-number of pilots to submit
-when the wmsqueue or the cloud is in status = offline
-
-
-.SH CONFIGURATION WHEN SCHEDPLUGIN IS SIMPLE
-
-
-.B sched.simple.default
-
-default number of pilots to be submitted
-when the context information does not exist
-or is not reliable.
-To be used in Simple Scheduler Plugin.
-
-
-.B sched.simple.maxpendingpilots
-
-maximum number of pilots
-to be idle on queue waiting to start execution.
-To be used in Simple Scheduler Plugin.
-
-
-.B sched.simple.maxpilotspercycle
-
-maximum number of pilots
-to be submitted per cycle.
-To be used in Simple Scheduler Plugin.
-
-
-.SH CONFIGURATION WHEN SCHEDPLUGIN IS TRIVIAL
-
-
-.B sched.trivial.default
-
-default number of pilots
-to be submitted when the context information
-does not exist or is not reliable.
-To be used in Trivial Scheduler Plugin.
-
-
-
-.SH CONFIGURATION WHEN SCHEDPLUGIN IS SCALE
-
-
-.B sched.scale.factor
-
-scale factor to correct the previous value
-of the number of pilots.
-
-Value is a float number.
-
-
-
-.SH CONFIGURATION WHEN SCHEDPLUGIN IS KEEPNRUNNING
-
-
-.B sched.keepnrunning.keep_running
-
-number of total jobs to keep running and/or pending.
-
-
-
-
-.SH BATCH SUBMIT PLUGIN VARIABLES
-
-
-.B batchsubmitplugin
-
-Batch Submit Plugin.
-Currently available options are: 
-     CondorGT2, 
-     CondorGT5, 
-     CondorCREAM, 
-     CondorLocal, 
-     CondorEC2, 
-     CondorDeltaCloud.
-
-
-
-.SH CONFIGURATION WHEN BATCHSUBMITPLUGIN IS CONDORGT2
-
-
-.B batchsubmit.condorgt2.gridresource
-
-name of the CE (e.g. gridtest01.racf.bnl.gov/jobmanager-condor)
-
-
-.B batchsubmit.condorgt2.submitargs
-
-list of command line input options
-to be included in the submission command *verbatim*
-e.g. 
-    batchsubmit.condorgt2.submitargs = -remote my_schedd 
-will drive into a command like
-    condor_submit -remote my_schedd submit.jdl
-
-
-.B batchsubmit.condorgt2.condor_attributes
-
-list of condor attributes,
-splited by comma, 
-to be included in the condor submit file *verbatim*
-e.g. +Experiment = "ATLAS",+VO = "usatlas",+Job_Type = "cas"
-Can be used to include any line in the Condor-G file
-that is not otherwise added programmatically by AutoPyFactory.
-Note the following directives are added by default:
-
-        transfer_executable = True
-        stream_output=False
-        stream_error=False
-        notification=Error
-        copy_to_spool = false
-
-
-.B batchsubmit.condorgt2.environ
-
-list of environment variables,
-splitted by white spaces, 
-to be included in the condor attribute environment *verbatim*
-Therefore, the format should be env1=var1 env2=var2 envN=varN
-split by whitespaces.
-
-
-.B batchsubmit.condorgt2.proxy
-
-name of the proxy handler in proxymanager for automatic proxy renewal
-(See etc/proxy.conf)
-None if no automatic proxy renewal is desired.
-
-
-
-.SH GLOBUSRSL GRAM2 VARIABLES
-
-
-.B gram2
-
-The following are GRAM2 RSL variables.
-They are just used to build batchsubmit.condorgt2.globusrsl 
-(if needed)
-The globusrsl directive in the condor submission file looks like
-
-    globusrsl=(jobtype=single)(queue=short)
-
-Documentation can be found here:
-
-        http://www.globus.org/toolkit/docs/2.4/gram/gram_rsl_parameters.html
-
-
-.B globusrsl.gram2.arguments
-
-
-
-.B globusrsl.gram2.count
-
-
-
-.B globusrsl.gram2.directory
-
-
-
-.B globusrsl.gram2.dryRun
-
-
-
-.B globusrsl.gram2.environment
-
-
-
-.B globusrsl.gram2.executable
-
-
-
-.B globusrsl.gram2.gramMyJob
-
-
-
-.B globusrsl.gram2.hostCount
-
-
-
-.B globusrsl.gram2.jobType
-
-
-
-.B globusrsl.gram2.maxCpuTime
-
-
-
-.B globusrsl.gram2.maxMemory
-
-
-
-.B globusrsl.gram2.maxTime
-
-
-
-.B globusrsl.gram2.maxWallTime
-
-
-
-.B globusrsl.gram2.minMemory
-
-
-
-.B globusrsl.gram2.project
-
-
-
-.B globusrsl.gram2.queue
-
-
-
-.B globusrsl.gram2.remote_io_url
-
-
-
-.B globusrsl.gram2.restart
-
-
-
-.B globusrsl.gram2.save_state
-
-
-
-.B globusrsl.gram2.stderr
-
-
-
-.B globusrsl.gram2.stderr_position
-
-
-
-.B globusrsl.gram2.stdin
-
-
-
-.B globusrsl.gram2.stdout
-
-
-
-.B globusrsl.gram2.stdout_position
-
-
-
-.B globusrsl.gram2.two_phase
-
-
-
-.B globusrsl.gram2.globusrsl
-
-GRAM RSL directive.
-If this variable is not setup, then it will be built
-programmatically from all non empty globusrsl.gram2.XYZ variables.
-If this variable is setup, then its value
-will be taken *verbatim*, and all possible values
-for globusrsl.gram2.XYZ variables will be ignored. 
-
-
-.B globusrsl.gram2.globusrsladd
-
-custom fields to be added
-*verbatim* to the GRAM RSL directive,
-after it has been built either from 
-globusrsl.gram2.globusrsl value
-or from all globusrsl.gram2.XYZ variables.
-e.g. (condorsubmit=('+AccountingGroup' '\"group_atlastest.usatlas1\"')('+Requirements' 'True'))
-
-
-
-.SH CONFIGURATION WHEN BATCHSUBMITPLUGIN IS CONDORGT5
-
-
-.B batchsubmit.condorgt5.gridresource
-
-name of the CE (e.g. gridtest01.racf.bnl.gov/jobmanager-condor)
-
-
-.B batchsubmit.condorgt5.submitargs
-
-list of command line input options
-to be included in the submission command *verbatim*
-e.g. 
-    batchsubmit.condorgt2.submitargs = -remote my_schedd 
-will drive into a command like
-    condor_submit -remote my_schedd submit.jdl
-
-
-.B batchsubmit.condorgt5.condor_attributes
-
-list of condor attributes,
-splited by comma, 
-to be included in the condor submit file *verbatim*
-e.g. +Experiment = "ATLAS",+VO = "usatlas",+Job_Type = "cas"
-Can be used to include any line in the Condor-G file
-that is not otherwise added programmatically by AutoPyFactory.
-Note the following directives are added by default:
-
-        transfer_executable = True
-        stream_output=False
-        stream_error=False
-        notification=Error
-        copy_to_spool = false
-
-
-.B batchsubmit.condorgt5.environ
-
-list of environment variables,
-splitted by white spaces, 
-to be included in the condor attribute environment *verbatim*
-Therefore, the format should be env1=var1 env2=var2 envN=varN
-split by whitespaces.
-
-
-.B batchsubmit.condorgt5.proxy
-
-name of the proxy handler in proxymanager for automatic proxy renewal
-(See etc/proxy.conf)
-None if no automatic proxy renewal is desired.
-
-
-
-.SH GLOBUSRSL GRAM5 VARIABLES
-
-
-.B gram5
-
-The following are GRAM5 RSL variables.
-They are just used to build batchsubmit.condorgt5.globusrsl 
-(if needed)
-The globusrsl directive in the condor submission file looks like
-
-    globusrsl=(jobtype=single)(queue=short)
-
-Documentation can be found here:
-
-       http://www.globus.org/toolkit/docs/5.2/5.2.0/gram5/user/#gram5-user-rsl 
-
-
-.B globusrsl.gram5.arguments
-
-
-
-.B globusrsl.gram5.count
-
-
-
-.B globusrsl.gram5.directory
-
-
-
-.B globusrsl.gram5.dry_run
-
-
-
-.B globusrsl.gram5.environment
-
-
-
-.B globusrsl.gram5.executable
-
-
-
-.B globusrsl.gram5.file_clean_up
-
-
-
-.B globusrsl.gram5.file_stage_in
-
-
-
-.B globusrsl.gram5.file_stage_in_shared
-
-
-
-.B globusrsl.gram5.file_stage_out
-
-
-
-.B globusrsl.gram5.gass_cache
-
-
-
-.B globusrsl.gram5.gram_my_job
-
-
-
-.B globusrsl.gram5.host_count
-
-
-
-.B globusrsl.gram5.job_type
-
-
-
-.B globusrsl.gram5.library_path
-
-
-
-.B globusrsl.gram5.loglevel
-
-
-
-.B globusrsl.gram5.logpattern
-
-
-
-.B globusrsl.gram5.max_cpu_time
-
-
-
-.B globusrsl.gram5.max_memory
-
-
-
-.B globusrsl.gram5.max_time
-
-
-
-.B globusrsl.gram5.max_wall_time
-
-
-
-.B globusrsl.gram5.min_memory
-
-
-
-.B globusrsl.gram5.project
-
-
-
-.B globusrsl.gram5.proxy_timeout
-
-
-
-.B globusrsl.gram5.queue
-
-
-
-.B globusrsl.gram5.remote_io_url
-
-
-
-.B globusrsl.gram5.restart
-
-
-
-.B globusrsl.gram5.rsl_substitution
-
-
-
-.B globusrsl.gram5.savejobdescription
-
-
-
-.B globusrsl.gram5.save_state
-
-
-
-.B globusrsl.gram5.scratch_dir
-
-
-
-.B globusrsl.gram5.stderr
-
-
-
-.B globusrsl.gram5.stderr_position
-
-
-
-.B globusrsl.gram5.stdin
-
-
-
-.B globusrsl.gram5.stdout
-
-
-
-.B globusrsl.gram5.stdout_position
-
-
-
-.B globusrsl.gram5.two_phase
-
-
-
-.B globusrsl.gram5.username
-
-
-
-
-.B globusrsl.gram5.globusrsl
-
-GRAM RSL directive.
-If this variable is not setup, then it will be built
-programmatically from all non empty globusrsl.gram5.XYZ variables.
-If this variable is setup, then its value
-will be taken *verbatim*, and all possible values
-for globusrsl.gram5.XYZ variables will be ignored. 
-
-
-.B globusrsl.gram5.globusrsladd
-
-custom fields to be added
-*verbatim* to the GRAM RSL directive,
-after it has been built either from 
-globusrsl.gram5.globusrsl value
-or from all globusrsl.gram5.XYZ variables.
-e.g. (condorsubmit=('+AccountingGroup' '\"group_atlastest.usatlas1\"')('+Requirements' 'True'))
-
-
-
-.SH CONFIGURATION WHEN BATCHSUBMITPLUGIN IS CONDORCREAM
-
-
-.B batchsubmit.condorcream.webservice
-
-web service address (e.g. ce04.esc.qmul.ac.uk:8443/ce-cream/services/CREAM2)
-
-
-.B batchsubmit.condorcream.submitargs
-
-list of command line input options
-to be included in the submission command *verbatim*
-e.g. 
-    batchsubmit.condorgt2.submitargs = -remote my_schedd 
-will drive into a command like
-    condor_submit -remote my_schedd submit.jdl
-
-
-.B batchsubmit.condorcream.condor_attributes
-
-list of condor attributes,
-splited by comma, 
-to be included in the condor submit file *verbatim*
-e.g. +Experiment = "ATLAS",+VO = "usatlas",+Job_Type = "cas"
-Can be used to include any line in the Condor-G file
-that is not otherwise added programmatically by AutoPyFactory.
-Note the following directives are added by default:
-
-        transfer_executable = True
-        stream_output=False
-        stream_error=False
-        notification=Error
-        copy_to_spool = false
-
-
-.B batchsubmit.condorcream.environ
-
-list of environment variables,
-splitted by white spaces, 
-to be included in the condor attribute environment *verbatim*
-Therefore, the format should be env1=var1 env2=var2 envN=varN
-split by whitespaces.
-
-
-.B batchsubmit.condorcream.queue
-
-queue within the local batch system (e.g. short)
-
-
-.B batchsubmit.condorcream.port
-
-port number.
-
-
-.B batchsubmit.condorcream.batch
-
-local batch system (pbs, sge...)
-
-
-.B batchsubmit.condorcream.gridresource
-
-grid resource, built from other vars using interpolation:
-batchsubmit.condorcream.gridresource = %(batchsubmit.condorcream.webservice)s:%(batchsubmit.condorcream.port)s/ce-cream/services/CREAM2 %(batchsubmit.condorcream.batch)s %(batchsubmit.condorcream.queue)s
-
-
-.B batchsubmit.condorcream.proxy
-
-name of the proxy handler in proxymanager for automatic proxy renewal
-(See etc/proxy.conf)
-None if no automatic proxy renewal is desired.
-
-
-
-.SH CONFIGURATION WHEN BATCHSUBMITPLUGIN IS CONDOROSGCE
-
-
-.B batchsubmit.condorosgce.remote_condor_schedd
-
-condor schedd
-
-
-.B batchsubmit.condorosgce.remote_condor_collector
-
-condor collector
-
-
-.B batchsubmit.condorosgce.gridresource
-
-grid resource, built from other vars using interpolation
-batchsubmit.condorosgce.gridresource = %(batchsubmit.condorosgce.remote_condor_schedd) %(batchsubmit.condorosgce.remote_condor_collector)    
-
-.B batchsubmit.condorosgce.proxy
-
-name of the proxy handler in proxymanager for automatic proxy renewal
-(See etc/proxy.conf)
-None if no automatic proxy renewal is desired.
-
-
-
-.SH CONFIGURATION WHEN BATCHSUBMITPLUGIN IS CONDOREC2
-
-
-.B batchsubmit.condorec2.gridresource
-
-ec2 service's URL (e.g. https://ec2.amazonaws.com/ )
-
-
-.B batchsubmit.condorec2.submitargs
-
-list of command line input options
-to be included in the submission command *verbatim*
-e.g. 
-    batchsubmit.condorgt2.submitargs = -remote my_schedd 
-will drive into a command like
-    condor_submit -remote my_schedd submit.jdl
-
-
-.B batchsubmit.condorec2.condor_attributes
-
-list of condor attributes,
-splited by comma, 
-to be included in the condor submit file *verbatim*
-
-
-.B batchsubmit.condorec2.environ
-
-list of environment variables,
-splitted by white spaces, 
-to be included in the condor attribute environment *verbatim*
-Therefore, the format should be env1=var1 env2=var2 envN=varN
-split by whitespaces.
-
-
-.B batchsubmit.condorec2.ami_id
-
-identifier for the VM image,
-previously registered in one of Amazon's storage service (S3 or EBS)
-
-
-.B batchsubmit.condorec2.instance_type
-
-hardware configurations for instances to run on.
-
-
-.B batchsubmit.condorec2.user_data
-
-up to 16Kbytes of contextualization data.
-This makes it easy for many instances to share the same VM image, but perform different work.
-
-
-.B batchsubmit.condorec2.access_key_id
-
-path to file with the EC2 Access Key ID
-
-
-.B batchsubmit.condorec2.secret_access_key
-
-path to file with the EC2 Secret Access Key
-
-
-.B batchsubmit.condorec2.proxy
-
-name of the proxy handler in proxymanager for automatic proxy renewal
-(See etc/proxy.conf)
-None if no automatic proxy renewal is desired.
-
-
-
-.SH CONFIGURATION WHEN BATCHSUBMITPLUGIN IS CONDORDELTACLOUD
-
-
-.B batchsubmit.condordeltacloud.gridresource
-
-ec2 service's URL (e.g. https://deltacloud.foo.org/api )
-
-
-.B batchsubmit.condordeltacloud.username
-
-credentials in DeltaCloud
-
-
-.B batchsubmit.condordeltacloud.password_file
-
-path to the file with the password
-
-
-.B batchsubmit.condordeltacloud.image_id
-
-identifier for the VM image,
-previously registered with the cloud service.
-
-
-.B batchsubmit.condordeltacloud.keyname
-
-in case of using SSH,
-the command keyname specifies the identifier of the SSH key pair to use. 
-
-
-.B batchsubmit.condordeltacloud.realm_id
-
-selects one between multiple locations the cloud service may have.
-
-
-.B batchsubmit.condordeltacloud.hardware_profile
-
-selects one between the multiple hardware profiles
-the cloud service may provide
-
-
-.B batchsubmit.condordeltacloud.hardware_profile_memory
-
-customize the hardware profile
-
-
-.B batchsubmit.condordeltacloud.hardware_profile_cpu
-
-customize the hardware profile
-
-
-.B batchsubmit.condordeltacloud.hardware_profile_storage
-
-customize the hardware profile
-
-
-.B batchsubmit.condordeltacloud.user_data
-
-contextualization data
-
-
-
-.SH CONFIGURATION WHEN BATCHSUBMITPLUGIN IS CONDORLOCAL
-
-
-.B batchsubmit.condorlocal.submitargs
-
-list of command line input options
-to be included in the submission command *verbatim*
-e.g. 
-    batchsubmit.condorgt2.submitargs = -remote my_schedd 
-will drive into a command like
-    condor_submit -remote my_schedd submit.jdl
-
-
-.B batchsubmit.condorlocal.condor_attributes
-
-list of condor attributes,
-splited by comma, 
-to be included in the condor submit file *verbatim*
-e.g. +Experiment = "ATLAS",+VO = "usatlas",+Job_Type = "cas"
-Can be used to include any line in the Condor-G file
-that is not otherwise added programmatically by AutoPyFactory.
-Note the following directives are added by default:
-
-        universe = vanilla
-        transfer_executable = True
-        should_transfer_files = IF_NEEDED
-        +TransferOutput = ""
-        stream_output=False
-        stream_error=False
-        notification=Error
-        periodic_remove = (JobStatus == 5 && (CurrentTime - EnteredCurrentStatus) > 3600) || (JobStatus == 1 && globusstatus =!= 1 && (CurrentTime - EnteredCurrentStatus) > 86400)
-
-To be used in CondorLocal Batch Submit Plugin.
-
-
-.B batchsubmit.condorlocal.environ
-
-list of environment variables,
-splitted by white spaces, 
-to be included in the condor attribute environment *verbatim*
-To be used by CondorLocal Batch Submit Plugin.
-Therefore, the format should be env1=var1 env2=var2 envN=varN
-split by whitespaces.
-
-
-.B batchsubmit.condorlocal.proxy
-
-name of the proxy handler in proxymanager for automatic proxy renewal
-(See etc/proxy.conf)
-None if no automatic proxy renewal is desired.
-
-
-
-
-.SH MONITOR SECTION
-
-
-.B monitorsection
-
-
-section in monitor.conf where info 
-about the actual monitor plugin can be found.
-The value can be a single section or a split by comma
-list of sections.
-Monitor plugins handle job info publishing 
-to one or more web monitor/dashboards. 
-To specify more than one (sections) 
-simply use a comma-separated list.   
-
-
-
-.SH EXECUTABLE VARIABLES
-
-
-.B executable
-
-path to the script which will be run by condor.
-The executable can be anything, however, 
-two possible executables are distributed with AutoPyFactory:
-
-        - libexec/wrapper.sh 
-        - libexec/runpilot3-wrapper.sh 
-
-
-.B executable.arguments
-
-input options to be passed verbatim to the executable script.
-This variable can be built making use of an auxiliar variable
-called executable.defaultarguments
-This proposed ancilla works as a template, and its content is
-created on the fly from the value of other variables.
-This mechanism is called "interpolation", docs can be found here:
-
-    http://docs.python.org/library/configparser.html
-
-These are two examples of this type of templates 
-(included in the DEFAULTS block):
-
-    executable.defaultarguments = --wrappergrid=%(grid)s
-                --wrapperwmsqueue=%(wmsqueue)s
-                --wrapperbatchqueue=%(batchqueue)s
-                --wrappervo=%(vo)s
-                --wrappertarballurl=http://dev.racf.bnl.gov/dist/wrapper/wrapper.tar.gz
-                --wrapperserverurl=http://pandaserver.cern.ch:25080/cache/pilot
-                --wrapperloglevel=debug
-
-    executable.defaultarguments =  -s %(wmsqueue)s
-                -h %(batchqueue)s -p 25443
-                -w https://pandaserver.cern.ch  -j false  -k 0  -u user
-
-
-
diff --git a/docs/autopyfactory.1 b/docs/autopyfactory.1
deleted file mode 100644
index 47d845e..0000000
--- a/docs/autopyfactory.1
+++ /dev/null
@@ -1,71 +0,0 @@
-.\" Process this file with
- autopyfactory
-.\"
-.TH AUTOPYFACTORY 1 "JUNE 2013" Linux "User Manuals"
-.SH NAME
-AutoPyFactory \- grid pilot factory 
-.SH SYNOPSIS
-.B /etc/init.d/factory
-.SH DESCRIPTION
-.B autopyfactory  
-
-ATLAS, one of the experiments at LHC at CERN, is one of the largest  users of grid computing infrastructure.
-As this infrastructure is now a central part of the experiment's computing operations,
-considerable efforts have been made to use this technology in the most efficient and effective way, including extensive use of pilot job based frameworks
-
-In this model the experiment submits 'pilot' jobs to sites without  payload. When these jobs begin to run they contact a central service  to pick-up a real payload to execute.
-
-The first generation of pilot factories were usually specific to a single VO, and were very bound to the particular architecture of that VO.
-A second generation is creating factories which are more flexible, not tied to any particular VO,
-and provide for more features other than just pilot submission (such as monitoring, logging, profiling, etc.)
-
-AutoPyFactory has a modular design and is highly configurable. It is able to send different types of pilots to sites, able to exploit
-different submission mechanisms and different charateristics of queues at sites.
-It has excellent integration with the PanDA job submission framework,
-tying pilot flows closely to the amount of work the site has to run.
-It is able to gather information from many sources, in order to correctly conigure itself for a site and its decision logic can easily be updated.
-
-Integrated into AutoPyFactory is a very flexible system for delivering both
-generic and specific wrappers which can perform many useful actions before starting to run end-user scientific applications,
-e.g., validation of the middleware, node profiling and diagnostics, monitoring and deciding what is the best end-user application that fits the resource.
-
-AutoPyFactory now also has a robust monitoring system and we show how this has helped setup a reliable pilot factory service for ATLAS.
-
-
-
-.SH FILES
-.I /etc/apf/queues.conf
-.RS
-The queues configuration file. See
-.BR queues (5)
-for further details.
-.RE
-.I /etc/apf/factory.conf
-.RS
-The main system configuration file. See
-.BR factory (5)
-for further details.
-.RE
-.I /etc/apf/proxy.conf
-.RS
-The proxy configuration file. See
-.BR proxy (5)
-for further details.
-.RE
-.I /etc/apf/monitor.conf
-.RS
-The monitor configuration file. See
-.BR monitor (5)
-for further details.
-.RE
-
-
-.SH AUTHOR
-Jose Caballero <jcaballero at bnl dot gov> and John Hover <jhover at bnl dot gov>
-
-.\".SH "SEE ALSO"
-.\".BR bar (1),
-.\".BR foo (5),
-.\".BR xyzzy (1)
-
-.\" groff -man -Tasc
diff --git a/docs/man/autopyfactory-factory.conf.5 b/docs/man/autopyfactory-factory.conf.5
new file mode 100644
index 0000000..295bf02
--- /dev/null
+++ b/docs/man/autopyfactory-factory.conf.5
@@ -0,0 +1,179 @@
+.\" Process this file with
+ autopyfactory-factory.conf.5
+.\"
+.TH AUTOPYFACTORY FACTORY.CONF 5 "JUNE 2013" Linux "User Manuals"
+.SH NAME
+AutoPyFactory factory.conf \- factory.conf configuration file for autopyfactory
+.SH DESCRIPTION
+.B factory.conf
+
+/etc/apf/factory.conf  Configuration file for the factories of AutoPyFactory.
+
+.B baseLogDir
+
+
+where outputs from pilots are stored
+
+.I NOTE: No trailing '/'!!!
+
+
+
+.B baseLogDirUrl
+
+
+where outputs from pilots are available via http
+
+.I NOTE: No trailing '/'!!!
+
+
+
+.B baseLogHttpPort
+
+
+What port to run the HTTP server to export logs. Make sure this matchesthe value in the baseLogDirUrl.
+
+
+
+.B batchstatus.condor.sleep
+
+
+time the Condor BatchStatus Plugin waits between cycles Value is in seconds.
+
+
+
+.B batchstatus.maxtime
+
+
+maximum time while the info is considered reasonable. If info stored is older than that, is considered not valid, and some NULL output will be returned.
+
+
+
+.B cycles
+
+
+maximum number of times the queues will loop. None means forever.
+
+
+
+.B cleanlogs.keepdays
+
+
+maximum number of days the condor logs will be kept, in case they are placed in a subdirectory for an APFQueue that is not being currently managed by AutoPyFactory.  For example, an apfqueue that has been created and used for a short amount of time, and it does not exist anymore. Still the created logs have to be cleaned at some point...
+
+
+
+.B factoryId
+
+
+Name that the factory instance will have in the APF web monitor.  Make factoryId something descriptive and unique for your factory, for example <site>-<host>-<admin> (e.g. BNL-gridui11-jhover)
+
+
+.B factoryAdminEmail
+
+
+Email of the local admin to contact in case of a problem with an specific APF instance.
+
+
+.B factorySMTPServer  
+
+
+Server to use to send alert emails to admin. 
+
+
+.B factory.sleep
+
+
+sleep time between cycles in mainLoop in Factory object Value is in seconds.
+
+
+
+.B factoryUser
+
+
+account under which APF will run
+
+.B maxperfactory.maximum
+
+
+maximum number of condor jobs to be running at the same time per Factory.  It is a global number, used by all APFQueues submitting pilots with condor.  The value will be used by MaxPerFactorySchedPlugin plugin
+
+
+.B monitorURL
+
+
+URL for the web monitor
+
+
+.B logserver.enabled
+
+
+determines if batch logs are exported via HTTP.  Valid values are True|False
+
+
+.B logserver.index
+
+
+determines if automatic directory indexing is allowed when log directories are browsed.  Valid values are True|False
+
+
+.B logserver.allowrobots
+
+if false, creates a robots.txt file in the docroot.  Valid valudes are True|False
+
+
+.B proxyConf
+
+local path to the configuration file for automatic proxy management.  
+.I NOTE: must be a local path, not a URI.
+
+
+.B proxymanager.enabled
+
+to determine if automatic proxy management is used or not.  Accepted values are True|False
+
+.B proxymanager.sleep
+
+Sleep interval for proxymanager thread. 
+
+
+.B queueConf
+
+URI plus path to the configuration file for APF queues.
+
+.I NOTE: Must be expressed as a URI (file:// or http://)
+.I NOTE: Cannot be used at the same time that queueDirConf
+
+
+.B queueDirConf
+
+directory with a set of configuration files, all of them to be used at the same time. 
+i.e. /etc/apf/queues.d/
+
+.I NOTE: Cannot be used at the same time that queueConf
+
+
+
+.B monitorConf
+
+
+local path to the configuration file for Monitor plugins.
+
+
+.B versionTag
+
+
+APF version number as it will be displayed in the web monitor
+
+
+.B wmsstatus.maximum
+
+
+maximum time while the info is considered reasonable.  If info stored is older than that, is considered not valid, and some NULL output will be returned.
+
+
+
+.B wmsstatus.panda.sleep
+
+
+time the WMSStatus Plugin waits between cycles Value is in seconds.
+
diff --git a/docs/man/autopyfactory-monitor.conf.5 b/docs/man/autopyfactory-monitor.conf.5
new file mode 100644
index 0000000..a9838e8
--- /dev/null
+++ b/docs/man/autopyfactory-monitor.conf.5
@@ -0,0 +1,20 @@
+.\" Process this file with
+ autopyfactory-proxy.conf.5
+.\"
+.TH AUTOPYFACTORY MONITOR.CONF 5 "JUNE 2013" Linux "User Manuals"
+.SH NAME
+AutoPyFactory monitor.conf \- monitor.conf configuration file for autopyfactory
+.SH DESCRIPTION
+.B monitor.conf
+
+/etc/apf/monitor.conf  Configuration file for the monitor setup for AutoPyFactory.
+
+.B monitorplugin 
+
+the type of plugin to handle this monitor instance 
+
+.B monitorURL   
+
+URL for the web monitor 
+
+
diff --git a/docs/man/autopyfactory-proxy.conf.5 b/docs/man/autopyfactory-proxy.conf.5
new file mode 100644
index 0000000..e35e149
--- /dev/null
+++ b/docs/man/autopyfactory-proxy.conf.5
@@ -0,0 +1,151 @@
+.\" Process this file with
+ autopyfactory-proxy.conf.5
+.\"
+.TH AUTOPYFACTORY PROXY.CONF 5 "JUNE 2013" Linux "User Manuals"
+.SH NAME
+AutoPyFactory proxy.conf \- proxy.conf configuration file for autopyfactory
+.SH DESCRIPTION
+.B proxy.conf
+
+/etc/apf/proxy.conf  Configuration file for the proxies management for AutoPyFactory.
+
+.B baseproxy
+
+
+if used, create a very long-lived proxy, e.g.
+
+    grid-proxy-init -valid 720:0 -out /tmp/plainProxy
+
+Note that maintenance of this proxy must occur completely outside of APF. 
+
+
+
+.B proxyfile
+
+
+path to the user grid proxy file.
+
+
+
+.B checktime
+
+
+How often to check proxy validity, in seconds
+
+
+
+.B interruptcheck
+
+
+Frequency to check for keyboard/signal interrupts, in seconds
+
+
+
+.B lifetime
+
+
+initial voms lifetime, in seconds (604800 = 7 days).  345600 is ATLAS VOMS maximum
+
+
+
+.B minlife
+
+
+Minimum lifetime of proxy (renew if less) in seconds
+
+
+
+.B renew
+
+
+If you do not want to use ProxyManager to renew proxies, set this  False and only define 'proxyfile'.  If renew is set to false, then no grid client setup is necessary. 
+
+
+
+.B usercert
+
+
+path to the user grid certificate file
+
+
+
+.B userkey
+
+
+path to the user grid key file
+
+
+
+.B vorole
+
+
+user VO role
+
+
+.B flavor
+
+
+voms or myproxy. voms directly generates proxy using cert or baseproxy myproxy retrieves a proxy from myproxy, then generates the target proxy against voms using it as baseproxy.
+
+
+.B myproxy_hostname
+
+
+Myproxy server host.
+
+
+.B myproxy_username
+
+
+User name to be used on MyProxy service
+
+
+.B myproxy_passphrase
+
+
+Passphrase for proxy retrieval from MyProxy
+
+
+.B retriever_profile
+
+
+A list of other proxymanager profiles to be used to authorize proxy retrieval from MyProxy.
+
+
+.B initdelay
+
+
+In seconds, how long to wait before generating. Needed for MyProxy when using cert authentication--we need to allow time for the auth credential to be generated (by another proxymanager profile).
+
+
+.B owner
+
+
+If running standalone (as root) and you want the proxy to be owned by another account.
+
+
+.B remote_host
+
+
+If defined, copy proxyfile to same path on remote host
+
+
+.B remote_user
+
+
+User to connect as?
+
+
+.B remote_owner
+
+
+If connect user is root, what account should own the file?
+
+
+.B remote_group
+
+
+If connect user is root, what group should own the file?
+
+
+
diff --git a/docs/man/autopyfactory-queues.conf.5 b/docs/man/autopyfactory-queues.conf.5
new file mode 100644
index 0000000..8b2990b
--- /dev/null
+++ b/docs/man/autopyfactory-queues.conf.5
@@ -0,0 +1,1153 @@
+.\" Process this file with
+ autopyfactory-queues.conf.5
+.\"
+.TH AUTOPYFACTORY QUEUES.CONF 5 "JUNE 2013" Linux "User Manuals"
+.SH NAME
+AutoPyFactory quues.conf \- queues.conf configuration file for autopyfactory 
+.SH DESCRIPTION
+.B queues.conf
+
+
+/etc/apf/queues.conf  Configuration file for APFQueue component of AutoPyFactory.
+
+Defaults for queues - these values are set when there is not an explicit value
+If you don't set them here the factory takes sensible default values, so nothing is mandatory
+see ConfigLoader._configurationDefaults() for these values. 
+
+Some of these values may be in the process of deprecation, especially submission parameters 
+which are now handled by the submit plugins. 
+
+.SH GENERIC VARIABLES
+
+.B override
+
+determines if values from this config file have precedence over
+the same values comming from different sources of information.
+If True then schedconfig does not clobber configuration file values.
+Valid values are True|False.
+
+
+.B cloud
+
+is the cloud this queue is in. You should set this to suppress pilot
+submission when the cloud goes offline
+N.B. Panda clouds are UPPER CASE, e.g., UK
+
+
+.B vo
+
+Virtual Organization
+
+
+.B grid
+
+Grid middleware flavor at the site. (e.g. OSG, EGI, NorduGrid)
+
+
+.B batchqueue
+
+the Batch system related queue name.
+E.g. the PanDA queue name (formerly called nickname)
+
+
+.B wmsqueue
+
+the WMS system queue name.
+E.g. the PanDA siteid name
+
+
+.B enabled
+
+determines if each queue section must be used by AutoPyFactory
+or not. Allows to disable a queue without commenting out all the values. 
+Valid values are True|False.
+
+
+.B status
+
+can be "test", "offline" or "online"
+
+
+.B apfqueue.sleep
+
+sleep time between cycles in APFQueue object.
+Value is in seconds.   
+
+
+.B autofill
+
+says if the info from this filled should be completed
+with info from a ConfigPlugin object
+
+
+.B cleanlogs.keepdays
+
+maximum number of days the condor logs
+will be kept
+
+
+
+.SH WMS STATUS PLUGIN VARIABLES
+
+
+.B wmsstatusplugin
+
+WMS Status Plugin.
+
+
+
+.SH CONFIG PLUGIN VARIABLES
+
+
+.B configplugin
+
+Config Plugin.
+Optional.
+E.g. Panda.
+
+
+
+.SH BATCH STATUS PLUGIN VARIABLES
+
+
+.B batchstatusplugin
+
+Batch Status Plugin.
+
+
+.B batchstatus.condor.queryargs
+
+list of command line input options
+to be included in the query command *verbatim*. E.g. 
+batchstatus.condor.query = -name <schedd_name> -pool <centralmanagerhostname[:portnumber]>
+
+
+
+.SH SCHED PLUGIN VARIABLES
+
+
+.B schedplugin
+
+specific Scheduler Plugin implementing
+the algorithm deciding how many new pilots
+to submit next cycle.
+The value can be a single Plugin or a split by comma
+list of Plugins.
+In the case of more than one plugin, 
+each one will acts as a filter with respect to the
+value returned by the previous one.
+By selecting the right combination of Plugins in a given order,
+a complex algorithm can be built.
+E.g., the algorithm can start by using Activated Plugin,
+which will determine the number of pilots based on 
+the number of activated jobs in the WMS queue and 
+the number of already submitted pilots.
+After that, this number can be filtered to 
+a maximum (MaxPerCycleSchedPlugin) or a minimum (MinPerCycleSchedPlugin)
+number of pilots.
+Or even can be filtered to a maximum number of pilots
+per factory (MaxPerFactorySchedPlugin)
+Also it can be filtered depending on the status of the wmsqueue 
+(StatusTestSchedPlugin, StatusOfflineSchedPlugin).
+
+
+
+.SH CONFIGURATION WHEN SCHEDPLUGIN IS ACTIVATED
+
+
+.I IMPORTANT NOTE: Deprecated. Activated Plugin is not maintained anymore. Instead, suggested option is to use Ready Plugin plus a chain of limiting plugins (MaxPerCycle, MinPerCycle...)
+
+.B sched.activated.default
+
+default number of pilots to be submitted
+when the context information 
+does not exist is not reliable 
+To be used in Activated Scheduler Plugin.
+
+
+.B sched.activated.max_jobs_torun
+
+maximum number of jobs running
+simoultaneously. 
+To be used in Activated Scheduler Plugin.
+
+
+.B sched.activated.max_pilots_per_cycle
+
+maximum number of pilots
+to be submitted per cycle.
+To be used in Activated Scheduler Plugin.
+
+
+.B sched.activated.min_pilots_per_cycle
+
+minimum number of pilots
+to be submitted per cycle.
+To be used in Activated Scheduler Plugin.
+
+
+.B sched.activated.min_pilots_pending
+
+minimum number of pilots
+to be idle on queue waiting to start execution.
+To be used in Activated Scheduler Plugin.
+
+
+.B sched.activated.max_pilots_pending
+
+maximum number of pilots
+to be idle on queue waiting to start execution.
+To be used in Activated Scheduler Plugin.
+
+
+.B sched.activated.testmode.allowed
+
+Boolean variable to trigger
+special mode of operation when the wmsqueue is in
+in status = test
+
+
+.B sched.activated.testmode.pilots
+
+number of pilots to submit
+when the wmsqueue is in status = test
+and sched.activated.testmode.allowed is True
+
+
+
+.SH CONFIGURATION WHEN SCHEDPLUGIN IS READY
+
+
+.B sched.ready.offset
+
+
+the minimum value in the number of ready jobs to trigger submission.
+
+
+
+.SH CONFIGURATION WHEN SCHEDPLUGIN IS FIXED
+
+
+.B sched.fixed.pilotspercycle
+
+fixed number of pilots to be submitted
+each cycle, when using the Fixed Scheduler Plugin.
+
+
+
+.SH CONFIGURATION WHEN SCHEDPLUGIN IS MAXPERCYCLE
+
+
+.B sched.maxpercycle.maximum
+
+maximum number of pilots to be submitted
+per cycle
+
+
+
+.SH CONFIGURATION WHEN SCHEDPLUGIN IS MINPERCYCLE
+
+
+.B sched.minpercycle.minimum
+
+minimum number of pilots to be submitted
+per cycle
+
+
+
+.SH CONFIGURATION WHEN SCHEDPLUGIN IS MAXPENDING
+
+
+.B sched.maxpending.maximum
+
+maximum number of pilots to be pending
+
+
+
+.SH CONFIGURATION WHEN SCHEDPLUGIN IS MINPENDING
+
+
+.B sched.minpending.minimum
+
+minimum number of pilots to be pending
+
+
+
+.SH CONFIGURATION WHEN SCHEDPLUGIN IS MAXTORUN
+
+
+.B sched.maxtorun.maximum
+
+maximum number of pilots allowed to, potentially,
+be running at a time. 
+
+
+
+.SH CONFIGURATION WHEN SCHEDPLUGIN IS STATUSTEST
+
+
+.B sched.statustest.pilots
+
+number of pilots to submit
+when the wmsqueue is in status = test
+
+
+.SH CONFIGURATION WHEN SCHEDPLUGIN IS STATUSOFFLINE
+
+
+.B sched.statusoffline.pilots
+
+number of pilots to submit
+when the wmsqueue or the cloud is in status = offline
+
+
+.SH CONFIGURATION WHEN SCHEDPLUGIN IS SIMPLE
+
+
+.B sched.simple.default
+
+default number of pilots to be submitted
+when the context information does not exist
+or is not reliable.
+To be used in Simple Scheduler Plugin.
+
+
+.B sched.simple.maxpendingpilots
+
+maximum number of pilots
+to be idle on queue waiting to start execution.
+To be used in Simple Scheduler Plugin.
+
+
+.B sched.simple.maxpilotspercycle
+
+maximum number of pilots
+to be submitted per cycle.
+To be used in Simple Scheduler Plugin.
+
+
+.SH CONFIGURATION WHEN SCHEDPLUGIN IS TRIVIAL
+
+
+.B sched.trivial.default
+
+default number of pilots
+to be submitted when the context information
+does not exist or is not reliable.
+To be used in Trivial Scheduler Plugin.
+
+
+
+.SH CONFIGURATION WHEN SCHEDPLUGIN IS SCALE
+
+
+.B sched.scale.factor
+
+scale factor to correct the previous value
+of the number of pilots.
+
+Value is a float number.
+
+
+
+.SH CONFIGURATION WHEN SCHEDPLUGIN IS KEEPNRUNNING
+
+
+.B sched.keepnrunning.keep_running
+
+number of total jobs to keep running and/or pending.
+
+
+
+
+.SH BATCH SUBMIT PLUGIN VARIABLES
+
+
+.B batchsubmitplugin
+
+Batch Submit Plugin.
+Currently available options are: 
+     CondorGT2, 
+     CondorGT5, 
+     CondorCREAM, 
+     CondorLocal, 
+     CondorEC2, 
+     CondorDeltaCloud.
+
+
+
+.SH CONFIGURATION WHEN BATCHSUBMITPLUGIN IS CONDORGT2
+
+
+.B batchsubmit.condorgt2.gridresource
+
+name of the CE (e.g. gridtest01.racf.bnl.gov/jobmanager-condor)
+
+
+.B batchsubmit.condorgt2.submitargs
+
+list of command line input options
+to be included in the submission command *verbatim*
+e.g. 
+    batchsubmit.condorgt2.submitargs = -remote my_schedd 
+will drive into a command like
+    condor_submit -remote my_schedd submit.jdl
+
+
+.B batchsubmit.condorgt2.condor_attributes
+
+list of condor attributes,
+splited by comma, 
+to be included in the condor submit file *verbatim*
+e.g. +Experiment = "ATLAS",+VO = "usatlas",+Job_Type = "cas"
+Can be used to include any line in the Condor-G file
+that is not otherwise added programmatically by AutoPyFactory.
+Note the following directives are added by default:
+
+        transfer_executable = True
+        stream_output=False
+        stream_error=False
+        notification=Error
+        copy_to_spool = false
+
+
+.B batchsubmit.condorgt2.environ
+
+list of environment variables,
+splitted by white spaces, 
+to be included in the condor attribute environment *verbatim*
+Therefore, the format should be env1=var1 env2=var2 envN=varN
+split by whitespaces.
+
+
+.B batchsubmit.condorgt2.proxy
+
+name of the proxy handler in proxymanager for automatic proxy renewal
+(See etc/proxy.conf)
+None if no automatic proxy renewal is desired.
+
+
+
+.SH GLOBUSRSL GRAM2 VARIABLES
+
+
+.B gram2
+
+The following are GRAM2 RSL variables.
+They are just used to build batchsubmit.condorgt2.globusrsl 
+(if needed)
+The globusrsl directive in the condor submission file looks like
+
+    globusrsl=(jobtype=single)(queue=short)
+
+Documentation can be found here:
+
+        http://www.globus.org/toolkit/docs/2.4/gram/gram_rsl_parameters.html
+
+
+.B globusrsl.gram2.arguments
+
+
+
+.B globusrsl.gram2.count
+
+
+
+.B globusrsl.gram2.directory
+
+
+
+.B globusrsl.gram2.dryRun
+
+
+
+.B globusrsl.gram2.environment
+
+
+
+.B globusrsl.gram2.executable
+
+
+
+.B globusrsl.gram2.gramMyJob
+
+
+
+.B globusrsl.gram2.hostCount
+
+
+
+.B globusrsl.gram2.jobType
+
+
+
+.B globusrsl.gram2.maxCpuTime
+
+
+
+.B globusrsl.gram2.maxMemory
+
+
+
+.B globusrsl.gram2.maxTime
+
+
+
+.B globusrsl.gram2.maxWallTime
+
+
+
+.B globusrsl.gram2.minMemory
+
+
+
+.B globusrsl.gram2.project
+
+
+
+.B globusrsl.gram2.queue
+
+
+
+.B globusrsl.gram2.remote_io_url
+
+
+
+.B globusrsl.gram2.restart
+
+
+
+.B globusrsl.gram2.save_state
+
+
+
+.B globusrsl.gram2.stderr
+
+
+
+.B globusrsl.gram2.stderr_position
+
+
+
+.B globusrsl.gram2.stdin
+
+
+
+.B globusrsl.gram2.stdout
+
+
+
+.B globusrsl.gram2.stdout_position
+
+
+
+.B globusrsl.gram2.two_phase
+
+
+
+.B globusrsl.gram2.globusrsl
+
+GRAM RSL directive.
+If this variable is not setup, then it will be built
+programmatically from all non empty globusrsl.gram2.XYZ variables.
+If this variable is setup, then its value
+will be taken *verbatim*, and all possible values
+for globusrsl.gram2.XYZ variables will be ignored. 
+
+
+.B globusrsl.gram2.globusrsladd
+
+custom fields to be added
+*verbatim* to the GRAM RSL directive,
+after it has been built either from 
+globusrsl.gram2.globusrsl value
+or from all globusrsl.gram2.XYZ variables.
+e.g. (condorsubmit=('+AccountingGroup' '\"group_atlastest.usatlas1\"')('+Requirements' 'True'))
+
+
+
+.SH CONFIGURATION WHEN BATCHSUBMITPLUGIN IS CONDORGT5
+
+
+.B batchsubmit.condorgt5.gridresource
+
+name of the CE (e.g. gridtest01.racf.bnl.gov/jobmanager-condor)
+
+
+.B batchsubmit.condorgt5.submitargs
+
+list of command line input options
+to be included in the submission command *verbatim*
+e.g. 
+    batchsubmit.condorgt2.submitargs = -remote my_schedd 
+will drive into a command like
+    condor_submit -remote my_schedd submit.jdl
+
+
+.B batchsubmit.condorgt5.condor_attributes
+
+list of condor attributes,
+splited by comma, 
+to be included in the condor submit file *verbatim*
+e.g. +Experiment = "ATLAS",+VO = "usatlas",+Job_Type = "cas"
+Can be used to include any line in the Condor-G file
+that is not otherwise added programmatically by AutoPyFactory.
+Note the following directives are added by default:
+
+        transfer_executable = True
+        stream_output=False
+        stream_error=False
+        notification=Error
+        copy_to_spool = false
+
+
+.B batchsubmit.condorgt5.environ
+
+list of environment variables,
+splitted by white spaces, 
+to be included in the condor attribute environment *verbatim*
+Therefore, the format should be env1=var1 env2=var2 envN=varN
+split by whitespaces.
+
+
+.B batchsubmit.condorgt5.proxy
+
+name of the proxy handler in proxymanager for automatic proxy renewal
+(See etc/proxy.conf)
+None if no automatic proxy renewal is desired.
+
+
+
+.SH GLOBUSRSL GRAM5 VARIABLES
+
+
+.B gram5
+
+The following are GRAM5 RSL variables.
+They are just used to build batchsubmit.condorgt5.globusrsl 
+(if needed)
+The globusrsl directive in the condor submission file looks like
+
+    globusrsl=(jobtype=single)(queue=short)
+
+Documentation can be found here:
+
+       http://www.globus.org/toolkit/docs/5.2/5.2.0/gram5/user/#gram5-user-rsl 
+
+
+.B globusrsl.gram5.arguments
+
+
+
+.B globusrsl.gram5.count
+
+
+
+.B globusrsl.gram5.directory
+
+
+
+.B globusrsl.gram5.dry_run
+
+
+
+.B globusrsl.gram5.environment
+
+
+
+.B globusrsl.gram5.executable
+
+
+
+.B globusrsl.gram5.file_clean_up
+
+
+
+.B globusrsl.gram5.file_stage_in
+
+
+
+.B globusrsl.gram5.file_stage_in_shared
+
+
+
+.B globusrsl.gram5.file_stage_out
+
+
+
+.B globusrsl.gram5.gass_cache
+
+
+
+.B globusrsl.gram5.gram_my_job
+
+
+
+.B globusrsl.gram5.host_count
+
+
+
+.B globusrsl.gram5.job_type
+
+
+
+.B globusrsl.gram5.library_path
+
+
+
+.B globusrsl.gram5.loglevel
+
+
+
+.B globusrsl.gram5.logpattern
+
+
+
+.B globusrsl.gram5.max_cpu_time
+
+
+
+.B globusrsl.gram5.max_memory
+
+
+
+.B globusrsl.gram5.max_time
+
+
+
+.B globusrsl.gram5.max_wall_time
+
+
+
+.B globusrsl.gram5.min_memory
+
+
+
+.B globusrsl.gram5.project
+
+
+
+.B globusrsl.gram5.proxy_timeout
+
+
+
+.B globusrsl.gram5.queue
+
+
+
+.B globusrsl.gram5.remote_io_url
+
+
+
+.B globusrsl.gram5.restart
+
+
+
+.B globusrsl.gram5.rsl_substitution
+
+
+
+.B globusrsl.gram5.savejobdescription
+
+
+
+.B globusrsl.gram5.save_state
+
+
+
+.B globusrsl.gram5.scratch_dir
+
+
+
+.B globusrsl.gram5.stderr
+
+
+
+.B globusrsl.gram5.stderr_position
+
+
+
+.B globusrsl.gram5.stdin
+
+
+
+.B globusrsl.gram5.stdout
+
+
+
+.B globusrsl.gram5.stdout_position
+
+
+
+.B globusrsl.gram5.two_phase
+
+
+
+.B globusrsl.gram5.username
+
+
+
+
+.B globusrsl.gram5.globusrsl
+
+GRAM RSL directive.
+If this variable is not setup, then it will be built
+programmatically from all non empty globusrsl.gram5.XYZ variables.
+If this variable is setup, then its value
+will be taken *verbatim*, and all possible values
+for globusrsl.gram5.XYZ variables will be ignored. 
+
+
+.B globusrsl.gram5.globusrsladd
+
+custom fields to be added
+*verbatim* to the GRAM RSL directive,
+after it has been built either from 
+globusrsl.gram5.globusrsl value
+or from all globusrsl.gram5.XYZ variables.
+e.g. (condorsubmit=('+AccountingGroup' '\"group_atlastest.usatlas1\"')('+Requirements' 'True'))
+
+
+
+.SH CONFIGURATION WHEN BATCHSUBMITPLUGIN IS CONDORCREAM
+
+
+.B batchsubmit.condorcream.webservice
+
+web service address (e.g. ce04.esc.qmul.ac.uk:8443/ce-cream/services/CREAM2)
+
+
+.B batchsubmit.condorcream.submitargs
+
+list of command line input options
+to be included in the submission command *verbatim*
+e.g. 
+    batchsubmit.condorgt2.submitargs = -remote my_schedd 
+will drive into a command like
+    condor_submit -remote my_schedd submit.jdl
+
+
+.B batchsubmit.condorcream.condor_attributes
+
+list of condor attributes,
+splited by comma, 
+to be included in the condor submit file *verbatim*
+e.g. +Experiment = "ATLAS",+VO = "usatlas",+Job_Type = "cas"
+Can be used to include any line in the Condor-G file
+that is not otherwise added programmatically by AutoPyFactory.
+Note the following directives are added by default:
+
+        transfer_executable = True
+        stream_output=False
+        stream_error=False
+        notification=Error
+        copy_to_spool = false
+
+
+.B batchsubmit.condorcream.environ
+
+list of environment variables,
+splitted by white spaces, 
+to be included in the condor attribute environment *verbatim*
+Therefore, the format should be env1=var1 env2=var2 envN=varN
+split by whitespaces.
+
+
+.B batchsubmit.condorcream.queue
+
+queue within the local batch system (e.g. short)
+
+
+.B batchsubmit.condorcream.port
+
+port number.
+
+
+.B batchsubmit.condorcream.batch
+
+local batch system (pbs, sge...)
+
+
+.B batchsubmit.condorcream.gridresource
+
+grid resource, built from other vars using interpolation:
+batchsubmit.condorcream.gridresource = %(batchsubmit.condorcream.webservice)s:%(batchsubmit.condorcream.port)s/ce-cream/services/CREAM2 %(batchsubmit.condorcream.batch)s %(batchsubmit.condorcream.queue)s
+
+
+.B batchsubmit.condorcream.proxy
+
+name of the proxy handler in proxymanager for automatic proxy renewal
+(See etc/proxy.conf)
+None if no automatic proxy renewal is desired.
+
+
+
+.SH CONFIGURATION WHEN BATCHSUBMITPLUGIN IS CONDOROSGCE
+
+
+.B batchsubmit.condorosgce.remote_condor_schedd
+
+condor schedd
+
+
+.B batchsubmit.condorosgce.remote_condor_collector
+
+condor collector
+
+
+.B batchsubmit.condorosgce.gridresource
+
+grid resource, built from other vars using interpolation
+batchsubmit.condorosgce.gridresource = %(batchsubmit.condorosgce.remote_condor_schedd) %(batchsubmit.condorosgce.remote_condor_collector)    
+
+.B batchsubmit.condorosgce.proxy
+
+name of the proxy handler in proxymanager for automatic proxy renewal
+(See etc/proxy.conf)
+None if no automatic proxy renewal is desired.
+
+
+
+.SH CONFIGURATION WHEN BATCHSUBMITPLUGIN IS CONDOREC2
+
+
+.B batchsubmit.condorec2.gridresource
+
+ec2 service's URL (e.g. https://ec2.amazonaws.com/ )
+
+
+.B batchsubmit.condorec2.submitargs
+
+list of command line input options
+to be included in the submission command *verbatim*
+e.g. 
+    batchsubmit.condorgt2.submitargs = -remote my_schedd 
+will drive into a command like
+    condor_submit -remote my_schedd submit.jdl
+
+
+.B batchsubmit.condorec2.condor_attributes
+
+list of condor attributes,
+splited by comma, 
+to be included in the condor submit file *verbatim*
+
+
+.B batchsubmit.condorec2.environ
+
+list of environment variables,
+splitted by white spaces, 
+to be included in the condor attribute environment *verbatim*
+Therefore, the format should be env1=var1 env2=var2 envN=varN
+split by whitespaces.
+
+
+.B batchsubmit.condorec2.ami_id
+
+identifier for the VM image,
+previously registered in one of Amazon's storage service (S3 or EBS)
+
+
+.B batchsubmit.condorec2.instance_type
+
+hardware configurations for instances to run on.
+
+
+.B batchsubmit.condorec2.user_data
+
+up to 16Kbytes of contextualization data.
+This makes it easy for many instances to share the same VM image, but perform different work.
+
+
+.B batchsubmit.condorec2.access_key_id
+
+path to file with the EC2 Access Key ID
+
+
+.B batchsubmit.condorec2.secret_access_key
+
+path to file with the EC2 Secret Access Key
+
+
+.B batchsubmit.condorec2.proxy
+
+name of the proxy handler in proxymanager for automatic proxy renewal
+(See etc/proxy.conf)
+None if no automatic proxy renewal is desired.
+
+
+
+.SH CONFIGURATION WHEN BATCHSUBMITPLUGIN IS CONDORDELTACLOUD
+
+
+.B batchsubmit.condordeltacloud.gridresource
+
+ec2 service's URL (e.g. https://deltacloud.foo.org/api )
+
+
+.B batchsubmit.condordeltacloud.username
+
+credentials in DeltaCloud
+
+
+.B batchsubmit.condordeltacloud.password_file
+
+path to the file with the password
+
+
+.B batchsubmit.condordeltacloud.image_id
+
+identifier for the VM image,
+previously registered with the cloud service.
+
+
+.B batchsubmit.condordeltacloud.keyname
+
+in case of using SSH,
+the command keyname specifies the identifier of the SSH key pair to use. 
+
+
+.B batchsubmit.condordeltacloud.realm_id
+
+selects one between multiple locations the cloud service may have.
+
+
+.B batchsubmit.condordeltacloud.hardware_profile
+
+selects one between the multiple hardware profiles
+the cloud service may provide
+
+
+.B batchsubmit.condordeltacloud.hardware_profile_memory
+
+customize the hardware profile
+
+
+.B batchsubmit.condordeltacloud.hardware_profile_cpu
+
+customize the hardware profile
+
+
+.B batchsubmit.condordeltacloud.hardware_profile_storage
+
+customize the hardware profile
+
+
+.B batchsubmit.condordeltacloud.user_data
+
+contextualization data
+
+
+
+.SH CONFIGURATION WHEN BATCHSUBMITPLUGIN IS CONDORLOCAL
+
+
+.B batchsubmit.condorlocal.submitargs
+
+list of command line input options
+to be included in the submission command *verbatim*
+e.g. 
+    batchsubmit.condorgt2.submitargs = -remote my_schedd 
+will drive into a command like
+    condor_submit -remote my_schedd submit.jdl
+
+
+.B batchsubmit.condorlocal.condor_attributes
+
+list of condor attributes,
+splited by comma, 
+to be included in the condor submit file *verbatim*
+e.g. +Experiment = "ATLAS",+VO = "usatlas",+Job_Type = "cas"
+Can be used to include any line in the Condor-G file
+that is not otherwise added programmatically by AutoPyFactory.
+Note the following directives are added by default:
+
+        universe = vanilla
+        transfer_executable = True
+        should_transfer_files = IF_NEEDED
+        +TransferOutput = ""
+        stream_output=False
+        stream_error=False
+        notification=Error
+        periodic_remove = (JobStatus == 5 && (CurrentTime - EnteredCurrentStatus) > 3600) || (JobStatus == 1 && globusstatus =!= 1 && (CurrentTime - EnteredCurrentStatus) > 86400)
+
+To be used in CondorLocal Batch Submit Plugin.
+
+
+.B batchsubmit.condorlocal.environ
+
+list of environment variables,
+splitted by white spaces, 
+to be included in the condor attribute environment *verbatim*
+To be used by CondorLocal Batch Submit Plugin.
+Therefore, the format should be env1=var1 env2=var2 envN=varN
+split by whitespaces.
+
+
+.B batchsubmit.condorlocal.proxy
+
+name of the proxy handler in proxymanager for automatic proxy renewal
+(See etc/proxy.conf)
+None if no automatic proxy renewal is desired.
+
+
+
+
+.SH MONITOR SECTION
+
+
+.B monitorsection
+
+
+section in monitor.conf where info 
+about the actual monitor plugin can be found.
+The value can be a single section or a split by comma
+list of sections.
+Monitor plugins handle job info publishing 
+to one or more web monitor/dashboards. 
+To specify more than one (sections) 
+simply use a comma-separated list.   
+
+
+
+.SH EXECUTABLE VARIABLES
+
+
+.B executable
+
+path to the script which will be run by condor.
+The executable can be anything, however, 
+two possible executables are distributed with AutoPyFactory:
+
+        - libexec/wrapper.sh 
+        - libexec/runpilot3-wrapper.sh 
+
+
+.B executable.arguments
+
+input options to be passed verbatim to the executable script.
+This variable can be built making use of an auxiliar variable
+called executable.defaultarguments
+This proposed ancilla works as a template, and its content is
+created on the fly from the value of other variables.
+This mechanism is called "interpolation", docs can be found here:
+
+    http://docs.python.org/library/configparser.html
+
+These are two examples of this type of templates 
+(included in the DEFAULTS block):
+
+    executable.defaultarguments = --wrappergrid=%(grid)s
+                --wrapperwmsqueue=%(wmsqueue)s
+                --wrapperbatchqueue=%(batchqueue)s
+                --wrappervo=%(vo)s
+                --wrappertarballurl=http://dev.racf.bnl.gov/dist/wrapper/wrapper.tar.gz
+                --wrapperserverurl=http://pandaserver.cern.ch:25080/cache/pilot
+                --wrapperloglevel=debug
+
+    executable.defaultarguments =  -s %(wmsqueue)s
+                -h %(batchqueue)s -p 25443
+                -w https://pandaserver.cern.ch  -j false  -k 0  -u user
+
+
+
diff --git a/docs/man/autopyfactory.1 b/docs/man/autopyfactory.1
new file mode 100644
index 0000000..47d845e
--- /dev/null
+++ b/docs/man/autopyfactory.1
@@ -0,0 +1,71 @@
+.\" Process this file with
+ autopyfactory
+.\"
+.TH AUTOPYFACTORY 1 "JUNE 2013" Linux "User Manuals"
+.SH NAME
+AutoPyFactory \- grid pilot factory 
+.SH SYNOPSIS
+.B /etc/init.d/factory
+.SH DESCRIPTION
+.B autopyfactory  
+
+ATLAS, one of the experiments at LHC at CERN, is one of the largest  users of grid computing infrastructure.
+As this infrastructure is now a central part of the experiment's computing operations,
+considerable efforts have been made to use this technology in the most efficient and effective way, including extensive use of pilot job based frameworks
+
+In this model the experiment submits 'pilot' jobs to sites without  payload. When these jobs begin to run they contact a central service  to pick-up a real payload to execute.
+
+The first generation of pilot factories were usually specific to a single VO, and were very bound to the particular architecture of that VO.
+A second generation is creating factories which are more flexible, not tied to any particular VO,
+and provide for more features other than just pilot submission (such as monitoring, logging, profiling, etc.)
+
+AutoPyFactory has a modular design and is highly configurable. It is able to send different types of pilots to sites, able to exploit
+different submission mechanisms and different charateristics of queues at sites.
+It has excellent integration with the PanDA job submission framework,
+tying pilot flows closely to the amount of work the site has to run.
+It is able to gather information from many sources, in order to correctly conigure itself for a site and its decision logic can easily be updated.
+
+Integrated into AutoPyFactory is a very flexible system for delivering both
+generic and specific wrappers which can perform many useful actions before starting to run end-user scientific applications,
+e.g., validation of the middleware, node profiling and diagnostics, monitoring and deciding what is the best end-user application that fits the resource.
+
+AutoPyFactory now also has a robust monitoring system and we show how this has helped setup a reliable pilot factory service for ATLAS.
+
+
+
+.SH FILES
+.I /etc/apf/queues.conf
+.RS
+The queues configuration file. See
+.BR queues (5)
+for further details.
+.RE
+.I /etc/apf/factory.conf
+.RS
+The main system configuration file. See
+.BR factory (5)
+for further details.
+.RE
+.I /etc/apf/proxy.conf
+.RS
+The proxy configuration file. See
+.BR proxy (5)
+for further details.
+.RE
+.I /etc/apf/monitor.conf
+.RS
+The monitor configuration file. See
+.BR monitor (5)
+for further details.
+.RE
+
+
+.SH AUTHOR
+Jose Caballero <jcaballero at bnl dot gov> and John Hover <jhover at bnl dot gov>
+
+.\".SH "SEE ALSO"
+.\".BR bar (1),
+.\".BR foo (5),
+.\".BR xyzzy (1)
+
+.\" groff -man -Tasc
diff --git a/etc/factory b/etc/factory
index 028a9b2..c2246f4 100755
--- a/etc/factory
+++ b/etc/factory
@@ -63,14 +63,14 @@ f_factdir
 
 if [ `id -u` = 0 ]; then
     APFHEAD=/
-    SYSCONF=/etc/sysconfig/factory.sysconfig
+    SYSCONF=/etc/sysconfig/factory
     BINDIR=/usr/bin
     ETCDIR=/etc/apf
     PIDFILE=/var/run/factory.pid
 else
     APFHEAD=`dirname $FACTDIR`
 
-    SYSCONF=$APFHEAD/etc/factory.sysconfig
+    SYSCONF=$APFHEAD/etc/sysconfig/factory
     BINDIR=$APFHEAD/bin
     ETCDIR=$APFHEAD/etc
     PIDFILE=$APFHEAD/var/run/factory.pid
diff --git a/etc/factory.conf b/etc/factory.conf
new file mode 100644
index 0000000..e4e1809
--- /dev/null
+++ b/etc/factory.conf
@@ -0,0 +1,165 @@
+#
+# factory.conf Configuration file for main Factory component of AutoPyFactory.
+#
+
+# ===========================================================================
+#               VARIABLES
+# ===========================================================================
+
+# ---------------------------------------------------------------------------
+# Factory
+# ---------------------------------------------------------------------------
+
+# baseLogDir
+# baseLogDirUrl
+# batchstatus.condor.sleep
+# batchstatus.maxtime
+# cycles
+# cleanlogs.keepdays
+# enablequeues 
+# factoryId
+# factoryAdminEmail
+# factorySMTPServer
+# factory.sleep
+# factoryUser
+# maxperfactory.maximum
+# logserver.enabled 
+# logserver.index 
+# logserver.excluderobots
+# proxyConf
+# proxymanager.enabled
+# proxymanager.sleep
+# queueConf
+# queueDirConf
+# monitorConf
+# wmsstatus.maxtime
+# wmsstatus.panda.sleep
+
+# ===========================================================================
+# Description:
+# ===========================================================================
+
+# ---------------------------------------------------------------------------
+# Factory
+# ---------------------------------------------------------------------------
+
+# baseLogDir =  where outputs from pilots are stored
+#               NOTE: No trailing '/'!!!
+#
+# baseLogDirUrl = where outputs from pilots are available via http.
+#               NOTE: It must include the port.
+#               NOTE: No trailing '/'!!!
+#
+# batchstatus.condor.sleep = time the Condor BatchStatus Plugin waits between cycles
+#               Value is in seconds.
+#
+# batchstatus.maxtime = maximum time while the info is considered reasonable. 
+#               If info stored is older than that, is considered not valid, 
+#               and some NULL output will be returned.
+#
+# cycles = maximum number of times the queues will loop. 
+#               None means forever.
+#
+# cleanlogs.keepdays = maximum number of days the condor logs
+#               will be kept, in case they are placed in a subdirectory
+#               for an APFQueue that is not being currently managed by 
+#               AutoPyFactory.
+#               For example, an apfqueue that has been created and used for a short
+#               amount of time, and it does not exist anymore.
+#               Still the created logs have to be cleaned at some point...
+#
+# enablequeues = default value to enable/disable all queues at once. 
+#               When True, its value will be overriden by the queue config variable 'enabled',
+#               queue by queue. 
+#               When False, all queues will stop working, 
+#               but the factory will still be alive performing basic actions (eg. printing logs).
+#
+# factoryId = Name that the factory instance will have in the APF web monitor. 
+#               Make factoryId something descriptive and unique for your factory,
+#               for example <site>-<host>-<admin> (e.g. BNL-gridui11-jhover)
+#
+# factoryAdminEmail = Email of the local admin to contact in case of a problem
+#               with an specific APF instance.
+#
+# factorySMTPServer = Server to use to send alert emails to admin. 
+#
+# factory.sleep = sleep time between cycles in mainLoop in Factory object
+#               Value is in seconds.
+#
+# factoryUser = account under which APF will run
+#
+# maxperfactory.maximum = maximum number of condor jobs 
+#               to be running at the same time per Factory.
+#               It is a global number, used by all APFQueues submitting
+#               pilots with condor.
+#               The value will be used by MaxPerFactorySchedPlugin plugin
+#
+#
+# logserver.enabled = determines if batch logs are exported via HTTP.
+#               Valid values are True|False
+#
+# logserver.index = determines if automatic directory indexing is allowed
+#               when log directories are browsed. 
+#               Valid values are True|False
+#
+# logserver.allowrobots = if false, creates a robots.txt file in the docroot. 
+#               Valid valudes are True|False
+#
+# proxyConf = local path to the configuration file for automatic proxy management.
+#             NOTE: must be a local path, not a URI. 
+#
+# proxymanager.enabled = to determine if automatic proxy management is used or not.
+#               Accepted values are True|False
+#
+# proxymanager.sleep =  Sleep interval for proxymanager thread. 
+#
+# queueConf = URI plus path to the configuration file for APF queues.
+#             NOTE: Must be expressed as a URI (file:// or http://)
+#             Cannot be used at the same time that queueDirConf
+#
+# queueDirConf = directory with a set of configuration files, 
+#            all of them to be used at the same time. 
+#            i.e.  /etc/apf/queues.d/
+#            Cannot be used at the same time that queueConf
+#
+# monitorConf = local path to the configuration file for Monitor plugins.
+#
+# wmsstatus.maximum = maximum time while the info is considered reasonable. 
+#               If info stored is older than that, is considered not valid, 
+#               and some NULL output will be returned.
+#
+# wmsstatus.panda.sleep = time the WMSStatus Plugin waits between cycles
+#               Value is in seconds.
+
+# ===========================================================================
+
+
+[Factory]
+
+factoryAdminEmail = neo@matrix.net
+factoryId = BNL-gridui11-jhover
+factorySMTPServer = mail.matrix.net
+factoryUser = apf
+enablequeues = True
+
+queueConf = file:///etc/apf/queues.conf
+queueDirConf = /etc/apf/queues.d/
+proxyConf = /etc/apf/proxy.conf
+proxymanager.enabled = True
+proxymanager.sleep = 30
+
+monitorConf = /etc/apf/monitor.conf
+
+cycles = None
+cleanlogs.keepdays = 14
+
+factory.sleep=30
+wmsstatus.panda.sleep = 150
+batchstatus.condor.sleep = 150
+
+baseLogDir = /home/apf/factory/logs
+baseLogDirUrl = http://myhost.matrix.net:25880
+
+logserver.enabled = True
+logserver.index = True
+logserver.allowrobots = False
diff --git a/etc/factory.conf-example b/etc/factory.conf-example
deleted file mode 100644
index e4e1809..0000000
--- a/etc/factory.conf-example
+++ /dev/null
@@ -1,165 +0,0 @@
-#
-# factory.conf Configuration file for main Factory component of AutoPyFactory.
-#
-
-# ===========================================================================
-#               VARIABLES
-# ===========================================================================
-
-# ---------------------------------------------------------------------------
-# Factory
-# ---------------------------------------------------------------------------
-
-# baseLogDir
-# baseLogDirUrl
-# batchstatus.condor.sleep
-# batchstatus.maxtime
-# cycles
-# cleanlogs.keepdays
-# enablequeues 
-# factoryId
-# factoryAdminEmail
-# factorySMTPServer
-# factory.sleep
-# factoryUser
-# maxperfactory.maximum
-# logserver.enabled 
-# logserver.index 
-# logserver.excluderobots
-# proxyConf
-# proxymanager.enabled
-# proxymanager.sleep
-# queueConf
-# queueDirConf
-# monitorConf
-# wmsstatus.maxtime
-# wmsstatus.panda.sleep
-
-# ===========================================================================
-# Description:
-# ===========================================================================
-
-# ---------------------------------------------------------------------------
-# Factory
-# ---------------------------------------------------------------------------
-
-# baseLogDir =  where outputs from pilots are stored
-#               NOTE: No trailing '/'!!!
-#
-# baseLogDirUrl = where outputs from pilots are available via http.
-#               NOTE: It must include the port.
-#               NOTE: No trailing '/'!!!
-#
-# batchstatus.condor.sleep = time the Condor BatchStatus Plugin waits between cycles
-#               Value is in seconds.
-#
-# batchstatus.maxtime = maximum time while the info is considered reasonable. 
-#               If info stored is older than that, is considered not valid, 
-#               and some NULL output will be returned.
-#
-# cycles = maximum number of times the queues will loop. 
-#               None means forever.
-#
-# cleanlogs.keepdays = maximum number of days the condor logs
-#               will be kept, in case they are placed in a subdirectory
-#               for an APFQueue that is not being currently managed by 
-#               AutoPyFactory.
-#               For example, an apfqueue that has been created and used for a short
-#               amount of time, and it does not exist anymore.
-#               Still the created logs have to be cleaned at some point...
-#
-# enablequeues = default value to enable/disable all queues at once. 
-#               When True, its value will be overriden by the queue config variable 'enabled',
-#               queue by queue. 
-#               When False, all queues will stop working, 
-#               but the factory will still be alive performing basic actions (eg. printing logs).
-#
-# factoryId = Name that the factory instance will have in the APF web monitor. 
-#               Make factoryId something descriptive and unique for your factory,
-#               for example <site>-<host>-<admin> (e.g. BNL-gridui11-jhover)
-#
-# factoryAdminEmail = Email of the local admin to contact in case of a problem
-#               with an specific APF instance.
-#
-# factorySMTPServer = Server to use to send alert emails to admin. 
-#
-# factory.sleep = sleep time between cycles in mainLoop in Factory object
-#               Value is in seconds.
-#
-# factoryUser = account under which APF will run
-#
-# maxperfactory.maximum = maximum number of condor jobs 
-#               to be running at the same time per Factory.
-#               It is a global number, used by all APFQueues submitting
-#               pilots with condor.
-#               The value will be used by MaxPerFactorySchedPlugin plugin
-#
-#
-# logserver.enabled = determines if batch logs are exported via HTTP.
-#               Valid values are True|False
-#
-# logserver.index = determines if automatic directory indexing is allowed
-#               when log directories are browsed. 
-#               Valid values are True|False
-#
-# logserver.allowrobots = if false, creates a robots.txt file in the docroot. 
-#               Valid valudes are True|False
-#
-# proxyConf = local path to the configuration file for automatic proxy management.
-#             NOTE: must be a local path, not a URI. 
-#
-# proxymanager.enabled = to determine if automatic proxy management is used or not.
-#               Accepted values are True|False
-#
-# proxymanager.sleep =  Sleep interval for proxymanager thread. 
-#
-# queueConf = URI plus path to the configuration file for APF queues.
-#             NOTE: Must be expressed as a URI (file:// or http://)
-#             Cannot be used at the same time that queueDirConf
-#
-# queueDirConf = directory with a set of configuration files, 
-#            all of them to be used at the same time. 
-#            i.e.  /etc/apf/queues.d/
-#            Cannot be used at the same time that queueConf
-#
-# monitorConf = local path to the configuration file for Monitor plugins.
-#
-# wmsstatus.maximum = maximum time while the info is considered reasonable. 
-#               If info stored is older than that, is considered not valid, 
-#               and some NULL output will be returned.
-#
-# wmsstatus.panda.sleep = time the WMSStatus Plugin waits between cycles
-#               Value is in seconds.
-
-# ===========================================================================
-
-
-[Factory]
-
-factoryAdminEmail = neo@matrix.net
-factoryId = BNL-gridui11-jhover
-factorySMTPServer = mail.matrix.net
-factoryUser = apf
-enablequeues = True
-
-queueConf = file:///etc/apf/queues.conf
-queueDirConf = /etc/apf/queues.d/
-proxyConf = /etc/apf/proxy.conf
-proxymanager.enabled = True
-proxymanager.sleep = 30
-
-monitorConf = /etc/apf/monitor.conf
-
-cycles = None
-cleanlogs.keepdays = 14
-
-factory.sleep=30
-wmsstatus.panda.sleep = 150
-batchstatus.condor.sleep = 150
-
-baseLogDir = /home/apf/factory/logs
-baseLogDirUrl = http://myhost.matrix.net:25880
-
-logserver.enabled = True
-logserver.index = True
-logserver.allowrobots = False
diff --git a/etc/factory.logrotate b/etc/factory.logrotate
deleted file mode 100644
index f37598c..0000000
--- a/etc/factory.logrotate
+++ /dev/null
@@ -1,32 +0,0 @@
-/var/log/apf/apf.log {
-  missingok
-  notifempty
-  sharedscripts
-  size 100M
-  rotate 10 
-  prerotate
-    [ -e /etc/init.d/factory ] && /etc/init.d/factory stop >/dev/null 2>&1 || true
-    sleep 5
-  endscript
-  postrotate
-    sleep 5
-    [ -e /etc/profile ] && . /etc/profile >/dev/null 2>&1 || true
-    [ -e /etc/init.d/factory ] && /etc/init.d/factory start >/dev/null 2>&1 || true
-  endscript
-}
-/var/log/apf/console.log {
-  missingok
-  notifempty
-  sharedscripts
-  size 50M
-  rotate 2
-  prerotate
-    [ -e /etc/init.d/factory ] && /etc/init.d/factory stop >/dev/null 2>&1 || true
-    sleep 5
-  endscript
-  postrotate
-    sleep 5
-    [ -e /etc/profile ] && . /etc/profile >/dev/null 2>&1 || true
-    [ -e /etc/init.d/factory ] && /etc/init.d/factory start >/dev/null 2>&1 || true
-  endscript
-}
diff --git a/etc/factory.sysconfig-example b/etc/factory.sysconfig-example
deleted file mode 100644
index 0a45448..0000000
--- a/etc/factory.sysconfig-example
+++ /dev/null
@@ -1,31 +0,0 @@
-#
-# Sysconfig file for autopyfactory
-#
-
-#  OPTIONS:
-# 
-#  --trace              Set logging level to TRACE [default WARNING], super verbose level
-#  --debug              Set logging level to DEBUG [default WARNING]
-#  --info               Set logging level to INFO [default WARNING]
-#  --quiet              Set logging level to WARNING [default WARNING]
-#  --conf=FILE1[,FILE2,FILE3]
-#                        Load configuration from FILEs (comma separated list)
-#  --runas=ACCOUNT       User account to run as. [apf]
-#  --sleep=TIME          Time to sleep between cycles. [60] 
-#  --log=LOGFILE         Send logging output to LOGFILE or SYSLOG or stdout
-#                        [default <syslog>]
-
-#
-# Override default conffile path if desired. 
-#
-# CONFFILE=/etc/apf/factory.conf
-# CONFFILE="/usatlas/us/caballer/etc/factory.conf"
-
-OPTIONS="--debug --sleep=60 --runas=apf --log=/var/log/apf/apf.log"
-CONSOLE_LOG=/var/log/apf/console.log
-
-#
-# Use this to provide voms-proxy-* on the path if it is not pre-setup by
-# default. 
-#
-# GRIDSETUP=/opt/osg-client/setup.sh
diff --git a/etc/logrotate/factory b/etc/logrotate/factory
new file mode 100644
index 0000000..f37598c
--- /dev/null
+++ b/etc/logrotate/factory
@@ -0,0 +1,32 @@
+/var/log/apf/apf.log {
+  missingok
+  notifempty
+  sharedscripts
+  size 100M
+  rotate 10 
+  prerotate
+    [ -e /etc/init.d/factory ] && /etc/init.d/factory stop >/dev/null 2>&1 || true
+    sleep 5
+  endscript
+  postrotate
+    sleep 5
+    [ -e /etc/profile ] && . /etc/profile >/dev/null 2>&1 || true
+    [ -e /etc/init.d/factory ] && /etc/init.d/factory start >/dev/null 2>&1 || true
+  endscript
+}
+/var/log/apf/console.log {
+  missingok
+  notifempty
+  sharedscripts
+  size 50M
+  rotate 2
+  prerotate
+    [ -e /etc/init.d/factory ] && /etc/init.d/factory stop >/dev/null 2>&1 || true
+    sleep 5
+  endscript
+  postrotate
+    sleep 5
+    [ -e /etc/profile ] && . /etc/profile >/dev/null 2>&1 || true
+    [ -e /etc/init.d/factory ] && /etc/init.d/factory start >/dev/null 2>&1 || true
+  endscript
+}
diff --git a/etc/logsmonitor.rotate.conf b/etc/logsmonitor.rotate.conf
new file mode 100644
index 0000000..3fe3751
--- /dev/null
+++ b/etc/logsmonitor.rotate.conf
@@ -0,0 +1,7 @@
+/home/apf/factory/logs/logsmonitor/error.lasthour.html {
+    rotate 23 
+    extension .html
+    missingok
+}
+
+
diff --git a/etc/logsmonitor.rotate.conf-example b/etc/logsmonitor.rotate.conf-example
deleted file mode 100644
index 3fe3751..0000000
--- a/etc/logsmonitor.rotate.conf-example
+++ /dev/null
@@ -1,7 +0,0 @@
-/home/apf/factory/logs/logsmonitor/error.lasthour.html {
-    rotate 23 
-    extension .html
-    missingok
-}
-
-
diff --git a/etc/monitor.conf b/etc/monitor.conf
new file mode 100644
index 0000000..cd72264
--- /dev/null
+++ b/etc/monitor.conf
@@ -0,0 +1,25 @@
+#
+# monitor.conf Configuration file for monitor plugins configuration
+#
+
+# ===========================================================================
+#               VARIABLES
+# ===========================================================================
+
+# monitorplugin 
+# monitorURL 
+
+# ===========================================================================
+# Description:
+# ===========================================================================
+
+# monitorplugin = the type of plugin to handle this monitor instance
+
+# monitorURL = URL for the web monitor
+
+# ===========================================================================
+
+
+[apfmon-lancaster]
+monitorplugin = APF
+monitorURL = http://apfmon.lancs.ac.uk/api
diff --git a/etc/monitor.conf-example b/etc/monitor.conf-example
deleted file mode 100644
index cd72264..0000000
--- a/etc/monitor.conf-example
+++ /dev/null
@@ -1,25 +0,0 @@
-#
-# monitor.conf Configuration file for monitor plugins configuration
-#
-
-# ===========================================================================
-#               VARIABLES
-# ===========================================================================
-
-# monitorplugin 
-# monitorURL 
-
-# ===========================================================================
-# Description:
-# ===========================================================================
-
-# monitorplugin = the type of plugin to handle this monitor instance
-
-# monitorURL = URL for the web monitor
-
-# ===========================================================================
-
-
-[apfmon-lancaster]
-monitorplugin = APF
-monitorURL = http://apfmon.lancs.ac.uk/api
diff --git a/etc/proxy.conf b/etc/proxy.conf
new file mode 100644
index 0000000..e1ed82b
--- /dev/null
+++ b/etc/proxy.conf
@@ -0,0 +1,149 @@
+#
+# proxy.conf   Configuration file for ProxyManager component of AutoPyFactory. 
+#
+
+# =========================================================================== 
+#               DESCRIPTION 
+# =========================================================================== 
+
+# baseproxy     If used, create a very long-lived proxy, e.g.
+#               grid-proxy-init -valid 720:0 -out /tmp/plainProxy
+#               Note that maintenance of this proxy must 
+#               occur completely outside of APF. 
+#
+# proxyfile     Target proxy path.
+#
+# lifetime      Initial voms lifetime, in seconds (604800 = 7 days)
+#               345600 is ATLAS VOMS maximum
+#
+# checktime     How often to check proxy validity, in seconds
+#
+# minlife       Minimum lifetime of proxy (renew if less) in seconds
+#
+# interruptcheck Frequency to check for keyboard/signal interrupts, in seconds
+#
+# renew         If you do not want to use ProxyManager to renew proxies, 
+#               set this  False and only define 'proxyfile'
+#               If renew is set to false, 
+#               then no grid client setup is necessary. 
+#
+# usercert      Path to the user grid certificate file
+#
+# userkey       Path to the user grid key file
+#
+# vorole        User VO role for target proxy. 
+#
+#             MyProxy Retrieval Functionality: Assumes you have created a long-lived
+#               proxy in a MyProxy server, out of band. 
+#
+# flavor        voms or myproxy. voms directly generates proxy using cert or baseproxy
+#               myproxy retrieves a proxy from myproxy, then generates the target proxy against
+#               voms using it as baseproxy.
+#
+# myproxy_hostname     Myproxy server host.   
+#
+# myproxy_username     User name to be used on MyProxy service
+# 
+# myproxy_passphrase   Passphrase for proxy retrieval from MyProxy
+#
+# retriever_profile    A list of other proxymanager profiles to be used to authorize proxy
+#                      retrieval from MyProxy. 
+#
+# initdelay            In seconds, how long to wait before generating. Needed for
+#                      MyProxy when using cert authentication--we need to allow time
+#                      for the auth credential to be generated (by another proxymanager profile). 
+#
+# owner                If running standalone (as root) and you want the proxy to be owned by another account. 
+#
+#             Remote Proxy Maintenance: Assumes you have enabled ssh-agent key-based access
+#               to the remote host where you want to maintain a proxy file. 
+#
+# remote_host          If defined, copy proxyfile to same path on remote host
+# remote_user          User to connect as? 
+# remote_owner         If connect user is root, what account should own the file?
+# remote_group         If connect user is root, what group should own the file?
+#
+#
+#
+# =========================================================================== 
+
+[DEFAULT]
+#envset_KEY = VALUE
+#envunset = VAR1,VAR2,VAR3
+# Baseproxy, if used.
+# If used, create a very long-lived proxy, e.g.
+#     grid-proxy-init -valid 720:0 -out /tmp/plainProxy
+# Note that maintenance of this proxy must occur completely outside of APF. 
+# baseproxy = /tmp/plainProxy
+
+baseproxy = None
+
+# flavor = voms OR myproxy
+flavor = voms
+remote_host = None         
+remote_user = None         
+remote_owner = None        
+remote_group = None
+
+usercert=~/.globus/usercert.pem
+userkey=~/.globus/userkeynopw.pem
+
+# Initial voms lifetime, in seconds (604800 = 7 days)
+# 345600 is ATLAS VOMS maximum
+# lifetime = 345600
+lifetime = 604800
+# How often to check proxy validity, in seconds
+checktime = 3600
+# checktime = 120
+# Minimum lifetime of proxy (renew if less) in seconds
+minlife = 259200
+# Frequency to check for keyboard/signal interrupts, in seconds
+interruptcheck = 1
+# Allow for init delay for MyProxy proxies that rely on other profiles. 
+initdelay = 0
+
+
+# If set, proxymanager will try to set the UNIX owner of the target proxy file accordingly. 
+# If unset, proxymanager will leave it owned by whatever user APF or the proxymanager is already 
+# running as. 
+# owner = apf
+
+#
+# If you don't want to use ProxyManager to renew proxies, set this  False
+# and only define 'proxyfile'
+#  If renew is set to false, then no grid client setup is necessary. 
+renew = True
+
+[atlas-usatlas]
+vorole = atlas:/atlas/usatlas
+proxyfile = /tmp/atlasProxy
+
+[atlas-production]
+# baseproxy = /tmp/plainProxy
+vorole = atlas:/atlas/Role=production
+proxyfile = /tmp/prodProxy
+
+#[osgvo]
+#vorole = osg:/osg
+
+# MyProxy proxy retrieved by certificate authentication
+[atlas-userproxy-one]
+myproxy_username = apfuser1
+flavor = myproxy
+initdelay = 15
+myproxy_hostname = myproxy.cern.ch
+vorole = atlas:/atlas/usatlas
+proxyfile = /tmp/userProxyOne
+retriever_list = atlas-production
+
+# MyProxy proxy retrieved by passphrase authentication
+[atlas-userproxy-two]
+myproxy_username = apfproxy
+flavor = myproxy
+# No init delay needed for passphrase retrieval
+initdelay = 0
+myproxy_hostname = myproxy.cern.ch
+vorole = atlas:/atlas/usatlas
+proxyfile = /tmp/userProxyTwo
+myproxy_passphrase = ppfortwo
+
diff --git a/etc/proxy.conf-example b/etc/proxy.conf-example
deleted file mode 100644
index e1ed82b..0000000
--- a/etc/proxy.conf-example
+++ /dev/null
@@ -1,149 +0,0 @@
-#
-# proxy.conf   Configuration file for ProxyManager component of AutoPyFactory. 
-#
-
-# =========================================================================== 
-#               DESCRIPTION 
-# =========================================================================== 
-
-# baseproxy     If used, create a very long-lived proxy, e.g.
-#               grid-proxy-init -valid 720:0 -out /tmp/plainProxy
-#               Note that maintenance of this proxy must 
-#               occur completely outside of APF. 
-#
-# proxyfile     Target proxy path.
-#
-# lifetime      Initial voms lifetime, in seconds (604800 = 7 days)
-#               345600 is ATLAS VOMS maximum
-#
-# checktime     How often to check proxy validity, in seconds
-#
-# minlife       Minimum lifetime of proxy (renew if less) in seconds
-#
-# interruptcheck Frequency to check for keyboard/signal interrupts, in seconds
-#
-# renew         If you do not want to use ProxyManager to renew proxies, 
-#               set this  False and only define 'proxyfile'
-#               If renew is set to false, 
-#               then no grid client setup is necessary. 
-#
-# usercert      Path to the user grid certificate file
-#
-# userkey       Path to the user grid key file
-#
-# vorole        User VO role for target proxy. 
-#
-#             MyProxy Retrieval Functionality: Assumes you have created a long-lived
-#               proxy in a MyProxy server, out of band. 
-#
-# flavor        voms or myproxy. voms directly generates proxy using cert or baseproxy
-#               myproxy retrieves a proxy from myproxy, then generates the target proxy against
-#               voms using it as baseproxy.
-#
-# myproxy_hostname     Myproxy server host.   
-#
-# myproxy_username     User name to be used on MyProxy service
-# 
-# myproxy_passphrase   Passphrase for proxy retrieval from MyProxy
-#
-# retriever_profile    A list of other proxymanager profiles to be used to authorize proxy
-#                      retrieval from MyProxy. 
-#
-# initdelay            In seconds, how long to wait before generating. Needed for
-#                      MyProxy when using cert authentication--we need to allow time
-#                      for the auth credential to be generated (by another proxymanager profile). 
-#
-# owner                If running standalone (as root) and you want the proxy to be owned by another account. 
-#
-#             Remote Proxy Maintenance: Assumes you have enabled ssh-agent key-based access
-#               to the remote host where you want to maintain a proxy file. 
-#
-# remote_host          If defined, copy proxyfile to same path on remote host
-# remote_user          User to connect as? 
-# remote_owner         If connect user is root, what account should own the file?
-# remote_group         If connect user is root, what group should own the file?
-#
-#
-#
-# =========================================================================== 
-
-[DEFAULT]
-#envset_KEY = VALUE
-#envunset = VAR1,VAR2,VAR3
-# Baseproxy, if used.
-# If used, create a very long-lived proxy, e.g.
-#     grid-proxy-init -valid 720:0 -out /tmp/plainProxy
-# Note that maintenance of this proxy must occur completely outside of APF. 
-# baseproxy = /tmp/plainProxy
-
-baseproxy = None
-
-# flavor = voms OR myproxy
-flavor = voms
-remote_host = None         
-remote_user = None         
-remote_owner = None        
-remote_group = None
-
-usercert=~/.globus/usercert.pem
-userkey=~/.globus/userkeynopw.pem
-
-# Initial voms lifetime, in seconds (604800 = 7 days)
-# 345600 is ATLAS VOMS maximum
-# lifetime = 345600
-lifetime = 604800
-# How often to check proxy validity, in seconds
-checktime = 3600
-# checktime = 120
-# Minimum lifetime of proxy (renew if less) in seconds
-minlife = 259200
-# Frequency to check for keyboard/signal interrupts, in seconds
-interruptcheck = 1
-# Allow for init delay for MyProxy proxies that rely on other profiles. 
-initdelay = 0
-
-
-# If set, proxymanager will try to set the UNIX owner of the target proxy file accordingly. 
-# If unset, proxymanager will leave it owned by whatever user APF or the proxymanager is already 
-# running as. 
-# owner = apf
-
-#
-# If you don't want to use ProxyManager to renew proxies, set this  False
-# and only define 'proxyfile'
-#  If renew is set to false, then no grid client setup is necessary. 
-renew = True
-
-[atlas-usatlas]
-vorole = atlas:/atlas/usatlas
-proxyfile = /tmp/atlasProxy
-
-[atlas-production]
-# baseproxy = /tmp/plainProxy
-vorole = atlas:/atlas/Role=production
-proxyfile = /tmp/prodProxy
-
-#[osgvo]
-#vorole = osg:/osg
-
-# MyProxy proxy retrieved by certificate authentication
-[atlas-userproxy-one]
-myproxy_username = apfuser1
-flavor = myproxy
-initdelay = 15
-myproxy_hostname = myproxy.cern.ch
-vorole = atlas:/atlas/usatlas
-proxyfile = /tmp/userProxyOne
-retriever_list = atlas-production
-
-# MyProxy proxy retrieved by passphrase authentication
-[atlas-userproxy-two]
-myproxy_username = apfproxy
-flavor = myproxy
-# No init delay needed for passphrase retrieval
-initdelay = 0
-myproxy_hostname = myproxy.cern.ch
-vorole = atlas:/atlas/usatlas
-proxyfile = /tmp/userProxyTwo
-myproxy_passphrase = ppfortwo
-
diff --git a/etc/proxymanager b/etc/proxymanager
index 1e7f03b..ccc6d39 100755
--- a/etc/proxymanager
+++ b/etc/proxymanager
@@ -62,14 +62,14 @@ done
 f_factdir
 
 if [ `id -u` = 0 ]; then
-    SYSCONF=/etc/sysconfig/proxymanager.sysconfig
+    SYSCONF=/etc/sysconfig/proxymanager
     BINDIR=/usr/bin
     ETCDIR=/etc/apf
     PIDFILE=/var/run/proxymanager.pid
 else
     APFHEAD=`dirname $FACTDIR`
 
-    SYSCONF=$APFHEAD/etc/proxymanager.sysconfig
+    SYSCONF=$APFHEAD/etc/sysconfig/proxymanager
     BINDIR=$APFHEAD/bin
     ETCDIR=$APFHEAD/etc
     PIDFILE=$APFHEAD/var/run/proxymanager.pid
diff --git a/etc/proxymanager.sysconfig-example b/etc/proxymanager.sysconfig-example
deleted file mode 100644
index a9d73d7..0000000
--- a/etc/proxymanager.sysconfig-example
+++ /dev/null
@@ -1,28 +0,0 @@
-#
-# Sysconfig file for proxymanager
-#
-
-#  OPTIONS:
-
-#  --debug              Set logging level to DEBUG [default WARNING]
-#  --info               Set logging level to INFO [default WARNING]
-#  --config=FILE1[,FILE2,FILE3]
-#                        Load configuration from FILEs (comma separated list)
-#  --runas=ACCOUNT       User account to run as. [apf]
-#  --log=LOGFILE         Send logging output to LOGFILE or SYSLOG or stdout
-#                        [default <syslog>]
-
-#
-# Override default conffile path if desired. 
-#
-# CONFFILE=/etc/apf/factory.conf
-# CONFFILE="/usatlas/us/caballer/etc/factory.conf"
-
-OPTIONS="--debug --log=/var/log/apf/proxymanager.log"
-CONSOLE_LOG=/var/log/apf/console.log
-
-#
-# Use this to provide voms-proxy-* on the path if it is not pre-setup by
-# default. 
-#
-# GRIDSETUP=/opt/osg-client/setup.sh
diff --git a/etc/queues.conf b/etc/queues.conf
new file mode 100644
index 0000000..f5580a9
--- /dev/null
+++ b/etc/queues.conf
@@ -0,0 +1,1119 @@
+#
+# queues.conf  Configuration file for APFQueue component of AutoPyFactory.
+#
+
+## Defaults for queues - these values are set when there is not an explicit value
+## If you don't set them here the factory takes sensible default values, so nothing is mandatory
+## see ConfigLoader._configurationDefaults() for these values. 
+#
+# Some of these values may be in the process of deprecation, especially submission parameters 
+# which are now handled by the submit plugins. 
+
+# =========================================================================== 
+#               VARIABLES
+# =========================================================================== 
+
+#
+# grid
+# vo
+# cloud
+# batchqueue 
+# wmsqueue 
+# enabled
+# status
+# apfqueue.sleep
+#
+# cleanlogs.keepdays
+#
+# wmsstatusplugin
+#
+# batchstatusplugin
+# batchstatus.condor.queryargs
+#
+# schedplugin
+# sched.activated.default
+# sched.activated.max_jobs_torun
+# sched.activated.max_pilots_per_cycle
+# sched.activated.min_pilots_per_cycle
+# sched.activated.min_pilots_pending
+# sched.activated.max_pilots_pending
+# sched.activated.testmode.allowed
+# sched.activated.testmode.pilots
+# sched.ready.offset
+# sched.fixed.pilotspercycle
+# sched.maxpercycle.maximum
+# sched.minpercycle.minimum
+# sched.maxpending.maximum
+# sched.minpending.minimum
+# sched.maxtorun.maximum
+# sched.statustest.pilots
+# sched.statusoffline.pilots
+# sched.simple.default
+# sched.simple.maxpendingpilots
+# sched.simple.maxpilotspercycle
+# sched.trivial.default
+# sched.scale.factor
+# sched.keepnrunning.keep_running
+#
+# batchsubmitplugin
+# batchsubmit.condorgt2.submitargs
+# batchsubmit.condorgt2.gridresource
+# batchsubmit.condorgt2.condor_attributes
+# batchsubmit.condorgt2.environ
+# batchsubmit.condorgt2.proxy
+#
+# globusrsl.gram2.arguments
+# globusrsl.gram2.count
+# globusrsl.gram2.directory
+# globusrsl.gram2.dryRun
+# globusrsl.gram2.environment
+# globusrsl.gram2.executable
+# globusrsl.gram2.gramMyJob
+# globusrsl.gram2.hostCount
+# globusrsl.gram2.jobType
+# globusrsl.gram2.maxCpuTime
+# globusrsl.gram2.maxMemory
+# globusrsl.gram2.maxTime
+# globusrsl.gram2.maxWallTime
+# globusrsl.gram2.minMemory
+# globusrsl.gram2.project
+# globusrsl.gram2.queue
+# globusrsl.gram2.remote_io_url
+# globusrsl.gram2.restart
+# globusrsl.gram2.save_state
+# globusrsl.gram2.stderr
+# globusrsl.gram2.stderr_position
+# globusrsl.gram2.stdin
+# globusrsl.gram2.stdout
+# globusrsl.gram2.stdout_position
+# globusrsl.gram2.two_phase
+# globusrsl.gram2.globusrsl
+# globusrsl.gram2.globusrsladd
+#
+# batchsubmit.condorgt5.submitargs
+# batchsubmit.condorgt5.gridresource
+# batchsubmit.condorgt5.condor_attributes
+# batchsubmit.condorgt5.environ
+# batchsubmit.condorgt5.proxy
+#
+# globusrsl.gram5.arguments
+# globusrsl.gram5.count
+# globusrsl.gram5.directory
+# globusrsl.gram5.dry_run
+# globusrsl.gram5.environment
+# globusrsl.gram5.executable
+# globusrsl.gram5.file_clean_up
+# globusrsl.gram5.file_stage_in
+# globusrsl.gram5.file_stage_in_shared
+# globusrsl.gram5.file_stage_out
+# globusrsl.gram5.gass_cache
+# globusrsl.gram5.gram_my_job
+# globusrsl.gram5.host_count
+# globusrsl.gram5.job_type
+# globusrsl.gram5.library_path
+# globusrsl.gram5.loglevel
+# globusrsl.gram5.logpattern
+# globusrsl.gram5.max_cpu_time
+# globusrsl.gram5.max_memory
+# globusrsl.gram5.max_time
+# globusrsl.gram5.max_wall_time
+# globusrsl.gram5.min_memory
+# globusrsl.gram5.project
+# globusrsl.gram5.proxy_timeout
+# globusrsl.gram5.queue
+# globusrsl.gram5.remote_io_url
+# globusrsl.gram5.restart
+# globusrsl.gram5.rsl_substitution
+# globusrsl.gram5.savejobdescription
+# globusrsl.gram5.save_state
+# globusrsl.gram5.scratch_dir
+# globusrsl.gram5.stderr
+# globusrsl.gram5.stderr_position
+# globusrsl.gram5.stdin
+# globusrsl.gram5.stdout
+# globusrsl.gram5.stdout_position
+# globusrsl.gram5.two_phase
+# globusrsl.gram5.username
+# globusrsl.gram5.globusrsl
+# globusrsl.gram5.globusrsladd
+#
+# batchsubmit.condorcream.submitargs
+# batchsubmit.condorcream.webservice
+# batchsubmit.condorcream.gridresource
+# batchsubmit.condorcream.condor_attributes
+# batchsubmit.condorcream.environ
+# batchsubmit.condorcream.queue
+# batchsubmit.condorcream.port
+# batchsubmit.condorcream.batch
+# batchsubmit.condorcream.proxy
+#
+# batchsubmit.condorosgce.remote_condor_schedd 
+# batchsubmit.condorosgce.remote_condor_collector
+# batchsubmit.condorosgce.gridresource
+# batchsubmit.condorosgce.proxy
+#
+# batchsubmit.condorec2.submitargs
+# batchsubmit.condorec2.gridresource
+# batchsubmit.condorec2.condor_attributes
+# batchsubmit.condorec2.environ
+# batchsubmit.condorec2.ami_id
+# batchsubmit.condorec2.instance_type
+# batchsubmit.condorec2.user_data
+# batchsubmit.condorec2.access_key_id
+# batchsubmit.condorec2.secret_access_key
+# batchsubmit.condorec2.proxy
+#
+# batchsubmit.condordeltacloud.gridresource
+# batchsubmit.condordeltacloud.username
+# batchsubmit.condordeltacloud.password_file
+# batchsubmit.condordeltacloud.image_id
+# batchsubmit.condordeltacloud.keyname
+# batchsubmit.condordeltacloud.realm_id
+# batchsubmit.condordeltacloud.hardware_profile
+# batchsubmit.condordeltacloud.hardware_profile_memory
+# batchsubmit.condordeltacloud.hardware_profile_cpu
+# batchsubmit.condordeltacloud.hardware_profile_storage
+# batchsubmit.condordeltacloud.user_data
+#
+# batchsubmit.condorlocal.submitargs
+# batchsubmit.condorlocal.condor_attributes
+# batchsubmit.condorlocal.environ
+# batchsubmit.condorlocal.proxy
+#
+# batchsubmit.condorlsf.proxy
+#
+# batchsubmit.condornordugrid.gridresource
+# nordugridrsl
+# nordugridrsladd
+# nordugridrsl.addenv.
+#
+# monitorsection
+#
+# executable
+# executable.arguments
+
+# =========================================================================== 
+#               DESCRIPTION 
+# =========================================================================== 
+
+#  --------------------------------------------------------------------
+#       Generic variables
+#  --------------------------------------------------------------------
+
+# cloud = is the cloud this queue is in. You should set this to suppress pilot 
+#               submission when the cloud goes offline
+#               N.B. Panda clouds are UPPER CASE, e.g., UK
+#
+# vo = Virtual Organization
+#
+# grid = Grid middleware flavor at the site. (e.g. OSG, EGI, NorduGrid) 
+#
+# batchqueue = the Batch system related queue name. 
+#               E.g. the PanDA queue name (formerly called nickname)
+#
+# wmsqueue = the WMS system queue name. 
+#               E.g. the PanDA siteid name
+#
+# enabled = determines if each queue section must be used by AutoPyFactory
+#               or not. Allows to disable a queue without commenting out all the values. 
+#               Valid values are True|False.
+#
+# status = can be "test", "offline" or "online"
+#
+# apfqueue.sleep = sleep time between cycles in APFQueue object.
+#               Value is in seconds.   
+#
+# cleanlogs.keepdays = maximum number of days the condor logs
+#               will be kept
+
+#  --------------------------------------------------------------------
+#       WMS Status Plugin variables 
+#  --------------------------------------------------------------------
+
+# wmsstatusplugin = WMS Status Plugin.
+
+#  --------------------------------------------------------------------
+#       Batch Status Plugin variables 
+#  --------------------------------------------------------------------
+
+# batchstatusplugin = Batch Status Plugin.
+#
+# batchstatus.condor.queryargs = list of command line input options
+#               to be included in the query command *verbatim*. E.g. 
+#               batchstatus.condor.query = -name <schedd_name> -pool <centralmanagerhostname[:portnumber]>
+
+#  --------------------------------------------------------------------
+#       Sched Plugin variables 
+#  --------------------------------------------------------------------
+
+# schedplugin = specific Scheduler Plugin implementing
+#               the algorithm deciding how many new pilots
+#               to submit next cycle.
+#               The value can be a single Plugin or a split by comma
+#               list of Plugins.
+#               In the case of more than one plugin, 
+#               each one will acts as a filter with respect to the
+#               value returned by the previous one.
+#               By selecting the right combination of Plugins in a given order,
+#               a complex algorithm can be built.
+#               E.g., the algorithm can start by using Ready Plugin,
+#               which will determine the number of pilots based on 
+#               the number of activated jobs in the WMS queue and 
+#               the number of already submitted pilots.
+#               After that, this number can be filtered to 
+#               a maximum (MaxPerCycleSchedPlugin) or a minimum (MinPerCycleSchedPlugin)
+#               number of pilots.
+#               Or even can be filtered to a maximum number of pilots
+#               per factory (MaxPerFactorySchedPlugin)
+#               Also it can be filtered depending on the status of the wmsqueue 
+#               (StatusTestSchedPlugin, StatusOfflineSchedPlugin).
+
+
+#  --------------------------------------------------------------------
+#       Configuration when schedplugin is Activated
+#  --------------------------------------------------------------------
+
+#  IMPORTANT NOTE: Activated plugin is decommissioned. 
+#                  It is not maintained anymore.
+
+# sched.activated.default = default number of pilots to be submitted
+#               when the context information 
+#               does not exist is not reliable 
+#               To be used in Activated Scheduler Plugin.
+#
+# sched.activated.max_jobs_torun = maximum number of jobs running
+#               simoultaneously. 
+#               To be used in Activated Scheduler Plugin.
+#
+# sched.activated.max_pilots_per_cycle = maximum number of pilots
+#               to be submitted per cycle.
+#               To be used in Activated Scheduler Plugin.
+#
+# sched.activated.min_pilots_per_cycle = minimum number of pilots
+#               to be submitted per cycle.
+#               To be used in Activated Scheduler Plugin.
+#
+# sched.activated.min_pilots_pending = minimum number of pilots
+#               to be idle on queue waiting to start execution.
+#               To be used in Activated Scheduler Plugin.
+#
+# sched.activated.max_pilots_pending = maximum number of pilots
+#               to be idle on queue waiting to start execution.
+#               To be used in Activated Scheduler Plugin.
+#
+# sched.activated.testmode.allowed = Boolean variable to trigger
+#               special mode of operation when the wmsqueue is in
+#               in status = test
+#
+# sched.activated.testmode.pilots = number of pilots to submit
+#               when the wmsqueue is in status = test
+#               and sched.activated.testmode.allowed is True
+#
+# sched.activated.testmode.max_pending = maximum number of pilots 
+#               permitted in pendig state when the wmsqueue is in 
+#               status = test and sched.activated.testmode.allowed is True
+#
+
+#  --------------------------------------------------------------------
+#       Configuration when schedplugin is Ready 
+#  --------------------------------------------------------------------
+
+#  sched.ready.offset = the minimum value in the number of ready jobs
+#               to trigger submission. 
+
+#  --------------------------------------------------------------------
+#       Configuration when schedplugin is Fixed
+#  --------------------------------------------------------------------
+
+# sched.fixed.pilotspercycle = fixed number of pilots to be submitted
+#               each cycle, when using the Fixed Scheduler Plugin.
+
+
+#  --------------------------------------------------------------------
+#       Configuration when schedplugin is MaxPerCycle 
+#  --------------------------------------------------------------------
+
+# sched.maxpercycle.maximum = maximum number of pilots to be submitted
+#               per cycle
+
+
+#  --------------------------------------------------------------------
+#       Configuration when schedplugin is MinPerCycle 
+#  --------------------------------------------------------------------
+
+# sched.minpercycle.minimum = minimum number of pilots to be submitted
+#               per cycle
+
+
+#  --------------------------------------------------------------------
+#       Configuration when schedplugin is MaxPending
+#  --------------------------------------------------------------------
+
+# sched.maxpending.maximum = maximum number of pilots to be pending
+#             
+
+#  --------------------------------------------------------------------
+#       Configuration when schedplugin is MinPending 
+#  --------------------------------------------------------------------
+
+# sched.minpending.minimum = minimum number of pilots to be pending
+#              
+
+#  --------------------------------------------------------------------
+#       Configuration when schedplugin is MaxToRun
+#  --------------------------------------------------------------------
+
+# sched.maxtorun.maximum = maximum number of pilots allowed to, potentially, 
+#               be running at a time. 
+#              
+
+#  --------------------------------------------------------------------
+#       Configuration when schedplugin is StatusTest
+#  --------------------------------------------------------------------
+
+# sched.statustest.pilots = number of pilots to submit
+#               when the wmsqueue is in status = test
+
+#  --------------------------------------------------------------------
+#       Configuration when schedplugin is StatusOffline
+#  --------------------------------------------------------------------
+
+# sched.statusoffline.pilots = number of pilots to submit
+#               when the wmsqueue or the cloud is in status = offline
+
+#  --------------------------------------------------------------------
+#       Configuration when schedplugin is Simple
+#  --------------------------------------------------------------------
+
+# sched.simple.default = default number of pilots to be submitted
+#               when the context information does not exist
+#               or is not reliable.
+#               To be used in Simple Scheduler Plugin.
+#
+# sched.simple.maxpendingpilots = maximum number of pilots
+#               to be idle on queue waiting to start execution.
+#               To be used in Simple Scheduler Plugin.
+#
+# sched.simple.maxpilotspercycle = maximum number of pilots
+#               to be submitted per cycle.
+#               To be used in Simple Scheduler Plugin.
+
+#  --------------------------------------------------------------------
+#       Configuration when schedplugin is Trivial
+#  --------------------------------------------------------------------
+
+# sched.trivial.default = default number of pilots
+#               to be submitted when the context information
+#               does not exist or is not reliable.
+#               To be used in Trivial Scheduler Plugin.
+
+#  --------------------------------------------------------------------
+#       Configuration when schedplugin is Scale 
+#  --------------------------------------------------------------------
+
+# sched.scale.factor = scale factor to correct the previous value
+#               of the number of pilots.
+#               Value is a float number.
+
+
+#  --------------------------------------------------------------------
+#       Configuration when schedplugin is KeepNRunning
+#  --------------------------------------------------------------------
+
+# sched.keepnrunning.keep_running = number of total jobs to keep 
+#               running and/or pending
+
+#  --------------------------------------------------------------------
+#       Batch Submit Plugin variables 
+#  --------------------------------------------------------------------
+
+# batchsubmitplugin = Batch Submit Plugin.
+#               Currently available options are: 
+#                    CondorGT2, 
+#                    CondorGT5, 
+#                    CondorCREAM, 
+#                    CondorLocal, 
+#                    CondorLSF,
+#                    CondorEC2, 
+#                    CondorDeltaCloud.
+
+#  --------------------------------------------------------------------
+#       Configuration when batchsubmitplugin is condorgt2
+#  --------------------------------------------------------------------
+
+# batchsubmit.condorgt2.gridresource = name of the CE (e.g. gridtest01.racf.bnl.gov/jobmanager-condor)
+#
+# batchsubmit.condorgt2.submitargs = list of command line input options
+#               to be included in the submission command *verbatim*
+#               e.g. 
+#                   batchsubmit.condorgt2.submitargs = -remote my_schedd 
+#               will drive into a command like
+#                   condor_submit -remote my_schedd submit.jdl
+#
+# batchsubmit.condorgt2.condor_attributes = list of condor attributes, 
+#               splited by comma, 
+#               to be included in the condor submit file *verbatim*
+#               e.g. +Experiment = "ATLAS",+VO = "usatlas",+Job_Type = "cas"
+#               Can be used to include any line in the Condor-G file
+#               that is not otherwise added programmatically by AutoPyFactory.
+#               Note the following directives are added by default:
+#
+#                       transfer_executable = True
+#                       stream_output=False
+#                       stream_error=False
+#                       notification=Error
+#                       copy_to_spool = false
+#
+# batchsubmit.condorgt2.environ = list of environment variables, 
+#               splitted by white spaces, 
+#               to be included in the condor attribute environment *verbatim*
+#               Therefore, the format should be env1=var1 env2=var2 envN=varN
+#               split by whitespaces.
+#
+# batchsubmit.condorgt2.proxy = name of the proxy handler in proxymanager for automatic proxy renewal 
+#               (See etc/proxy.conf)
+#               None if no automatic proxy renewal is desired.
+#
+
+#  --------------------------------------------------------------------
+#       GlobusRSL GRAM2 variables
+#  --------------------------------------------------------------------
+
+# gram2 =       The following are GRAM2 RSL variables. 
+#               They are just used to build batchsubmit.condorgt2.globusrsl 
+#               (if needed)
+#               The globusrsl directive in the condor submission file looks like
+#
+#                   globusrsl=(jobtype=single)(queue=short)
+#
+#               Documentation can be found here:
+#
+#                       http://www.globus.org/toolkit/docs/2.4/gram/gram_rsl_parameters.html
+#
+# globusrsl.gram2.arguments = 
+# globusrsl.gram2.count = 
+# globusrsl.gram2.directory =
+# globusrsl.gram2.dryRun =
+# globusrsl.gram2.environment =
+# globusrsl.gram2.executable =
+# globusrsl.gram2.gramMyJob =
+# globusrsl.gram2.hostCount =
+# globusrsl.gram2.jobType =
+# globusrsl.gram2.maxCpuTime =
+# globusrsl.gram2.maxMemory =
+# globusrsl.gram2.maxTime =
+# globusrsl.gram2.maxWallTime =
+# globusrsl.gram2.minMemory =
+# globusrsl.gram2.project =
+# globusrsl.gram2.queue =
+# globusrsl.gram2.remote_io_url =
+# globusrsl.gram2.restart =
+# globusrsl.gram2.save_state =
+# globusrsl.gram2.stderr =
+# globusrsl.gram2.stderr_position =
+# globusrsl.gram2.stdin =
+# globusrsl.gram2.stdout =
+# globusrsl.gram2.stdout_position =
+# globusrsl.gram2.two_phase =
+#
+# globusrsl.gram2.globusrsl = GRAM RSL directive.
+#               If this variable is not setup, then it will be built
+#               programmatically from all non empty globusrsl.gram2.XYZ variables.
+#               If this variable is setup, then its value
+#               will be taken *verbatim*, and all possible values
+#               for globusrsl.gram2.XYZ variables will be ignored. 
+#
+# globusrsl.gram2.globusrsladd = custom fields to be added
+#               *verbatim* to the GRAM RSL directive,
+#               after it has been built either from 
+#               globusrsl.gram2.globusrsl value
+#               or from all globusrsl.gram2.XYZ variables.
+#               e.g. (condorsubmit=('+AccountingGroup' '\"group_atlastest.usatlas1\"')('+Requirements' 'True'))
+
+
+#  --------------------------------------------------------------------
+#       Configuration when batchsubmitplugin is condorgt5
+#  --------------------------------------------------------------------
+
+# batchsubmit.condorgt5.gridresource = name of the CE (e.g. gridtest01.racf.bnl.gov/jobmanager-condor)
+#
+# batchsubmit.condorgt5.submitargs = list of command line input options
+#               to be included in the submission command *verbatim*
+#               e.g. 
+#                   batchsubmit.condorgt2.submitargs = -remote my_schedd 
+#               will drive into a command like
+#                   condor_submit -remote my_schedd submit.jdl
+#
+# batchsubmit.condorgt5.condor_attributes = list of condor attributes, 
+#               splited by comma, 
+#               to be included in the condor submit file *verbatim*
+#               e.g. +Experiment = "ATLAS",+VO = "usatlas",+Job_Type = "cas"
+#               Can be used to include any line in the Condor-G file
+#               that is not otherwise added programmatically by AutoPyFactory.
+#               Note the following directives are added by default:
+#
+#                       transfer_executable = True
+#                       stream_output=False
+#                       stream_error=False
+#                       notification=Error
+#                       copy_to_spool = false
+#
+# batchsubmit.condorgt5.environ = list of environment variables, 
+#               splitted by white spaces, 
+#               to be included in the condor attribute environment *verbatim*
+#               Therefore, the format should be env1=var1 env2=var2 envN=varN
+#               split by whitespaces.
+#
+# batchsubmit.condorgt5.proxy = name of the proxy handler in proxymanager for automatic proxy renewal 
+#               (See etc/proxy.conf)
+#               None if no automatic proxy renewal is desired.
+
+#  --------------------------------------------------------------------
+#       GlobusRSL GRAM5 variables
+#  --------------------------------------------------------------------
+
+# gram5 = The following are GRAM5 RSL variables. 
+#               They are just used to build batchsubmit.condorgt5.globusrsl 
+#               (if needed)
+#               The globusrsl directive in the condor submission file looks like
+#
+#                   globusrsl=(jobtype=single)(queue=short)
+#
+#               Documentation can be found here:
+#
+#                      http://www.globus.org/toolkit/docs/5.2/5.2.0/gram5/user/#gram5-user-rsl 
+#
+# globusrsl.gram5.arguments =
+# globusrsl.gram5.count =
+# globusrsl.gram5.directory =
+# globusrsl.gram5.dry_run =
+# globusrsl.gram5.environment =
+# globusrsl.gram5.executable =
+# globusrsl.gram5.file_clean_up =
+# globusrsl.gram5.file_stage_in =
+# globusrsl.gram5.file_stage_in_shared =
+# globusrsl.gram5.file_stage_out =
+# globusrsl.gram5.gass_cache =
+# globusrsl.gram5.gram_my_job =
+# globusrsl.gram5.host_count =
+# globusrsl.gram5.job_type =
+# globusrsl.gram5.library_path =
+# globusrsl.gram5.loglevel =
+# globusrsl.gram5.logpattern =
+# globusrsl.gram5.max_cpu_time =
+# globusrsl.gram5.max_memory =
+# globusrsl.gram5.max_time =
+# globusrsl.gram5.max_wall_time =
+# globusrsl.gram5.min_memory =
+# globusrsl.gram5.project =
+# globusrsl.gram5.proxy_timeout =
+# globusrsl.gram5.queue =
+# globusrsl.gram5.remote_io_url =
+# globusrsl.gram5.restart =
+# globusrsl.gram5.rsl_substitution =
+# globusrsl.gram5.savejobdescription =
+# globusrsl.gram5.save_state =
+# globusrsl.gram5.scratch_dir =
+# globusrsl.gram5.stderr =
+# globusrsl.gram5.stderr_position =
+# globusrsl.gram5.stdin =
+# globusrsl.gram5.stdout =
+# globusrsl.gram5.stdout_position =
+# globusrsl.gram5.two_phase =
+# globusrsl.gram5.username =
+#
+# globusrsl.gram5.globusrsl = GRAM RSL directive.
+#               If this variable is not setup, then it will be built
+#               programmatically from all non empty globusrsl.gram5.XYZ variables.
+#               If this variable is setup, then its value
+#               will be taken *verbatim*, and all possible values
+#               for globusrsl.gram5.XYZ variables will be ignored. 
+#
+# globusrsl.gram5.globusrsladd = custom fields to be added
+#               *verbatim* to the GRAM RSL directive,
+#               after it has been built either from 
+#               globusrsl.gram5.globusrsl value
+#               or from all globusrsl.gram5.XYZ variables.
+#               e.g. (condorsubmit=('+AccountingGroup' '\"group_atlastest.usatlas1\"')('+Requirements' 'True'))
+#
+
+#  --------------------------------------------------------------------
+#       Configuration when batchsubmitplugin is condorcream
+#  --------------------------------------------------------------------
+
+# batchsubmit.condorcream.webservice = web service address (e.g. ce04.esc.qmul.ac.uk:8443/ce-cream/services/CREAM2)
+#
+# batchsubmit.condorcream.submitargs = list of command line input options
+#               to be included in the submission command *verbatim*
+#               e.g. 
+#                   batchsubmit.condorgt2.submitargs = -remote my_schedd 
+#               will drive into a command like
+#                   condor_submit -remote my_schedd submit.jdl
+#
+# batchsubmit.condorcream.condor_attributes = list of condor attributes, 
+#               splited by comma, 
+#               to be included in the condor submit file *verbatim*
+#               e.g. +Experiment = "ATLAS",+VO = "usatlas",+Job_Type = "cas"
+#               Can be used to include any line in the Condor-G file
+#               that is not otherwise added programmatically by AutoPyFactory.
+#               Note the following directives are added by default:
+#
+#                       transfer_executable = True
+#                       stream_output=False
+#                       stream_error=False
+#                       notification=Error
+#                       copy_to_spool = false
+#
+# batchsubmit.condorcream.environ = list of environment variables, 
+#               splitted by white spaces, 
+#               to be included in the condor attribute environment *verbatim*
+#               Therefore, the format should be env1=var1 env2=var2 envN=varN
+#               split by whitespaces.
+#
+# batchsubmit.condorcream.queue = queue within the local batch system (e.g. short)
+#
+# batchsubmit.condorcream.port = port number.
+#
+# batchsubmit.condorcream.batch = local batch system (pbs, sge...)
+#
+# batchsubmit.condorcream.gridresource = grid resource, built from other vars using interpolation:
+#               batchsubmit.condorcream.gridresource = %(batchsubmit.condorcream.webservice)s:%(batchsubmit.condorcream.port)s/ce-cream/services/CREAM2 %(batchsubmit.condorcream.batch)s %(batchsubmit.condorcream.queue)s
+#
+# batchsubmit.condorcream.proxy = name of the proxy handler in proxymanager for automatic proxy renewal 
+#               (See etc/proxy.conf)
+#               None if no automatic proxy renewal is desired.
+
+#  --------------------------------------------------------------------
+#       Configuration when batchsubmitplugin is condorosgce
+#  --------------------------------------------------------------------
+
+# batchsubmit.condorosgce.remote_condor_schedd = condor schedd 
+#
+# batchsubmit.condorosgce.remote_condor_collector =  condor collector
+#
+# batchsubmit.condorosgce.gridresource = grid resource, built from other vars using interpolation
+#               batchsubmit.condorosgce.gridresource = %(batchsubmit.condorosgce.remote_condor_schedd) %(batchsubmit.condorosgce.remote_condor_collector)    
+# batchsubmit.condorosgce.proxy = name of the proxy handler in proxymanager for automatic proxy renewal 
+#               (See etc/proxy.conf)
+#               None if no automatic proxy renewal is desired.
+
+#  --------------------------------------------------------------------
+#       Configuration when batchsubmitplugin is condorec2
+#  --------------------------------------------------------------------
+
+# batchsubmit.condorec2.gridresource = ec2 service's URL (e.g. https://ec2.amazonaws.com/ )
+#
+# batchsubmit.condorec2.submitargs = list of command line input options
+#               to be included in the submission command *verbatim*
+#               e.g. 
+#                   batchsubmit.condorgt2.submitargs = -remote my_schedd 
+#               will drive into a command like
+#                   condor_submit -remote my_schedd submit.jdl
+#
+# batchsubmit.condorec2.condor_attributes = list of condor attributes, 
+#               splited by comma, 
+#               to be included in the condor submit file *verbatim*
+#
+# batchsubmit.condorec2.environ = list of environment variables, 
+#               splitted by white spaces, 
+#               to be included in the condor attribute environment *verbatim*
+#               Therefore, the format should be env1=var1 env2=var2 envN=varN
+#               split by whitespaces.
+#
+# batchsubmit.condorec2.ami_id = identifier for the VM image, 
+#               previously registered in one of Amazon's storage service (S3 or EBS)
+#
+# batchsubmit.condorec2.ec2_spot_price = max price to pay, in dollars to three decimal places. e.g. .040
+#
+# batchsubmit.condorec2.instance_type = hardware configurations for instances to run on, .e.g m1.medium
+#
+# batchsubmit.condorec2.user_data = up to 16Kbytes of contextualization data.
+#               This makes it easy for many instances to share the same VM image, but perform different work.
+#
+# batchsubmit.condorec2.access_key_id = path to file with the EC2 Access Key ID
+#
+# batchsubmit.condorec2.secret_access_key = path to file with the EC2 Secret Access Key
+#
+# batchsubmit.condorec2.proxy = name of the proxy handler in proxymanager for automatic proxy renewal 
+#               (See etc/proxy.conf)
+#               None if no automatic proxy renewal is desired.
+
+#  --------------------------------------------------------------------
+#       Configuration when batchsubmitplugin is condordeltacloud
+#  --------------------------------------------------------------------
+
+# batchsubmit.condordeltacloud.gridresource = ec2 service's URL (e.g. https://deltacloud.foo.org/api )
+#
+# batchsubmit.condordeltacloud.username = credentials in DeltaCloud
+#
+# batchsubmit.condordeltacloud.password_file = path to the file with the password
+#
+# batchsubmit.condordeltacloud.image_id = identifier for the VM image,
+#               previously registered with the cloud service.
+#
+# batchsubmit.condordeltacloud.keyname = in case of using SSH, 
+#               the command keyname specifies the identifier of the SSH key pair to use. 
+#
+# batchsubmit.condordeltacloud.realm_id = selects one between multiple locations the cloud service may have.
+#
+# batchsubmit.condordeltacloud.hardware_profile = selects one between the multiple hardware profiles
+#               the cloud service may provide
+#
+# batchsubmit.condordeltacloud.hardware_profile_memory = customize the hardware profile
+#
+# batchsubmit.condordeltacloud.hardware_profile_cpu = customize the hardware profile
+#
+# batchsubmit.condordeltacloud.hardware_profile_storage = customize the hardware profile
+#
+# batchsubmit.condordeltacloud.user_data = contextualization data
+
+#  --------------------------------------------------------------------
+#       Configuration when batchsubmitplugin is condorlocal
+#  --------------------------------------------------------------------
+
+# batchsubmit.condorlocal.submitargs = list of command line input options
+#               to be included in the submission command *verbatim*
+#               e.g. 
+#                   batchsubmit.condorgt2.submitargs = -remote my_schedd 
+#               will drive into a command like
+#                   condor_submit -remote my_schedd submit.jdl
+#
+# batchsubmit.condorlocal.condor_attributes = list of condor attributes, 
+#               splited by comma, 
+#               to be included in the condor submit file *verbatim*
+#               e.g. +Experiment = "ATLAS",+VO = "usatlas",+Job_Type = "cas"
+#               Can be used to include any line in the Condor-G file
+#               that is not otherwise added programmatically by AutoPyFactory.
+#               Note the following directives are added by default:
+#
+#                       universe = vanilla
+#                       transfer_executable = True
+#                       should_transfer_files = IF_NEEDED
+#                       +TransferOutput = ""
+#                       stream_output=False
+#                       stream_error=False
+#                       notification=Error
+#                       periodic_remove = (JobStatus == 5 && (CurrentTime - EnteredCurrentStatus) > 3600) || (JobStatus == 1 && globusstatus =!= 1 && (CurrentTime - EnteredCurrentStatus) > 86400)
+#
+#               To be used in CondorLocal Batch Submit Plugin.
+#
+# batchsubmit.condorlocal.environ = list of environment variables, 
+#               splitted by white spaces, 
+#               to be included in the condor attribute environment *verbatim*
+#               To be used by CondorLocal Batch Submit Plugin.
+#               Therefore, the format should be env1=var1 env2=var2 envN=varN
+#               split by whitespaces.
+#
+# batchsubmit.condorlocal.proxy = name of the proxy handler in proxymanager for automatic proxy renewal 
+#               (See etc/proxy.conf)
+#               None if no automatic proxy renewal is desired.
+#
+
+
+#  --------------------------------------------------------------------
+#       Configuration when batchsubmitplugin is condorlsf
+#  --------------------------------------------------------------------
+
+# batchsubmit.condorlsf.proxy = name of the proxy handler in proxymanager for automatic proxy renewal 
+#               (See etc/proxy.conf)
+#               None if no automatic proxy renewal is desired.
+
+#  --------------------------------------------------------------------
+#       Configuration when batchsubmitplugin is nordugrid 
+#  --------------------------------------------------------------------
+
+# batchsubmit.condornordugrid.gridresource = name of the ARC CE
+#               i.e. lcg-lrz-ce2.grid.lrz.de
+#
+# nordugridrsl = Entire RSL line.
+#               i.e. (jobname = 'prod_pilot')(queue=lcg)(runtimeenvironment = APPS/HEP/ATLAS-SITE-LCG)(runtimeenvironment = ENV/PROXY ) (environment = ('APFFID' 'voatlas94') ('PANDA_JSID' 'voatlas94') ('GTAG' 'http://voatlas94.cern.ch/pilots/2012-11-19/LRZ-LMU_arc/$(Cluster).$(Process).out') ('RUCIO_ACCOUNT' 'pilot') ('APFCID' '$(Cluster).$(Process)') ('APFMON' 'http://apfmon.lancs.ac.uk/mon/') ('FACTORYQUEUE' 'LRZ-LMU_arc') 
+#
+# nordugridrsladd = A given tag to be added to the Nordugrid RSL line
+#
+# nordugridrsl.addenv.<XYZ> = A given tag to be added within the 'environment' tag to the Nordugrid RSL line
+#               i.e. nordugridrsl.addenv.RUCIO_ACCOUNT = pilot
+#                    will be added as ('RUCIO_ACCOUNT' 'pilot' )
+
+#  --------------------------------------------------------------------
+#       Monitor Section 
+#  --------------------------------------------------------------------
+
+# monitorsection = section in monitor.conf where info 
+#               about the actual monitor plugin can be found.
+#               The value can be a single section or a split by comma
+#               list of sections.
+#               Monitor plugins handle job info publishing 
+#               to one or more web monitor/dashboards. 
+#               To specify more than one (sections) 
+#               simply use a comma-separated list.   
+#
+
+#  --------------------------------------------------------------------
+#       Executable variables 
+#  --------------------------------------------------------------------
+
+# executable = path to the script which will be run by condor. 
+#               The executable can be anything, however, 
+#               two possible executables are distributed with AutoPyFactory:
+#
+#                       - libexec/wrapper.sh 
+#                       - libexec/runpilot3-wrapper.sh 
+#
+# executable.arguments = input options to be passed verbatim to the executable script.
+#               This variable can be built making use of an auxiliar variable
+#               called executable.defaultarguments
+#               This proposed ancilla works as a template, and its content is
+#               created on the fly from the value of other variables.
+#               This mechanism is called "interpolation", docs can be found here:
+#
+#                   http://docs.python.org/library/configparser.html
+#
+#               These are two examples of this type of templates 
+#               (included in the DEFAULTS block):
+#
+#                   executable.defaultarguments = --wrappergrid=%(grid)s \
+#                               --wrapperwmsqueue=%(wmsqueue)s \
+#                               --wrapperbatchqueue=%(batchqueue)s \
+#                               --wrappervo=%(vo)s \
+#                               --wrappertarballurl=http://dev.racf.bnl.gov/dist/wrapper/wrapper.tar.gz \
+#                               --wrapperserverurl=http://pandaserver.cern.ch:25080/cache/pilot \
+#                               --wrapperloglevel=debug
+# 
+#                   executable.defaultarguments =  -s %(wmsqueue)s \
+#                               -h %(batchqueue)s -p 25443 \
+#                               -w https://pandaserver.cern.ch  -j false  -k 0  -u user
+#
+# =========================================================================== 
+
+
+[DEFAULT]
+
+grid = OSG
+vo = ATLAS
+cloud = US
+country = US
+group = None
+status = online
+
+cleanlogs.keepdays = 7
+
+# plugins
+batchstatusplugin = Condor
+wmsstatusplugin = Panda
+batchsubmitplugin = CondorGT2
+schedplugin = Ready 
+monitorsection = apfmon-lancaster
+
+
+
+sched.trivial.default = 0
+sched.simple.default = 0
+sched.activated.default = 0
+# defaults for testmode
+sched.activated.testmode.allowed = True
+sched.activated.testmode.pilots = 5
+
+# proxy = atlas-usatlas
+batchsubmit.condorgt2.proxy = None
+batchsubmit.condorgt5.proxy = None
+batchsubmit.condorcream.proxy = None
+batchsubmit.condorec2.proxy = None
+batchsubmit.condorlocal.proxy = None
+
+# gram and globusrsl
+#   jobtype and queue are given a default value.
+
+globusrsl.gram2.arguments = ""
+globusrsl.gram2.count = ""
+globusrsl.gram2.directory = ""
+globusrsl.gram2.dryRun = ""
+globusrsl.gram2.environment = ""
+globusrsl.gram2.executable = ""
+globusrsl.gram2.gramMyJob = ""
+globusrsl.gram2.hostCount = ""
+globusrsl.gram2.jobType = single
+globusrsl.gram2.maxCpuTime = ""
+globusrsl.gram2.maxMemory = ""  
+globusrsl.gram2.maxTime = ""
+globusrsl.gram2.maxWallTime = ""
+globusrsl.gram2.minMemory = "" 
+globusrsl.gram2.project = ""
+globusrsl.gram2.queue = short
+globusrsl.gram2.remote_io_url = ""
+globusrsl.gram2.restart = ""
+globusrsl.gram2.save_state = ""
+globusrsl.gram2.stderr = ""
+globusrsl.gram2.stderr_position = ""
+globusrsl.gram2.stdin = ""
+globusrsl.gram2.stdout = ""
+globusrsl.gram2.stdout_position = ""
+globusrsl.gram2.two_phase = ""
+
+globusrsl.gram5.arguments = ""
+globusrsl.gram5.count = ""
+globusrsl.gram5.directory = ""
+globusrsl.gram5.dry_run = ""
+globusrsl.gram5.environment = ""
+globusrsl.gram5.executable = ""
+globusrsl.gram5.file_clean_up = ""
+globusrsl.gram5.file_stage_in = ""
+globusrsl.gram5.file_stage_in_shared = ""
+globusrsl.gram5.file_stage_out = ""
+globusrsl.gram5.gass_cache = ""
+globusrsl.gram5.gram_my_job = ""
+globusrsl.gram5.host_count = ""
+globusrsl.gram5.job_type = single
+globusrsl.gram5.library_path = ""
+globusrsl.gram5.loglevel = ""
+globusrsl.gram5.logpattern = ""
+globusrsl.gram5.max_cpu_time = ""
+globusrsl.gram5.max_memory = ""
+globusrsl.gram5.max_time = ""
+globusrsl.gram5.max_wall_time = ""
+globusrsl.gram5.min_memory = ""
+globusrsl.gram5.project = ""
+globusrsl.gram5.proxy_timeout = ""
+globusrsl.gram5.queue = single
+globusrsl.gram5.remote_io_url = ""
+globusrsl.gram5.restart = ""
+globusrsl.gram5.rsl_substitution = ""
+globusrsl.gram5.savejobdescription = ""
+globusrsl.gram5.save_state = ""
+globusrsl.gram5.scratch_dir = ""
+globusrsl.gram5.stderr = ""
+globusrsl.gram5.stderr_position = ""
+globusrsl.gram5.stdin = ""
+globusrsl.gram5.stdout = ""
+globusrsl.gram5.stdout_position = ""
+globusrsl.gram5.two_phase = ""
+globusrsl.gram5.username = ""
+
+
+periodic_hold = periodic_hold=GlobusResourceUnavailableTime =!= UNDEFINED &&(CurrentTime-GlobusResourceUnavailableTime>30)
+periodic_remove = periodic_remove=(JobStatus == 5 && (CurrentTime - EnteredCurrentStatus) > 3600) || (JobStatus == 1 && globusstatus =!= 1 && (CurrentTime - EnteredCurrentStatus) > 86400) || (JobStatus == 2 && (CurrentTime - EnteredCurrentStatus) > 604800)
+batchsubmit.condorgt2.condor_attributes = %(periodic_hold)s,%(periodic_remove)s 
+
+apfqueue.sleep = 360
+
+# The following are valid for wrapper.sh
+executable = /usr/libexec/wrapper.sh
+executable.defaultarguments = --wrappergrid=%(grid)s --wrapperwmsqueue=%(wmsqueue)s --wrapperbatchqueue=%(batchqueue)s --wrappervo=%(vo)s --wrappertarballurl=http://dev.racf.bnl.gov/dist/wrapper/wrapper.tar.gz --wrapperserverurl=http://pandaserver.cern.ch:25080/cache/pilot --wrapperloglevel=debug
+
+# The following are valid for runpilot3.sh
+#executable = /usr/libexec/runpilot3.sh
+#executable.defaultarguments =  -s %(wmsqueue)s -h %(batchqueue)s -p 25443 -w https://pandaserver.cern.ch  -j false  -k 0  -u user
+
+enabled = True
+
+
+# ====================================================================== 
+#               Individual queue configurations
+# ====================================================================== 
+
+# ---------------------------------------------------------------------- 
+#               Local Condor example
+# ---------------------------------------------------------------------- 
+
+[ANALY_BNL_CLOUD-sl6]
+enabled = False
+
+wmsqueue = ANALY_BNL_CLOUD
+batchqueue = ANALY_BNL_CLOUD
+
+batchstatusplugin = Condor
+batchsubmit = CondorLocal
+
+batchsubmit.condorlocal.condor_attributes =  Requirements = ( Arch == "X86_64" && OpSysAndVer == "SL6" && NodeType == "atlas" ) ,request_memory = 1699 ,+AccountingGroup = "group_analy.apf"
+batchsubmit.condorlocal.proxy = atlas-production
+
+schedplugin = Ready, MinPerCycle, MaxPerCycle, MaxPending
+sched.minpercycle.minimum = 10
+sched.maxpercycle.maximum = 20
+sched.maxpending.maximum = 50
+
+# These arguments are passed through to the payload job (e.g., the Panda pilot). 
+executable.arguments = %(executable.defaultarguments)s --script=pilot.py --libcode=pilotcode.tar.gz,pilotcode-rc.tar.gz --pilotsrcurl=http://panda.cern.ch:25880/cache --user user
+
+
+
+# ---------------------------------------------------------------------- 
+#               GRAM example
+# ---------------------------------------------------------------------- 
+
+[ANALY_BNL_ATLAS_1]
+enabled = False
+
+wmsqueue = ANALY_BNL_ATLAS_1
+batchqueue = ANALY_BNL_ATLAS_1-condor
+
+batchsubmit.condorgt2.gridresource = gridgk05.racf.bnl.gov/jobmanager-condor
+batchsubmit.condorgt2.queue = short
+batchsubmit.condorgt2.condor_attributes = periodic_remove = (JobStatus == 5 && (CurrentTime - EnteredCurrentStatus) > 3600) || (JobStatus == 1 && globusstatus =!= 1 && (CurrentTime - EnteredCurrentStatus) > 86400)
+batchsubmit.condorgt2.proxy = atlas-production
+
+schedplugin = Ready, MinPerCycle, MaxPerCycle, MaxPending
+sched.minpercycle.minimum = 10
+sched.maxpercycle.maximum = 20
+sched.maxpending.maximum = 50
+
+# These arguments are passed through to the payload job (e.g., the Panda pilot). 
+executable.arguments = %(executable.defaultarguments)s --script=pilot.py --libcode=pilotcode.tar.gz,pilotcode-rc.tar.gz --pilotsrcurl=http://panda.cern.ch:25880/cache --user user
+
+# If using runpilot3.sh, you only need the defaultarguments, as it seems the payload args are not necessary. 
+# executable.arguments = %(executable.defaultarguments)s
+
+
+# ---------------------------------------------------------------------- 
+#               CREAM example
+# ---------------------------------------------------------------------- 
+
+[RAL-LCG2-lcgce04-grid3000M-pbs-3379]
+enabled = False
+
+batchqueue = RAL-LCG2-lcgce04-grid3000M-pbs
+wmsqueue = RAL-LCG2
+
+batchsubmitplugin = CondorCREAM
+
+batchsubmit.condorcream.webservice = lcgce04.gridpp.rl.ac.uk
+batchsubmit.condorcream.port = 8443
+batchsubmit.condorcream.batch = pbs
+batchsubmit.condorcream.queue = grid3000M 
+batchsubmit.condorcream.condor_attributes = periodic_remove = (JobStatus == 5 && (CurrentTime - EnteredCurrentStatus) > 3600) || (JobStatus == 1 && globusstatus =!= 1 && (CurrentTime - EnteredCurrentStatus) > 86400)
+batchsubmit.condorcream.proxy = atlas-production
+
+schedplugin = Ready, MinPerCycle, MaxPerCycle, MaxPending
+sched.minpercycle.minimum = 10
+sched.maxpercycle.maximum = 20
+sched.maxpending.maximum = 50
+
+# These arguments are passed through to the payload job (e.g., the Panda pilot). 
+executable.arguments = %(executable.defaultarguments)s --script=pilot.py --libcode=pilotcode.tar.gz,pilotcode-rc.tar.gz --pilotsrcurl=http://panda.cern.ch:25880/cache --user user
+
+# If using runpilot3.sh, you only need the defaultarguments, as it seems the payload args are not necessary. 
+# executable.arguments = %(executable.defaultarguments)s
+
+
+# ---------------------------------------------------------------------- 
+#               Nordugrid example
+# ---------------------------------------------------------------------- 
+
+[ANALY_LRZ]
+enabled = False
+
+wmsqueue = ANALY_LRZ 
+batchqueue = ANALY_LRZ
+
+batchsubmitplugin = CondorNordugrid
+batchsubmit.condornordugrid.gridresource = lcg-lrz-ce2.grid.lrz.de 
+nordugridrsl.jobname = 'analy_pilot'
+nordugridrsl.queue = lcg
+nordugridrsl.nordugridrsladd = (runtimeenvironment = APPS/HEP/ATLAS-SITE-LCG)(runtimeenvironment = ENV/PROXY )
+nordugridrsl.addenv.RUCIO_ACCOUNT = pilot
+
+schedplugin = Ready, MinPerCycle, MaxPerCycle, MaxPending
+sched.minpercycle.minimum = 10
+sched.maxpercycle.maximum = 20
+sched.maxpending.maximum = 50
+
+executable.arguments = %(executable.defaultarguments)s --script=pilot.py --libcode=pilotcode.tar.gz,pilotcode-rc.tar.gz --pilotsrcurl=http://panda.cern.ch:25880/cache --user user
diff --git a/etc/queues.conf-example b/etc/queues.conf-example
deleted file mode 100644
index f5580a9..0000000
--- a/etc/queues.conf-example
+++ /dev/null
@@ -1,1119 +0,0 @@
-#
-# queues.conf  Configuration file for APFQueue component of AutoPyFactory.
-#
-
-## Defaults for queues - these values are set when there is not an explicit value
-## If you don't set them here the factory takes sensible default values, so nothing is mandatory
-## see ConfigLoader._configurationDefaults() for these values. 
-#
-# Some of these values may be in the process of deprecation, especially submission parameters 
-# which are now handled by the submit plugins. 
-
-# =========================================================================== 
-#               VARIABLES
-# =========================================================================== 
-
-#
-# grid
-# vo
-# cloud
-# batchqueue 
-# wmsqueue 
-# enabled
-# status
-# apfqueue.sleep
-#
-# cleanlogs.keepdays
-#
-# wmsstatusplugin
-#
-# batchstatusplugin
-# batchstatus.condor.queryargs
-#
-# schedplugin
-# sched.activated.default
-# sched.activated.max_jobs_torun
-# sched.activated.max_pilots_per_cycle
-# sched.activated.min_pilots_per_cycle
-# sched.activated.min_pilots_pending
-# sched.activated.max_pilots_pending
-# sched.activated.testmode.allowed
-# sched.activated.testmode.pilots
-# sched.ready.offset
-# sched.fixed.pilotspercycle
-# sched.maxpercycle.maximum
-# sched.minpercycle.minimum
-# sched.maxpending.maximum
-# sched.minpending.minimum
-# sched.maxtorun.maximum
-# sched.statustest.pilots
-# sched.statusoffline.pilots
-# sched.simple.default
-# sched.simple.maxpendingpilots
-# sched.simple.maxpilotspercycle
-# sched.trivial.default
-# sched.scale.factor
-# sched.keepnrunning.keep_running
-#
-# batchsubmitplugin
-# batchsubmit.condorgt2.submitargs
-# batchsubmit.condorgt2.gridresource
-# batchsubmit.condorgt2.condor_attributes
-# batchsubmit.condorgt2.environ
-# batchsubmit.condorgt2.proxy
-#
-# globusrsl.gram2.arguments
-# globusrsl.gram2.count
-# globusrsl.gram2.directory
-# globusrsl.gram2.dryRun
-# globusrsl.gram2.environment
-# globusrsl.gram2.executable
-# globusrsl.gram2.gramMyJob
-# globusrsl.gram2.hostCount
-# globusrsl.gram2.jobType
-# globusrsl.gram2.maxCpuTime
-# globusrsl.gram2.maxMemory
-# globusrsl.gram2.maxTime
-# globusrsl.gram2.maxWallTime
-# globusrsl.gram2.minMemory
-# globusrsl.gram2.project
-# globusrsl.gram2.queue
-# globusrsl.gram2.remote_io_url
-# globusrsl.gram2.restart
-# globusrsl.gram2.save_state
-# globusrsl.gram2.stderr
-# globusrsl.gram2.stderr_position
-# globusrsl.gram2.stdin
-# globusrsl.gram2.stdout
-# globusrsl.gram2.stdout_position
-# globusrsl.gram2.two_phase
-# globusrsl.gram2.globusrsl
-# globusrsl.gram2.globusrsladd
-#
-# batchsubmit.condorgt5.submitargs
-# batchsubmit.condorgt5.gridresource
-# batchsubmit.condorgt5.condor_attributes
-# batchsubmit.condorgt5.environ
-# batchsubmit.condorgt5.proxy
-#
-# globusrsl.gram5.arguments
-# globusrsl.gram5.count
-# globusrsl.gram5.directory
-# globusrsl.gram5.dry_run
-# globusrsl.gram5.environment
-# globusrsl.gram5.executable
-# globusrsl.gram5.file_clean_up
-# globusrsl.gram5.file_stage_in
-# globusrsl.gram5.file_stage_in_shared
-# globusrsl.gram5.file_stage_out
-# globusrsl.gram5.gass_cache
-# globusrsl.gram5.gram_my_job
-# globusrsl.gram5.host_count
-# globusrsl.gram5.job_type
-# globusrsl.gram5.library_path
-# globusrsl.gram5.loglevel
-# globusrsl.gram5.logpattern
-# globusrsl.gram5.max_cpu_time
-# globusrsl.gram5.max_memory
-# globusrsl.gram5.max_time
-# globusrsl.gram5.max_wall_time
-# globusrsl.gram5.min_memory
-# globusrsl.gram5.project
-# globusrsl.gram5.proxy_timeout
-# globusrsl.gram5.queue
-# globusrsl.gram5.remote_io_url
-# globusrsl.gram5.restart
-# globusrsl.gram5.rsl_substitution
-# globusrsl.gram5.savejobdescription
-# globusrsl.gram5.save_state
-# globusrsl.gram5.scratch_dir
-# globusrsl.gram5.stderr
-# globusrsl.gram5.stderr_position
-# globusrsl.gram5.stdin
-# globusrsl.gram5.stdout
-# globusrsl.gram5.stdout_position
-# globusrsl.gram5.two_phase
-# globusrsl.gram5.username
-# globusrsl.gram5.globusrsl
-# globusrsl.gram5.globusrsladd
-#
-# batchsubmit.condorcream.submitargs
-# batchsubmit.condorcream.webservice
-# batchsubmit.condorcream.gridresource
-# batchsubmit.condorcream.condor_attributes
-# batchsubmit.condorcream.environ
-# batchsubmit.condorcream.queue
-# batchsubmit.condorcream.port
-# batchsubmit.condorcream.batch
-# batchsubmit.condorcream.proxy
-#
-# batchsubmit.condorosgce.remote_condor_schedd 
-# batchsubmit.condorosgce.remote_condor_collector
-# batchsubmit.condorosgce.gridresource
-# batchsubmit.condorosgce.proxy
-#
-# batchsubmit.condorec2.submitargs
-# batchsubmit.condorec2.gridresource
-# batchsubmit.condorec2.condor_attributes
-# batchsubmit.condorec2.environ
-# batchsubmit.condorec2.ami_id
-# batchsubmit.condorec2.instance_type
-# batchsubmit.condorec2.user_data
-# batchsubmit.condorec2.access_key_id
-# batchsubmit.condorec2.secret_access_key
-# batchsubmit.condorec2.proxy
-#
-# batchsubmit.condordeltacloud.gridresource
-# batchsubmit.condordeltacloud.username
-# batchsubmit.condordeltacloud.password_file
-# batchsubmit.condordeltacloud.image_id
-# batchsubmit.condordeltacloud.keyname
-# batchsubmit.condordeltacloud.realm_id
-# batchsubmit.condordeltacloud.hardware_profile
-# batchsubmit.condordeltacloud.hardware_profile_memory
-# batchsubmit.condordeltacloud.hardware_profile_cpu
-# batchsubmit.condordeltacloud.hardware_profile_storage
-# batchsubmit.condordeltacloud.user_data
-#
-# batchsubmit.condorlocal.submitargs
-# batchsubmit.condorlocal.condor_attributes
-# batchsubmit.condorlocal.environ
-# batchsubmit.condorlocal.proxy
-#
-# batchsubmit.condorlsf.proxy
-#
-# batchsubmit.condornordugrid.gridresource
-# nordugridrsl
-# nordugridrsladd
-# nordugridrsl.addenv.
-#
-# monitorsection
-#
-# executable
-# executable.arguments
-
-# =========================================================================== 
-#               DESCRIPTION 
-# =========================================================================== 
-
-#  --------------------------------------------------------------------
-#       Generic variables
-#  --------------------------------------------------------------------
-
-# cloud = is the cloud this queue is in. You should set this to suppress pilot 
-#               submission when the cloud goes offline
-#               N.B. Panda clouds are UPPER CASE, e.g., UK
-#
-# vo = Virtual Organization
-#
-# grid = Grid middleware flavor at the site. (e.g. OSG, EGI, NorduGrid) 
-#
-# batchqueue = the Batch system related queue name. 
-#               E.g. the PanDA queue name (formerly called nickname)
-#
-# wmsqueue = the WMS system queue name. 
-#               E.g. the PanDA siteid name
-#
-# enabled = determines if each queue section must be used by AutoPyFactory
-#               or not. Allows to disable a queue without commenting out all the values. 
-#               Valid values are True|False.
-#
-# status = can be "test", "offline" or "online"
-#
-# apfqueue.sleep = sleep time between cycles in APFQueue object.
-#               Value is in seconds.   
-#
-# cleanlogs.keepdays = maximum number of days the condor logs
-#               will be kept
-
-#  --------------------------------------------------------------------
-#       WMS Status Plugin variables 
-#  --------------------------------------------------------------------
-
-# wmsstatusplugin = WMS Status Plugin.
-
-#  --------------------------------------------------------------------
-#       Batch Status Plugin variables 
-#  --------------------------------------------------------------------
-
-# batchstatusplugin = Batch Status Plugin.
-#
-# batchstatus.condor.queryargs = list of command line input options
-#               to be included in the query command *verbatim*. E.g. 
-#               batchstatus.condor.query = -name <schedd_name> -pool <centralmanagerhostname[:portnumber]>
-
-#  --------------------------------------------------------------------
-#       Sched Plugin variables 
-#  --------------------------------------------------------------------
-
-# schedplugin = specific Scheduler Plugin implementing
-#               the algorithm deciding how many new pilots
-#               to submit next cycle.
-#               The value can be a single Plugin or a split by comma
-#               list of Plugins.
-#               In the case of more than one plugin, 
-#               each one will acts as a filter with respect to the
-#               value returned by the previous one.
-#               By selecting the right combination of Plugins in a given order,
-#               a complex algorithm can be built.
-#               E.g., the algorithm can start by using Ready Plugin,
-#               which will determine the number of pilots based on 
-#               the number of activated jobs in the WMS queue and 
-#               the number of already submitted pilots.
-#               After that, this number can be filtered to 
-#               a maximum (MaxPerCycleSchedPlugin) or a minimum (MinPerCycleSchedPlugin)
-#               number of pilots.
-#               Or even can be filtered to a maximum number of pilots
-#               per factory (MaxPerFactorySchedPlugin)
-#               Also it can be filtered depending on the status of the wmsqueue 
-#               (StatusTestSchedPlugin, StatusOfflineSchedPlugin).
-
-
-#  --------------------------------------------------------------------
-#       Configuration when schedplugin is Activated
-#  --------------------------------------------------------------------
-
-#  IMPORTANT NOTE: Activated plugin is decommissioned. 
-#                  It is not maintained anymore.
-
-# sched.activated.default = default number of pilots to be submitted
-#               when the context information 
-#               does not exist is not reliable 
-#               To be used in Activated Scheduler Plugin.
-#
-# sched.activated.max_jobs_torun = maximum number of jobs running
-#               simoultaneously. 
-#               To be used in Activated Scheduler Plugin.
-#
-# sched.activated.max_pilots_per_cycle = maximum number of pilots
-#               to be submitted per cycle.
-#               To be used in Activated Scheduler Plugin.
-#
-# sched.activated.min_pilots_per_cycle = minimum number of pilots
-#               to be submitted per cycle.
-#               To be used in Activated Scheduler Plugin.
-#
-# sched.activated.min_pilots_pending = minimum number of pilots
-#               to be idle on queue waiting to start execution.
-#               To be used in Activated Scheduler Plugin.
-#
-# sched.activated.max_pilots_pending = maximum number of pilots
-#               to be idle on queue waiting to start execution.
-#               To be used in Activated Scheduler Plugin.
-#
-# sched.activated.testmode.allowed = Boolean variable to trigger
-#               special mode of operation when the wmsqueue is in
-#               in status = test
-#
-# sched.activated.testmode.pilots = number of pilots to submit
-#               when the wmsqueue is in status = test
-#               and sched.activated.testmode.allowed is True
-#
-# sched.activated.testmode.max_pending = maximum number of pilots 
-#               permitted in pendig state when the wmsqueue is in 
-#               status = test and sched.activated.testmode.allowed is True
-#
-
-#  --------------------------------------------------------------------
-#       Configuration when schedplugin is Ready 
-#  --------------------------------------------------------------------
-
-#  sched.ready.offset = the minimum value in the number of ready jobs
-#               to trigger submission. 
-
-#  --------------------------------------------------------------------
-#       Configuration when schedplugin is Fixed
-#  --------------------------------------------------------------------
-
-# sched.fixed.pilotspercycle = fixed number of pilots to be submitted
-#               each cycle, when using the Fixed Scheduler Plugin.
-
-
-#  --------------------------------------------------------------------
-#       Configuration when schedplugin is MaxPerCycle 
-#  --------------------------------------------------------------------
-
-# sched.maxpercycle.maximum = maximum number of pilots to be submitted
-#               per cycle
-
-
-#  --------------------------------------------------------------------
-#       Configuration when schedplugin is MinPerCycle 
-#  --------------------------------------------------------------------
-
-# sched.minpercycle.minimum = minimum number of pilots to be submitted
-#               per cycle
-
-
-#  --------------------------------------------------------------------
-#       Configuration when schedplugin is MaxPending
-#  --------------------------------------------------------------------
-
-# sched.maxpending.maximum = maximum number of pilots to be pending
-#             
-
-#  --------------------------------------------------------------------
-#       Configuration when schedplugin is MinPending 
-#  --------------------------------------------------------------------
-
-# sched.minpending.minimum = minimum number of pilots to be pending
-#              
-
-#  --------------------------------------------------------------------
-#       Configuration when schedplugin is MaxToRun
-#  --------------------------------------------------------------------
-
-# sched.maxtorun.maximum = maximum number of pilots allowed to, potentially, 
-#               be running at a time. 
-#              
-
-#  --------------------------------------------------------------------
-#       Configuration when schedplugin is StatusTest
-#  --------------------------------------------------------------------
-
-# sched.statustest.pilots = number of pilots to submit
-#               when the wmsqueue is in status = test
-
-#  --------------------------------------------------------------------
-#       Configuration when schedplugin is StatusOffline
-#  --------------------------------------------------------------------
-
-# sched.statusoffline.pilots = number of pilots to submit
-#               when the wmsqueue or the cloud is in status = offline
-
-#  --------------------------------------------------------------------
-#       Configuration when schedplugin is Simple
-#  --------------------------------------------------------------------
-
-# sched.simple.default = default number of pilots to be submitted
-#               when the context information does not exist
-#               or is not reliable.
-#               To be used in Simple Scheduler Plugin.
-#
-# sched.simple.maxpendingpilots = maximum number of pilots
-#               to be idle on queue waiting to start execution.
-#               To be used in Simple Scheduler Plugin.
-#
-# sched.simple.maxpilotspercycle = maximum number of pilots
-#               to be submitted per cycle.
-#               To be used in Simple Scheduler Plugin.
-
-#  --------------------------------------------------------------------
-#       Configuration when schedplugin is Trivial
-#  --------------------------------------------------------------------
-
-# sched.trivial.default = default number of pilots
-#               to be submitted when the context information
-#               does not exist or is not reliable.
-#               To be used in Trivial Scheduler Plugin.
-
-#  --------------------------------------------------------------------
-#       Configuration when schedplugin is Scale 
-#  --------------------------------------------------------------------
-
-# sched.scale.factor = scale factor to correct the previous value
-#               of the number of pilots.
-#               Value is a float number.
-
-
-#  --------------------------------------------------------------------
-#       Configuration when schedplugin is KeepNRunning
-#  --------------------------------------------------------------------
-
-# sched.keepnrunning.keep_running = number of total jobs to keep 
-#               running and/or pending
-
-#  --------------------------------------------------------------------
-#       Batch Submit Plugin variables 
-#  --------------------------------------------------------------------
-
-# batchsubmitplugin = Batch Submit Plugin.
-#               Currently available options are: 
-#                    CondorGT2, 
-#                    CondorGT5, 
-#                    CondorCREAM, 
-#                    CondorLocal, 
-#                    CondorLSF,
-#                    CondorEC2, 
-#                    CondorDeltaCloud.
-
-#  --------------------------------------------------------------------
-#       Configuration when batchsubmitplugin is condorgt2
-#  --------------------------------------------------------------------
-
-# batchsubmit.condorgt2.gridresource = name of the CE (e.g. gridtest01.racf.bnl.gov/jobmanager-condor)
-#
-# batchsubmit.condorgt2.submitargs = list of command line input options
-#               to be included in the submission command *verbatim*
-#               e.g. 
-#                   batchsubmit.condorgt2.submitargs = -remote my_schedd 
-#               will drive into a command like
-#                   condor_submit -remote my_schedd submit.jdl
-#
-# batchsubmit.condorgt2.condor_attributes = list of condor attributes, 
-#               splited by comma, 
-#               to be included in the condor submit file *verbatim*
-#               e.g. +Experiment = "ATLAS",+VO = "usatlas",+Job_Type = "cas"
-#               Can be used to include any line in the Condor-G file
-#               that is not otherwise added programmatically by AutoPyFactory.
-#               Note the following directives are added by default:
-#
-#                       transfer_executable = True
-#                       stream_output=False
-#                       stream_error=False
-#                       notification=Error
-#                       copy_to_spool = false
-#
-# batchsubmit.condorgt2.environ = list of environment variables, 
-#               splitted by white spaces, 
-#               to be included in the condor attribute environment *verbatim*
-#               Therefore, the format should be env1=var1 env2=var2 envN=varN
-#               split by whitespaces.
-#
-# batchsubmit.condorgt2.proxy = name of the proxy handler in proxymanager for automatic proxy renewal 
-#               (See etc/proxy.conf)
-#               None if no automatic proxy renewal is desired.
-#
-
-#  --------------------------------------------------------------------
-#       GlobusRSL GRAM2 variables
-#  --------------------------------------------------------------------
-
-# gram2 =       The following are GRAM2 RSL variables. 
-#               They are just used to build batchsubmit.condorgt2.globusrsl 
-#               (if needed)
-#               The globusrsl directive in the condor submission file looks like
-#
-#                   globusrsl=(jobtype=single)(queue=short)
-#
-#               Documentation can be found here:
-#
-#                       http://www.globus.org/toolkit/docs/2.4/gram/gram_rsl_parameters.html
-#
-# globusrsl.gram2.arguments = 
-# globusrsl.gram2.count = 
-# globusrsl.gram2.directory =
-# globusrsl.gram2.dryRun =
-# globusrsl.gram2.environment =
-# globusrsl.gram2.executable =
-# globusrsl.gram2.gramMyJob =
-# globusrsl.gram2.hostCount =
-# globusrsl.gram2.jobType =
-# globusrsl.gram2.maxCpuTime =
-# globusrsl.gram2.maxMemory =
-# globusrsl.gram2.maxTime =
-# globusrsl.gram2.maxWallTime =
-# globusrsl.gram2.minMemory =
-# globusrsl.gram2.project =
-# globusrsl.gram2.queue =
-# globusrsl.gram2.remote_io_url =
-# globusrsl.gram2.restart =
-# globusrsl.gram2.save_state =
-# globusrsl.gram2.stderr =
-# globusrsl.gram2.stderr_position =
-# globusrsl.gram2.stdin =
-# globusrsl.gram2.stdout =
-# globusrsl.gram2.stdout_position =
-# globusrsl.gram2.two_phase =
-#
-# globusrsl.gram2.globusrsl = GRAM RSL directive.
-#               If this variable is not setup, then it will be built
-#               programmatically from all non empty globusrsl.gram2.XYZ variables.
-#               If this variable is setup, then its value
-#               will be taken *verbatim*, and all possible values
-#               for globusrsl.gram2.XYZ variables will be ignored. 
-#
-# globusrsl.gram2.globusrsladd = custom fields to be added
-#               *verbatim* to the GRAM RSL directive,
-#               after it has been built either from 
-#               globusrsl.gram2.globusrsl value
-#               or from all globusrsl.gram2.XYZ variables.
-#               e.g. (condorsubmit=('+AccountingGroup' '\"group_atlastest.usatlas1\"')('+Requirements' 'True'))
-
-
-#  --------------------------------------------------------------------
-#       Configuration when batchsubmitplugin is condorgt5
-#  --------------------------------------------------------------------
-
-# batchsubmit.condorgt5.gridresource = name of the CE (e.g. gridtest01.racf.bnl.gov/jobmanager-condor)
-#
-# batchsubmit.condorgt5.submitargs = list of command line input options
-#               to be included in the submission command *verbatim*
-#               e.g. 
-#                   batchsubmit.condorgt2.submitargs = -remote my_schedd 
-#               will drive into a command like
-#                   condor_submit -remote my_schedd submit.jdl
-#
-# batchsubmit.condorgt5.condor_attributes = list of condor attributes, 
-#               splited by comma, 
-#               to be included in the condor submit file *verbatim*
-#               e.g. +Experiment = "ATLAS",+VO = "usatlas",+Job_Type = "cas"
-#               Can be used to include any line in the Condor-G file
-#               that is not otherwise added programmatically by AutoPyFactory.
-#               Note the following directives are added by default:
-#
-#                       transfer_executable = True
-#                       stream_output=False
-#                       stream_error=False
-#                       notification=Error
-#                       copy_to_spool = false
-#
-# batchsubmit.condorgt5.environ = list of environment variables, 
-#               splitted by white spaces, 
-#               to be included in the condor attribute environment *verbatim*
-#               Therefore, the format should be env1=var1 env2=var2 envN=varN
-#               split by whitespaces.
-#
-# batchsubmit.condorgt5.proxy = name of the proxy handler in proxymanager for automatic proxy renewal 
-#               (See etc/proxy.conf)
-#               None if no automatic proxy renewal is desired.
-
-#  --------------------------------------------------------------------
-#       GlobusRSL GRAM5 variables
-#  --------------------------------------------------------------------
-
-# gram5 = The following are GRAM5 RSL variables. 
-#               They are just used to build batchsubmit.condorgt5.globusrsl 
-#               (if needed)
-#               The globusrsl directive in the condor submission file looks like
-#
-#                   globusrsl=(jobtype=single)(queue=short)
-#
-#               Documentation can be found here:
-#
-#                      http://www.globus.org/toolkit/docs/5.2/5.2.0/gram5/user/#gram5-user-rsl 
-#
-# globusrsl.gram5.arguments =
-# globusrsl.gram5.count =
-# globusrsl.gram5.directory =
-# globusrsl.gram5.dry_run =
-# globusrsl.gram5.environment =
-# globusrsl.gram5.executable =
-# globusrsl.gram5.file_clean_up =
-# globusrsl.gram5.file_stage_in =
-# globusrsl.gram5.file_stage_in_shared =
-# globusrsl.gram5.file_stage_out =
-# globusrsl.gram5.gass_cache =
-# globusrsl.gram5.gram_my_job =
-# globusrsl.gram5.host_count =
-# globusrsl.gram5.job_type =
-# globusrsl.gram5.library_path =
-# globusrsl.gram5.loglevel =
-# globusrsl.gram5.logpattern =
-# globusrsl.gram5.max_cpu_time =
-# globusrsl.gram5.max_memory =
-# globusrsl.gram5.max_time =
-# globusrsl.gram5.max_wall_time =
-# globusrsl.gram5.min_memory =
-# globusrsl.gram5.project =
-# globusrsl.gram5.proxy_timeout =
-# globusrsl.gram5.queue =
-# globusrsl.gram5.remote_io_url =
-# globusrsl.gram5.restart =
-# globusrsl.gram5.rsl_substitution =
-# globusrsl.gram5.savejobdescription =
-# globusrsl.gram5.save_state =
-# globusrsl.gram5.scratch_dir =
-# globusrsl.gram5.stderr =
-# globusrsl.gram5.stderr_position =
-# globusrsl.gram5.stdin =
-# globusrsl.gram5.stdout =
-# globusrsl.gram5.stdout_position =
-# globusrsl.gram5.two_phase =
-# globusrsl.gram5.username =
-#
-# globusrsl.gram5.globusrsl = GRAM RSL directive.
-#               If this variable is not setup, then it will be built
-#               programmatically from all non empty globusrsl.gram5.XYZ variables.
-#               If this variable is setup, then its value
-#               will be taken *verbatim*, and all possible values
-#               for globusrsl.gram5.XYZ variables will be ignored. 
-#
-# globusrsl.gram5.globusrsladd = custom fields to be added
-#               *verbatim* to the GRAM RSL directive,
-#               after it has been built either from 
-#               globusrsl.gram5.globusrsl value
-#               or from all globusrsl.gram5.XYZ variables.
-#               e.g. (condorsubmit=('+AccountingGroup' '\"group_atlastest.usatlas1\"')('+Requirements' 'True'))
-#
-
-#  --------------------------------------------------------------------
-#       Configuration when batchsubmitplugin is condorcream
-#  --------------------------------------------------------------------
-
-# batchsubmit.condorcream.webservice = web service address (e.g. ce04.esc.qmul.ac.uk:8443/ce-cream/services/CREAM2)
-#
-# batchsubmit.condorcream.submitargs = list of command line input options
-#               to be included in the submission command *verbatim*
-#               e.g. 
-#                   batchsubmit.condorgt2.submitargs = -remote my_schedd 
-#               will drive into a command like
-#                   condor_submit -remote my_schedd submit.jdl
-#
-# batchsubmit.condorcream.condor_attributes = list of condor attributes, 
-#               splited by comma, 
-#               to be included in the condor submit file *verbatim*
-#               e.g. +Experiment = "ATLAS",+VO = "usatlas",+Job_Type = "cas"
-#               Can be used to include any line in the Condor-G file
-#               that is not otherwise added programmatically by AutoPyFactory.
-#               Note the following directives are added by default:
-#
-#                       transfer_executable = True
-#                       stream_output=False
-#                       stream_error=False
-#                       notification=Error
-#                       copy_to_spool = false
-#
-# batchsubmit.condorcream.environ = list of environment variables, 
-#               splitted by white spaces, 
-#               to be included in the condor attribute environment *verbatim*
-#               Therefore, the format should be env1=var1 env2=var2 envN=varN
-#               split by whitespaces.
-#
-# batchsubmit.condorcream.queue = queue within the local batch system (e.g. short)
-#
-# batchsubmit.condorcream.port = port number.
-#
-# batchsubmit.condorcream.batch = local batch system (pbs, sge...)
-#
-# batchsubmit.condorcream.gridresource = grid resource, built from other vars using interpolation:
-#               batchsubmit.condorcream.gridresource = %(batchsubmit.condorcream.webservice)s:%(batchsubmit.condorcream.port)s/ce-cream/services/CREAM2 %(batchsubmit.condorcream.batch)s %(batchsubmit.condorcream.queue)s
-#
-# batchsubmit.condorcream.proxy = name of the proxy handler in proxymanager for automatic proxy renewal 
-#               (See etc/proxy.conf)
-#               None if no automatic proxy renewal is desired.
-
-#  --------------------------------------------------------------------
-#       Configuration when batchsubmitplugin is condorosgce
-#  --------------------------------------------------------------------
-
-# batchsubmit.condorosgce.remote_condor_schedd = condor schedd 
-#
-# batchsubmit.condorosgce.remote_condor_collector =  condor collector
-#
-# batchsubmit.condorosgce.gridresource = grid resource, built from other vars using interpolation
-#               batchsubmit.condorosgce.gridresource = %(batchsubmit.condorosgce.remote_condor_schedd) %(batchsubmit.condorosgce.remote_condor_collector)    
-# batchsubmit.condorosgce.proxy = name of the proxy handler in proxymanager for automatic proxy renewal 
-#               (See etc/proxy.conf)
-#               None if no automatic proxy renewal is desired.
-
-#  --------------------------------------------------------------------
-#       Configuration when batchsubmitplugin is condorec2
-#  --------------------------------------------------------------------
-
-# batchsubmit.condorec2.gridresource = ec2 service's URL (e.g. https://ec2.amazonaws.com/ )
-#
-# batchsubmit.condorec2.submitargs = list of command line input options
-#               to be included in the submission command *verbatim*
-#               e.g. 
-#                   batchsubmit.condorgt2.submitargs = -remote my_schedd 
-#               will drive into a command like
-#                   condor_submit -remote my_schedd submit.jdl
-#
-# batchsubmit.condorec2.condor_attributes = list of condor attributes, 
-#               splited by comma, 
-#               to be included in the condor submit file *verbatim*
-#
-# batchsubmit.condorec2.environ = list of environment variables, 
-#               splitted by white spaces, 
-#               to be included in the condor attribute environment *verbatim*
-#               Therefore, the format should be env1=var1 env2=var2 envN=varN
-#               split by whitespaces.
-#
-# batchsubmit.condorec2.ami_id = identifier for the VM image, 
-#               previously registered in one of Amazon's storage service (S3 or EBS)
-#
-# batchsubmit.condorec2.ec2_spot_price = max price to pay, in dollars to three decimal places. e.g. .040
-#
-# batchsubmit.condorec2.instance_type = hardware configurations for instances to run on, .e.g m1.medium
-#
-# batchsubmit.condorec2.user_data = up to 16Kbytes of contextualization data.
-#               This makes it easy for many instances to share the same VM image, but perform different work.
-#
-# batchsubmit.condorec2.access_key_id = path to file with the EC2 Access Key ID
-#
-# batchsubmit.condorec2.secret_access_key = path to file with the EC2 Secret Access Key
-#
-# batchsubmit.condorec2.proxy = name of the proxy handler in proxymanager for automatic proxy renewal 
-#               (See etc/proxy.conf)
-#               None if no automatic proxy renewal is desired.
-
-#  --------------------------------------------------------------------
-#       Configuration when batchsubmitplugin is condordeltacloud
-#  --------------------------------------------------------------------
-
-# batchsubmit.condordeltacloud.gridresource = ec2 service's URL (e.g. https://deltacloud.foo.org/api )
-#
-# batchsubmit.condordeltacloud.username = credentials in DeltaCloud
-#
-# batchsubmit.condordeltacloud.password_file = path to the file with the password
-#
-# batchsubmit.condordeltacloud.image_id = identifier for the VM image,
-#               previously registered with the cloud service.
-#
-# batchsubmit.condordeltacloud.keyname = in case of using SSH, 
-#               the command keyname specifies the identifier of the SSH key pair to use. 
-#
-# batchsubmit.condordeltacloud.realm_id = selects one between multiple locations the cloud service may have.
-#
-# batchsubmit.condordeltacloud.hardware_profile = selects one between the multiple hardware profiles
-#               the cloud service may provide
-#
-# batchsubmit.condordeltacloud.hardware_profile_memory = customize the hardware profile
-#
-# batchsubmit.condordeltacloud.hardware_profile_cpu = customize the hardware profile
-#
-# batchsubmit.condordeltacloud.hardware_profile_storage = customize the hardware profile
-#
-# batchsubmit.condordeltacloud.user_data = contextualization data
-
-#  --------------------------------------------------------------------
-#       Configuration when batchsubmitplugin is condorlocal
-#  --------------------------------------------------------------------
-
-# batchsubmit.condorlocal.submitargs = list of command line input options
-#               to be included in the submission command *verbatim*
-#               e.g. 
-#                   batchsubmit.condorgt2.submitargs = -remote my_schedd 
-#               will drive into a command like
-#                   condor_submit -remote my_schedd submit.jdl
-#
-# batchsubmit.condorlocal.condor_attributes = list of condor attributes, 
-#               splited by comma, 
-#               to be included in the condor submit file *verbatim*
-#               e.g. +Experiment = "ATLAS",+VO = "usatlas",+Job_Type = "cas"
-#               Can be used to include any line in the Condor-G file
-#               that is not otherwise added programmatically by AutoPyFactory.
-#               Note the following directives are added by default:
-#
-#                       universe = vanilla
-#                       transfer_executable = True
-#                       should_transfer_files = IF_NEEDED
-#                       +TransferOutput = ""
-#                       stream_output=False
-#                       stream_error=False
-#                       notification=Error
-#                       periodic_remove = (JobStatus == 5 && (CurrentTime - EnteredCurrentStatus) > 3600) || (JobStatus == 1 && globusstatus =!= 1 && (CurrentTime - EnteredCurrentStatus) > 86400)
-#
-#               To be used in CondorLocal Batch Submit Plugin.
-#
-# batchsubmit.condorlocal.environ = list of environment variables, 
-#               splitted by white spaces, 
-#               to be included in the condor attribute environment *verbatim*
-#               To be used by CondorLocal Batch Submit Plugin.
-#               Therefore, the format should be env1=var1 env2=var2 envN=varN
-#               split by whitespaces.
-#
-# batchsubmit.condorlocal.proxy = name of the proxy handler in proxymanager for automatic proxy renewal 
-#               (See etc/proxy.conf)
-#               None if no automatic proxy renewal is desired.
-#
-
-
-#  --------------------------------------------------------------------
-#       Configuration when batchsubmitplugin is condorlsf
-#  --------------------------------------------------------------------
-
-# batchsubmit.condorlsf.proxy = name of the proxy handler in proxymanager for automatic proxy renewal 
-#               (See etc/proxy.conf)
-#               None if no automatic proxy renewal is desired.
-
-#  --------------------------------------------------------------------
-#       Configuration when batchsubmitplugin is nordugrid 
-#  --------------------------------------------------------------------
-
-# batchsubmit.condornordugrid.gridresource = name of the ARC CE
-#               i.e. lcg-lrz-ce2.grid.lrz.de
-#
-# nordugridrsl = Entire RSL line.
-#               i.e. (jobname = 'prod_pilot')(queue=lcg)(runtimeenvironment = APPS/HEP/ATLAS-SITE-LCG)(runtimeenvironment = ENV/PROXY ) (environment = ('APFFID' 'voatlas94') ('PANDA_JSID' 'voatlas94') ('GTAG' 'http://voatlas94.cern.ch/pilots/2012-11-19/LRZ-LMU_arc/$(Cluster).$(Process).out') ('RUCIO_ACCOUNT' 'pilot') ('APFCID' '$(Cluster).$(Process)') ('APFMON' 'http://apfmon.lancs.ac.uk/mon/') ('FACTORYQUEUE' 'LRZ-LMU_arc') 
-#
-# nordugridrsladd = A given tag to be added to the Nordugrid RSL line
-#
-# nordugridrsl.addenv.<XYZ> = A given tag to be added within the 'environment' tag to the Nordugrid RSL line
-#               i.e. nordugridrsl.addenv.RUCIO_ACCOUNT = pilot
-#                    will be added as ('RUCIO_ACCOUNT' 'pilot' )
-
-#  --------------------------------------------------------------------
-#       Monitor Section 
-#  --------------------------------------------------------------------
-
-# monitorsection = section in monitor.conf where info 
-#               about the actual monitor plugin can be found.
-#               The value can be a single section or a split by comma
-#               list of sections.
-#               Monitor plugins handle job info publishing 
-#               to one or more web monitor/dashboards. 
-#               To specify more than one (sections) 
-#               simply use a comma-separated list.   
-#
-
-#  --------------------------------------------------------------------
-#       Executable variables 
-#  --------------------------------------------------------------------
-
-# executable = path to the script which will be run by condor. 
-#               The executable can be anything, however, 
-#               two possible executables are distributed with AutoPyFactory:
-#
-#                       - libexec/wrapper.sh 
-#                       - libexec/runpilot3-wrapper.sh 
-#
-# executable.arguments = input options to be passed verbatim to the executable script.
-#               This variable can be built making use of an auxiliar variable
-#               called executable.defaultarguments
-#               This proposed ancilla works as a template, and its content is
-#               created on the fly from the value of other variables.
-#               This mechanism is called "interpolation", docs can be found here:
-#
-#                   http://docs.python.org/library/configparser.html
-#
-#               These are two examples of this type of templates 
-#               (included in the DEFAULTS block):
-#
-#                   executable.defaultarguments = --wrappergrid=%(grid)s \
-#                               --wrapperwmsqueue=%(wmsqueue)s \
-#                               --wrapperbatchqueue=%(batchqueue)s \
-#                               --wrappervo=%(vo)s \
-#                               --wrappertarballurl=http://dev.racf.bnl.gov/dist/wrapper/wrapper.tar.gz \
-#                               --wrapperserverurl=http://pandaserver.cern.ch:25080/cache/pilot \
-#                               --wrapperloglevel=debug
-# 
-#                   executable.defaultarguments =  -s %(wmsqueue)s \
-#                               -h %(batchqueue)s -p 25443 \
-#                               -w https://pandaserver.cern.ch  -j false  -k 0  -u user
-#
-# =========================================================================== 
-
-
-[DEFAULT]
-
-grid = OSG
-vo = ATLAS
-cloud = US
-country = US
-group = None
-status = online
-
-cleanlogs.keepdays = 7
-
-# plugins
-batchstatusplugin = Condor
-wmsstatusplugin = Panda
-batchsubmitplugin = CondorGT2
-schedplugin = Ready 
-monitorsection = apfmon-lancaster
-
-
-
-sched.trivial.default = 0
-sched.simple.default = 0
-sched.activated.default = 0
-# defaults for testmode
-sched.activated.testmode.allowed = True
-sched.activated.testmode.pilots = 5
-
-# proxy = atlas-usatlas
-batchsubmit.condorgt2.proxy = None
-batchsubmit.condorgt5.proxy = None
-batchsubmit.condorcream.proxy = None
-batchsubmit.condorec2.proxy = None
-batchsubmit.condorlocal.proxy = None
-
-# gram and globusrsl
-#   jobtype and queue are given a default value.
-
-globusrsl.gram2.arguments = ""
-globusrsl.gram2.count = ""
-globusrsl.gram2.directory = ""
-globusrsl.gram2.dryRun = ""
-globusrsl.gram2.environment = ""
-globusrsl.gram2.executable = ""
-globusrsl.gram2.gramMyJob = ""
-globusrsl.gram2.hostCount = ""
-globusrsl.gram2.jobType = single
-globusrsl.gram2.maxCpuTime = ""
-globusrsl.gram2.maxMemory = ""  
-globusrsl.gram2.maxTime = ""
-globusrsl.gram2.maxWallTime = ""
-globusrsl.gram2.minMemory = "" 
-globusrsl.gram2.project = ""
-globusrsl.gram2.queue = short
-globusrsl.gram2.remote_io_url = ""
-globusrsl.gram2.restart = ""
-globusrsl.gram2.save_state = ""
-globusrsl.gram2.stderr = ""
-globusrsl.gram2.stderr_position = ""
-globusrsl.gram2.stdin = ""
-globusrsl.gram2.stdout = ""
-globusrsl.gram2.stdout_position = ""
-globusrsl.gram2.two_phase = ""
-
-globusrsl.gram5.arguments = ""
-globusrsl.gram5.count = ""
-globusrsl.gram5.directory = ""
-globusrsl.gram5.dry_run = ""
-globusrsl.gram5.environment = ""
-globusrsl.gram5.executable = ""
-globusrsl.gram5.file_clean_up = ""
-globusrsl.gram5.file_stage_in = ""
-globusrsl.gram5.file_stage_in_shared = ""
-globusrsl.gram5.file_stage_out = ""
-globusrsl.gram5.gass_cache = ""
-globusrsl.gram5.gram_my_job = ""
-globusrsl.gram5.host_count = ""
-globusrsl.gram5.job_type = single
-globusrsl.gram5.library_path = ""
-globusrsl.gram5.loglevel = ""
-globusrsl.gram5.logpattern = ""
-globusrsl.gram5.max_cpu_time = ""
-globusrsl.gram5.max_memory = ""
-globusrsl.gram5.max_time = ""
-globusrsl.gram5.max_wall_time = ""
-globusrsl.gram5.min_memory = ""
-globusrsl.gram5.project = ""
-globusrsl.gram5.proxy_timeout = ""
-globusrsl.gram5.queue = single
-globusrsl.gram5.remote_io_url = ""
-globusrsl.gram5.restart = ""
-globusrsl.gram5.rsl_substitution = ""
-globusrsl.gram5.savejobdescription = ""
-globusrsl.gram5.save_state = ""
-globusrsl.gram5.scratch_dir = ""
-globusrsl.gram5.stderr = ""
-globusrsl.gram5.stderr_position = ""
-globusrsl.gram5.stdin = ""
-globusrsl.gram5.stdout = ""
-globusrsl.gram5.stdout_position = ""
-globusrsl.gram5.two_phase = ""
-globusrsl.gram5.username = ""
-
-
-periodic_hold = periodic_hold=GlobusResourceUnavailableTime =!= UNDEFINED &&(CurrentTime-GlobusResourceUnavailableTime>30)
-periodic_remove = periodic_remove=(JobStatus == 5 && (CurrentTime - EnteredCurrentStatus) > 3600) || (JobStatus == 1 && globusstatus =!= 1 && (CurrentTime - EnteredCurrentStatus) > 86400) || (JobStatus == 2 && (CurrentTime - EnteredCurrentStatus) > 604800)
-batchsubmit.condorgt2.condor_attributes = %(periodic_hold)s,%(periodic_remove)s 
-
-apfqueue.sleep = 360
-
-# The following are valid for wrapper.sh
-executable = /usr/libexec/wrapper.sh
-executable.defaultarguments = --wrappergrid=%(grid)s --wrapperwmsqueue=%(wmsqueue)s --wrapperbatchqueue=%(batchqueue)s --wrappervo=%(vo)s --wrappertarballurl=http://dev.racf.bnl.gov/dist/wrapper/wrapper.tar.gz --wrapperserverurl=http://pandaserver.cern.ch:25080/cache/pilot --wrapperloglevel=debug
-
-# The following are valid for runpilot3.sh
-#executable = /usr/libexec/runpilot3.sh
-#executable.defaultarguments =  -s %(wmsqueue)s -h %(batchqueue)s -p 25443 -w https://pandaserver.cern.ch  -j false  -k 0  -u user
-
-enabled = True
-
-
-# ====================================================================== 
-#               Individual queue configurations
-# ====================================================================== 
-
-# ---------------------------------------------------------------------- 
-#               Local Condor example
-# ---------------------------------------------------------------------- 
-
-[ANALY_BNL_CLOUD-sl6]
-enabled = False
-
-wmsqueue = ANALY_BNL_CLOUD
-batchqueue = ANALY_BNL_CLOUD
-
-batchstatusplugin = Condor
-batchsubmit = CondorLocal
-
-batchsubmit.condorlocal.condor_attributes =  Requirements = ( Arch == "X86_64" && OpSysAndVer == "SL6" && NodeType == "atlas" ) ,request_memory = 1699 ,+AccountingGroup = "group_analy.apf"
-batchsubmit.condorlocal.proxy = atlas-production
-
-schedplugin = Ready, MinPerCycle, MaxPerCycle, MaxPending
-sched.minpercycle.minimum = 10
-sched.maxpercycle.maximum = 20
-sched.maxpending.maximum = 50
-
-# These arguments are passed through to the payload job (e.g., the Panda pilot). 
-executable.arguments = %(executable.defaultarguments)s --script=pilot.py --libcode=pilotcode.tar.gz,pilotcode-rc.tar.gz --pilotsrcurl=http://panda.cern.ch:25880/cache --user user
-
-
-
-# ---------------------------------------------------------------------- 
-#               GRAM example
-# ---------------------------------------------------------------------- 
-
-[ANALY_BNL_ATLAS_1]
-enabled = False
-
-wmsqueue = ANALY_BNL_ATLAS_1
-batchqueue = ANALY_BNL_ATLAS_1-condor
-
-batchsubmit.condorgt2.gridresource = gridgk05.racf.bnl.gov/jobmanager-condor
-batchsubmit.condorgt2.queue = short
-batchsubmit.condorgt2.condor_attributes = periodic_remove = (JobStatus == 5 && (CurrentTime - EnteredCurrentStatus) > 3600) || (JobStatus == 1 && globusstatus =!= 1 && (CurrentTime - EnteredCurrentStatus) > 86400)
-batchsubmit.condorgt2.proxy = atlas-production
-
-schedplugin = Ready, MinPerCycle, MaxPerCycle, MaxPending
-sched.minpercycle.minimum = 10
-sched.maxpercycle.maximum = 20
-sched.maxpending.maximum = 50
-
-# These arguments are passed through to the payload job (e.g., the Panda pilot). 
-executable.arguments = %(executable.defaultarguments)s --script=pilot.py --libcode=pilotcode.tar.gz,pilotcode-rc.tar.gz --pilotsrcurl=http://panda.cern.ch:25880/cache --user user
-
-# If using runpilot3.sh, you only need the defaultarguments, as it seems the payload args are not necessary. 
-# executable.arguments = %(executable.defaultarguments)s
-
-
-# ---------------------------------------------------------------------- 
-#               CREAM example
-# ---------------------------------------------------------------------- 
-
-[RAL-LCG2-lcgce04-grid3000M-pbs-3379]
-enabled = False
-
-batchqueue = RAL-LCG2-lcgce04-grid3000M-pbs
-wmsqueue = RAL-LCG2
-
-batchsubmitplugin = CondorCREAM
-
-batchsubmit.condorcream.webservice = lcgce04.gridpp.rl.ac.uk
-batchsubmit.condorcream.port = 8443
-batchsubmit.condorcream.batch = pbs
-batchsubmit.condorcream.queue = grid3000M 
-batchsubmit.condorcream.condor_attributes = periodic_remove = (JobStatus == 5 && (CurrentTime - EnteredCurrentStatus) > 3600) || (JobStatus == 1 && globusstatus =!= 1 && (CurrentTime - EnteredCurrentStatus) > 86400)
-batchsubmit.condorcream.proxy = atlas-production
-
-schedplugin = Ready, MinPerCycle, MaxPerCycle, MaxPending
-sched.minpercycle.minimum = 10
-sched.maxpercycle.maximum = 20
-sched.maxpending.maximum = 50
-
-# These arguments are passed through to the payload job (e.g., the Panda pilot). 
-executable.arguments = %(executable.defaultarguments)s --script=pilot.py --libcode=pilotcode.tar.gz,pilotcode-rc.tar.gz --pilotsrcurl=http://panda.cern.ch:25880/cache --user user
-
-# If using runpilot3.sh, you only need the defaultarguments, as it seems the payload args are not necessary. 
-# executable.arguments = %(executable.defaultarguments)s
-
-
-# ---------------------------------------------------------------------- 
-#               Nordugrid example
-# ---------------------------------------------------------------------- 
-
-[ANALY_LRZ]
-enabled = False
-
-wmsqueue = ANALY_LRZ 
-batchqueue = ANALY_LRZ
-
-batchsubmitplugin = CondorNordugrid
-batchsubmit.condornordugrid.gridresource = lcg-lrz-ce2.grid.lrz.de 
-nordugridrsl.jobname = 'analy_pilot'
-nordugridrsl.queue = lcg
-nordugridrsl.nordugridrsladd = (runtimeenvironment = APPS/HEP/ATLAS-SITE-LCG)(runtimeenvironment = ENV/PROXY )
-nordugridrsl.addenv.RUCIO_ACCOUNT = pilot
-
-schedplugin = Ready, MinPerCycle, MaxPerCycle, MaxPending
-sched.minpercycle.minimum = 10
-sched.maxpercycle.maximum = 20
-sched.maxpending.maximum = 50
-
-executable.arguments = %(executable.defaultarguments)s --script=pilot.py --libcode=pilotcode.tar.gz,pilotcode-rc.tar.gz --pilotsrcurl=http://panda.cern.ch:25880/cache --user user
diff --git a/etc/sysconfig/factory b/etc/sysconfig/factory
new file mode 100644
index 0000000..0a45448
--- /dev/null
+++ b/etc/sysconfig/factory
@@ -0,0 +1,31 @@
+#
+# Sysconfig file for autopyfactory
+#
+
+#  OPTIONS:
+# 
+#  --trace              Set logging level to TRACE [default WARNING], super verbose level
+#  --debug              Set logging level to DEBUG [default WARNING]
+#  --info               Set logging level to INFO [default WARNING]
+#  --quiet              Set logging level to WARNING [default WARNING]
+#  --conf=FILE1[,FILE2,FILE3]
+#                        Load configuration from FILEs (comma separated list)
+#  --runas=ACCOUNT       User account to run as. [apf]
+#  --sleep=TIME          Time to sleep between cycles. [60] 
+#  --log=LOGFILE         Send logging output to LOGFILE or SYSLOG or stdout
+#                        [default <syslog>]
+
+#
+# Override default conffile path if desired. 
+#
+# CONFFILE=/etc/apf/factory.conf
+# CONFFILE="/usatlas/us/caballer/etc/factory.conf"
+
+OPTIONS="--debug --sleep=60 --runas=apf --log=/var/log/apf/apf.log"
+CONSOLE_LOG=/var/log/apf/console.log
+
+#
+# Use this to provide voms-proxy-* on the path if it is not pre-setup by
+# default. 
+#
+# GRIDSETUP=/opt/osg-client/setup.sh
diff --git a/etc/sysconfig/proxymanager b/etc/sysconfig/proxymanager
new file mode 100644
index 0000000..a9d73d7
--- /dev/null
+++ b/etc/sysconfig/proxymanager
@@ -0,0 +1,28 @@
+#
+# Sysconfig file for proxymanager
+#
+
+#  OPTIONS:
+
+#  --debug              Set logging level to DEBUG [default WARNING]
+#  --info               Set logging level to INFO [default WARNING]
+#  --config=FILE1[,FILE2,FILE3]
+#                        Load configuration from FILEs (comma separated list)
+#  --runas=ACCOUNT       User account to run as. [apf]
+#  --log=LOGFILE         Send logging output to LOGFILE or SYSLOG or stdout
+#                        [default <syslog>]
+
+#
+# Override default conffile path if desired. 
+#
+# CONFFILE=/etc/apf/factory.conf
+# CONFFILE="/usatlas/us/caballer/etc/factory.conf"
+
+OPTIONS="--debug --log=/var/log/apf/proxymanager.log"
+CONSOLE_LOG=/var/log/apf/console.log
+
+#
+# Use this to provide voms-proxy-* on the path if it is not pre-setup by
+# default. 
+#
+# GRIDSETUP=/opt/osg-client/setup.sh
diff --git a/setup.py b/setup.py
index 3f2a203..4bd1c4b 100644
--- a/setup.py
+++ b/setup.py
@@ -30,20 +30,22 @@ if major == 2:
 
 libexec_files = ['libexec/%s' %file for file in os.listdir('libexec') if os.path.isfile('libexec/%s' %file)]
 
-etc_files = ['etc/factory.conf-example',
-             'etc/queues.conf-example',
-             'etc/proxy.conf-example',
-             'etc/monitor.conf-example',
-             'etc/factory.sysconfig-example',
-             'etc/proxymanager.sysconfig-example',
-             'etc/logsmonitor.rotate.conf-example',
+etc_files = ['etc/factory.conf',
+             'etc/queues.conf',
+             'etc/proxy.conf',
+             'etc/monitor.conf',
+             'etc/logsmonitor.rotate.conf',
              'etc/apf-search-failed.sh-example',
              ]
 
 initd_files = ['etc/factory',
                'etc/proxymanager']
 
-logrotate_files = ['etc/factory.logrotate',]
+logrotate_files = ['etc/logrotate/factory',]
+
+sysconfig_files = ['etc/sysconfig/factory',
+                   'etc/sysconfig/proxymanager',
+                  ]
 
 # docs files:
 #   --everything in the docs/ directory
@@ -51,6 +53,13 @@ logrotate_files = ['etc/factory.logrotate',]
 docs_files = ['docs/%s' %file for file in os.listdir('docs') if os.path.isfile('docs/%s' %file)]
 docs_files.append('RELEASE_NOTES')
 
+man1_files = ['docs/man/autopyfactory.1']
+man5_files = ['docs/man/autopyfactory-factory.conf.5',
+              'docs/man/autopyfactory-monitor.conf.5',
+              'docs/man/autopyfactory-proxy.conf.5',
+              'docs/man/autopyfactory-queues.conf.5',
+             ]
+
 # NOTE: these utils are going to be distributed from now on 
 #       in a separated RPM
 #
@@ -65,19 +74,25 @@ docs_files.append('RELEASE_NOTES')
 
 # -----------------------------------------------------------
 
-rpm_data_files=[('/etc/apf',           libexec_files),
-                ('/etc/apf',           etc_files),
-                ('/etc/init.d',        initd_files),
-                ('/etc/logrotate.d',   logrotate_files),                                        
-                ('/usr/share/doc/apf', docs_files),                                        
-                #('/usr/share/apf',     utils_files),                                        
+rpm_data_files=[('/etc/apf',            libexec_files),
+                ('/etc/apf',            etc_files),
+                ('/etc/rc.d/init.d',    initd_files),
+                ('/etc/logrotate.d',    logrotate_files),
+                ('/etc/sysconfig',      sysconfig_files),
+                ('/usr/share/doc/apf',  docs_files),
+                ('/usr/share/man/man1', man1_files),
+                ('/usr/share/man/man5', man5_files),
+                #('/usr/share/apf',     utils_files),
                ]
 
-home_data_files=[('etc',       libexec_files),
-                 ('etc',       etc_files),
-                 ('etc',       initd_files),
-                 ('doc/apf',   docs_files ),
-                 #('share/apf', utils_files),                                        
+home_data_files=[('etc',           libexec_files),
+                 ('etc',           etc_files),
+                 ('etc/init.d',    initd_files),
+                 ('etc/sysconfig', sysconfig_files),
+                 ('doc/apf',       docs_files),
+                 ('man/man1',      man1_files),
+                 ('man/man5',      man5_files),
+                 #('share/apf',    utils_files),
                 ]
 
 # -----------------------------------------------------------

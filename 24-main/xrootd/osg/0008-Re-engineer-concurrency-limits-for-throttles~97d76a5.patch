From 069922fc15cd2a511d0e0f9649872e3c056dd5f8 Mon Sep 17 00:00:00 2001
From: Brian Bockelman <bbockelman@morgridge.org>
Date: Tue, 29 Apr 2025 13:03:49 -0500
Subject: [PATCH] Re-engineer concurrency limits for throttles

Previously, the concurrency limits for the throttle was not fair: once they
were hit, all requests were put into the same queue, meaning a single badly-
behaved user could mess up the experience for all users.

Now, we track the per-user recent concurrency and use it to determine how
close to "fair" the load is between all the active users.  If the user has
few active transfers compared to others, they will receive more "shares",
increasing the probability the next randomly-selected user to wake is them.

The concurrency tracking was modernized to remove the use of sequentially
consistent counters (which weren't needed) and switches to acquire/release
semantics instead.  This removes the use of fencing instructions on x86,
meaning the overhead of the throttle tracking is significantly lower than
before under high concurrency.

The list of ongoing operations (used to determine the concurrency) is broken
up across several lists to avoid a single global mutex protecting the operations.

The concurrency itself is calculated using exponential moving averages with
a decay time of 10s.
---
 src/XrdThrottle/XrdThrottle.hh                |   3 +
 src/XrdThrottle/XrdThrottleFile.cc            |   8 +-
 .../XrdThrottleFileSystemConfig.cc            |  29 +-
 src/XrdThrottle/XrdThrottleManager.cc         | 312 ++++++++++++++----
 src/XrdThrottle/XrdThrottleManager.hh         | 228 ++++++++++---
 5 files changed, 461 insertions(+), 119 deletions(-)

diff --git a/src/XrdThrottle/XrdThrottle.hh b/src/XrdThrottle/XrdThrottle.hh
index f00363374..e5cb3acd9 100644
--- a/src/XrdThrottle/XrdThrottle.hh
+++ b/src/XrdThrottle/XrdThrottle.hh
@@ -286,6 +286,9 @@ friend XrdSfsFileSystem * XrdSfsGetFileSystem_Internal(XrdSfsFileSystem *, XrdSy
    int
    xmaxconn(XrdOucStream &Config);
 
+   int
+   xmaxwait(XrdOucStream &Config);
+
    static FileSystem  *m_instance;
    XrdSysError         m_eroute;
    XrdOucTrace         m_trace;
diff --git a/src/XrdThrottle/XrdThrottleFile.cc b/src/XrdThrottle/XrdThrottleFile.cc
index 5667a50f2..16a7672b5 100644
--- a/src/XrdThrottle/XrdThrottleFile.cc
+++ b/src/XrdThrottle/XrdThrottleFile.cc
@@ -20,7 +20,13 @@ using namespace XrdThrottle;
 #define DO_THROTTLE(amount) \
 DO_LOADSHED \
 m_throttle.Apply(amount, 1, m_uid); \
-XrdThrottleTimer xtimer = m_throttle.StartIOTimer();
+bool ok; \
+auto xtimer = m_throttle.StartIOTimer(m_uid, ok); \
+if (!ok) { \
+   error.setErrInfo(EMFILE, "I/O limit exceeded and wait time hit"); \
+   return SFS_ERROR; \
+}
+
 
 File::File(const char                     *user,
                  unique_sfs_ptr            sfs,
diff --git a/src/XrdThrottle/XrdThrottleFileSystemConfig.cc b/src/XrdThrottle/XrdThrottleFileSystemConfig.cc
index 29be98037..7cdca577b 100644
--- a/src/XrdThrottle/XrdThrottleFileSystemConfig.cc
+++ b/src/XrdThrottle/XrdThrottleFileSystemConfig.cc
@@ -163,6 +163,7 @@ FileSystem::Configure(XrdSysError & log, XrdSfsFileSystem *native_fs, XrdOucEnv
       TS_Xeq("throttle.max_active_connections", xmaxconn);
       TS_Xeq("throttle.throttle", xthrottle);
       TS_Xeq("throttle.loadshed", xloadshed);
+      TS_Xeq("throttle.max_wait_time", xmaxwait);
       TS_Xeq("throttle.trace", xtrace);
       if (NoGo)
       {
@@ -234,14 +235,40 @@ FileSystem::xmaxconn(XrdOucStream &Config)
 {
     auto val = Config.GetWord();
     if (!val || val[0] == '\0')
-       {m_eroute.Emsg("Config", "Max active cconnections not specified!  Example usage: throttle.max_active_connections 4000");}
+       {m_eroute.Emsg("Config", "Max active connections not specified!  Example usage: throttle.max_active_connections 4000");}
     long long max_conn = -1;
     if (XrdOuca2x::a2sz(m_eroute, "max active connections value", val, &max_conn, 1)) return 1;
 
     m_throttle.SetMaxConns(max_conn);
     return 0;
 }
 
+/******************************************************************************/
+/*                            x m a x w a i t                                 */
+/******************************************************************************/
+
+/* Function: xmaxwait
+
+   Purpose:  Parse the directive: throttle.max_wait_time <limit>
+
+             <limit>   maximum wait time, in seconds, before an operation should fail
+
+   If the directive is not provided, the default is 30 seconds.
+
+  Output: 0 upon success or !0 upon failure.
+*/
+int
+FileSystem::xmaxwait(XrdOucStream &Config)
+{
+    auto val = Config.GetWord();
+    if (!val || val[0] == '\0')
+       {m_eroute.Emsg("Config", "Max waiting time not specified (must be in seconds)!  Example usage: throttle.max_wait_time 20");}
+    long long max_wait = -1;
+    if (XrdOuca2x::a2sz(m_eroute, "max waiting time value", val, &max_wait, 1)) return 1;
+
+    m_throttle.SetMaxWait(max_wait);
+    return 0;
+}
 
 /******************************************************************************/
 /*                            x t h r o t t l e                               */
diff --git a/src/XrdThrottle/XrdThrottleManager.cc b/src/XrdThrottle/XrdThrottleManager.cc
index d1daec7cc..a025aca6a 100644
--- a/src/XrdThrottle/XrdThrottleManager.cc
+++ b/src/XrdThrottle/XrdThrottleManager.cc
@@ -10,59 +10,69 @@
 #define XRD_TRACE m_trace->
 #include "XrdThrottle/XrdThrottleTrace.hh"
 
+#include <algorithm>
+#include <cmath>
+#include <random>
 #include <sstream>
 
-const char *
-XrdThrottleManager::TraceID = "ThrottleManager";
+#if defined(__linux__)
 
-const
-int XrdThrottleManager::m_max_users = 1024;
+#include <sched.h>
+unsigned XrdThrottleManager::GetTimerListHash() {
+    int cpu = sched_getcpu();
+    if (cpu < 0) {
+        return 0;
+    }
+    return cpu % m_timer_list_size;
+}
 
-#if defined(__linux__) || defined(__APPLE__) || defined(__GNU__) || (defined(__FreeBSD_kernel__) && defined(__GLIBC__))
-clockid_t XrdThrottleTimer::clock_id = CLOCK_MONOTONIC;
 #else
-int XrdThrottleTimer::clock_id = 0;
+
+unsigned XrdThrottleManager::GetTimerListHash() {
+    return 0;
+}
+
 #endif
 
+const char *
+XrdThrottleManager::TraceID = "ThrottleManager";
+
 XrdThrottleManager::XrdThrottleManager(XrdSysError *lP, XrdOucTrace *tP) :
    m_trace(tP),
    m_log(lP),
    m_interval_length_seconds(1.0),
    m_bytes_per_second(-1),
    m_ops_per_second(-1),
    m_concurrency_limit(-1),
    m_last_round_allocation(100*1024),
-   m_io_active(0),
    m_loadshed_host(""),
    m_loadshed_port(0),
-   m_loadshed_frequency(0),
-   m_loadshed_limit_hit(0)
+   m_loadshed_frequency(0)
 {
-   m_stable_io_wait.tv_sec = 0;
-   m_stable_io_wait.tv_nsec = 0;
 }
 
 void
 XrdThrottleManager::Init()
 {
    TRACE(DEBUG, "Initializing the throttle manager.");
    // Initialize all our shares to zero.
    m_primary_bytes_shares.resize(m_max_users);
    m_secondary_bytes_shares.resize(m_max_users);
    m_primary_ops_shares.resize(m_max_users);
    m_secondary_ops_shares.resize(m_max_users);
+   for (auto & waiter : m_waiter_info) {
+      waiter.m_manager = this;
+   }
+
    // Allocate each user 100KB and 10 ops to bootstrap;
    for (int i=0; i<m_max_users; i++)
    {
       m_primary_bytes_shares[i] = m_last_round_allocation;
       m_secondary_bytes_shares[i] = 0;
       m_primary_ops_shares[i] = 10;
       m_secondary_ops_shares[i] = 0;
    }
 
-   m_io_wait.tv_sec = 0;
-   m_io_wait.tv_nsec = 0;
-
    int rc;
    pthread_t tid;
    if ((rc = XrdSysThread::Run(&tid, XrdThrottleManager::RecomputeBootstrap, static_cast<void *>(this), 0, "Buffer Manager throttle")))
@@ -289,14 +299,155 @@ XrdThrottleManager::Apply(int reqsize, int reqops, int uid)
          if (reqsize) TRACE(BANDWIDTH, "Sleeping to wait for throttle fairshare.");
          if (reqops) TRACE(IOPS, "Sleeping to wait for throttle fairshare.");
          m_compute_var.Wait();
-         AtomicBeg(m_compute_var);
-         AtomicInc(m_loadshed_limit_hit);
-         AtomicEnd(m_compute_var);
+         m_loadshed_limit_hit++;
       }
    }
 
 }
 
+void
+XrdThrottleManager::UserIOAccounting()
+{
+    std::chrono::steady_clock::duration::rep total_active_time = 0;
+    for (size_t idx = 0; idx < m_timer_list.size(); idx++) {
+        auto &timerList = m_timer_list[idx];
+        std::unique_lock<std::mutex> lock(timerList.m_mutex);
+        auto timer = timerList.m_first;
+        while (timer) {
+            auto next = timer->m_next;
+            auto uid = timer->m_owner;
+            auto &waiter = m_waiter_info[uid];
+            auto recent_duration = timer->Reset();
+            waiter.m_io_time += recent_duration.count();
+
+            total_active_time += recent_duration.count();
+            timer = next;
+        }
+    }
+    m_io_active_time += total_active_time;
+}
+
+void
+XrdThrottleManager::ComputeWaiterOrder()
+{
+    // Update the IO time for long-running I/O operations.  This prevents,
+    // for example, a 2-minute I/O operation from causing a spike in
+    // concurrency because it's otherwise only reported at the end.
+    UserIOAccounting();
+
+    auto now = std::chrono::steady_clock::now();
+    auto elapsed = now - m_last_waiter_recompute_time;
+    m_last_waiter_recompute_time = now;
+    std::chrono::duration<double> elapsed_secs = elapsed;
+    // Alpha is the decay factor for the exponential moving average.  One window is 10 seconds,
+    // so every 10 seconds we decay the prior average by 1/e (that is, the weight is 64% of the
+    // total).  This means the contribution of I/O load from a minute ago is 0.2% of the total.
+
+    // The moving average will be used to determine how close the user is to their "fair share"
+    // of the concurrency limit among the users that are waiting.
+    auto alpha = 1 - std::exp(-1 * elapsed_secs.count() / 10.0);
+
+    std::vector<double> share;
+    share.resize(m_max_users);
+    size_t users_with_waiters = 0;
+    // For each user, compute their current concurrency and determine how many waiting users
+    // total there are.
+    for (int i = 0; i < m_max_users; i++)
+    {
+        auto &waiter = m_waiter_info[i];
+        auto io_duration_rep = waiter.m_io_time.exchange(std::chrono::steady_clock::duration(0).count());
+        std::chrono::steady_clock::duration io_duration = std::chrono::steady_clock::duration(io_duration_rep);
+        std::chrono::duration<double> io_duration_secs = io_duration;
+        auto recent_concurrency = io_duration_secs.count() / elapsed_secs.count();
+
+        auto new_concurrency = (1 - alpha) * waiter.m_concurrency + alpha * recent_concurrency;
+        waiter.m_concurrency = new_concurrency;
+        if (new_concurrency > 0) {
+            TRACE(DEBUG, "User " << i << " has concurrency of " << waiter.m_concurrency);
+        }
+        unsigned waiting;
+        {
+            std::lock_guard<std::mutex> lock(waiter.m_mutex);
+            waiting = waiter.m_waiting;
+        }
+        if (waiting > 0)
+        {
+            share[i] = new_concurrency;
+            TRACE(DEBUG, "User " << i << " has concurrency of " << share[i] << " and is waiting for " << waiting);
+            // Handle the division-by-zero case; if we have no history of usage whatsoever, we should pretend we
+            // have at least some minimal load
+            if (share[i] == 0) {
+                share[i] = 0.1;
+            }
+            users_with_waiters++;
+        }
+        else
+        {
+            share[i] = 0;
+        }
+    }
+    auto fair_share = static_cast<double>(m_concurrency_limit) / static_cast<double>(users_with_waiters);
+    std::vector<uint16_t> waiter_order;
+    waiter_order.resize(m_max_users);
+
+    // Calculate the share for each user.  We assume the user should get a share proportional to how
+    // far above or below the fair share they are.  So, a user with concurrency of 20 when the fairshare
+    // is 10 will get 0.5 shares; a user with concurrency of 5 when the fairshare is 10 will get 2.0 shares.
+    double shares_sum = 0;
+    for (int idx = 0; idx < m_max_users; idx++)
+    {
+        if (share[idx]) {
+            shares_sum += fair_share / share[idx];
+        }
+    }
+
+    // We must quantize the overall shares into an array of 1024 elements.  We do this by
+    // scaling up (or down) based on the total number of shares computed above.  Note this
+    // quantization can lead to an over-provisioned user being assigned zero shares; thus,
+    // we scale based on (1024-#users) so we can give one extra share to each user.
+    auto scale_factor = (static_cast<double>(m_max_users) - static_cast<double>(users_with_waiters)) / shares_sum;
+    size_t offset = 0;
+    for (int uid = 0; uid < m_max_users; uid++) {
+        if (share[uid] > 0) {
+            auto shares = static_cast<unsigned>(scale_factor * fair_share / share[uid]) + 1;
+            TRACE(DEBUG, "User " << uid << " has " << shares << " shares");
+            for (unsigned idx = 0; idx < shares; idx++)
+            {
+                waiter_order[offset % m_max_users] = uid;
+                offset++;
+            }
+        }
+    }
+    if (offset < m_max_users) {
+        for (size_t idx = offset; idx < m_max_users; idx++) {
+            waiter_order[idx] = -1;
+        }
+    }
+    // Shuffle the order to randomize the wakeup order.
+    std::shuffle(waiter_order.begin(), waiter_order.end(), std::default_random_engine());
+
+    // Copy the order to the inactive array.  We do not shuffle in-place because RAtomics are
+    // not move constructible, which is a requirement for std::shuffle.
+    auto &waiter_order_to_modify = (m_wake_order_active == 0) ? m_wake_order_1 : m_wake_order_0;
+    std::copy(waiter_order.begin(), waiter_order.end(), waiter_order_to_modify.begin());
+
+    // Set the array we just modified to be the active one.  Since this is a relaxed write, it could take
+    // some time for other CPUs to see the change; that's OK as this is all stochastic anyway.
+    m_wake_order_active = (m_wake_order_active + 1) % 2;
+
+    m_waiter_offset = 0;
+
+    // If we find ourselves below the concurrency limit because we woke up too few operations in the last
+    // interval, try waking up enough operations to fill the gap.  If we race with new incoming operations,
+    // the threads will just go back to sleep.
+    if (users_with_waiters) {
+        auto io_active = m_io_active.load(std::memory_order_acquire);
+        for (size_t idx = io_active; idx < m_concurrency_limit; idx++) {
+            NotifyOne();
+        }
+    }
+}
+
 void *
 XrdThrottleManager::RecomputeBootstrap(void *instance)
 {
@@ -353,6 +504,7 @@ XrdThrottleManager::Recompute()
 
       TRACE(DEBUG, "Recomputing fairshares for throttle.");
       RecomputeInternal();
+      ComputeWaiterOrder();
       TRACE(DEBUG, "Finished recomputing fairshares for throttle; sleeping for " << m_interval_length_seconds << " seconds.");
       XrdSysTimer::Wait(static_cast<int>(1000*m_interval_length_seconds));
    }
@@ -421,40 +573,31 @@ XrdThrottleManager::RecomputeInternal()
       m_primary_ops_shares[i] = ops_shares;
    }
 
+   AtomicEnd(m_compute_var);
+
    // Reset the loadshed limit counter.
-   int limit_hit = AtomicFAZ(m_loadshed_limit_hit);
+   int limit_hit = m_loadshed_limit_hit.exchange(0);
    TRACE(DEBUG, "Throttle limit hit " << limit_hit << " times during last interval.");
 
-   AtomicEnd(m_compute_var);
-
    // Update the IO counters
    m_compute_var.Lock();
-   m_stable_io_active = AtomicGet(m_io_active);
+   m_stable_io_active = m_io_active.load(std::memory_order_acquire);
    auto io_active = m_stable_io_active;
-   m_stable_io_total = static_cast<unsigned>(AtomicGet(m_io_total));
+   m_stable_io_total = m_io_total;
    auto io_total = m_stable_io_total;
-   time_t secs; AtomicFZAP(secs, m_io_wait.tv_sec);
-   long nsecs; AtomicFZAP(nsecs, m_io_wait.tv_nsec);
-   m_stable_io_wait.tv_sec += static_cast<long>(secs * intervals_per_second);
-   m_stable_io_wait.tv_nsec += static_cast<long>(nsecs * intervals_per_second);
-   while (m_stable_io_wait.tv_nsec > 1000000000)
-   {
-      m_stable_io_wait.tv_nsec -= 1000000000;
-      m_stable_io_wait.tv_sec ++;
-   }
-   struct timespec io_wait_ts;
-   io_wait_ts.tv_sec = m_stable_io_wait.tv_sec;
-   io_wait_ts.tv_nsec = m_stable_io_wait.tv_nsec;
+   auto io_wait_rep = m_io_active_time.exchange(std::chrono::steady_clock::duration(0).count());
+   m_stable_io_wait += std::chrono::steady_clock::duration(io_wait_rep);
 
    m_compute_var.UnLock();
-   uint64_t io_wait_ms = io_wait_ts.tv_sec*1000+io_wait_ts.tv_nsec/1000000;
-   TRACE(IOLOAD, "Current IO counter is " << io_active << "; total IO wait time is " << io_wait_ms << "ms.");
+
+   auto io_wait_ms = std::chrono::duration_cast<std::chrono::milliseconds>(m_stable_io_wait).count();
+   TRACE(IOLOAD, "Current IO counter is " << io_active << "; total IO active time is " << io_wait_ms << "ms.");
    if (m_gstream)
    {
         char buf[128];
         auto len = snprintf(buf, 128,
-                            R"({"event":"throttle_update","io_wait":%.4f,"io_active":%d,"io_total":%d})",
-                            static_cast<double>(io_wait_ms) / 1000.0, io_active, io_total);
+                            R"({"event":"throttle_update","io_wait":%.4f,"io_active":%d,"io_total":%llu})",
+                            static_cast<double>(io_wait_ms) / 1000.0, io_active, static_cast<long long unsigned>(io_total));
         auto suc = (len < 128) ? m_gstream->Insert(buf, len + 1) : false;
         if (!suc)
         {
@@ -482,42 +625,70 @@ XrdThrottleManager::GetUid(const char *username)
    return hval;
 }
 
+/*
+ * Notify a single waiter thread that it can proceed.
+ */
+void
+XrdThrottleManager::NotifyOne()
+{
+    auto &wake_order = (m_wake_order_active == 0) ? m_wake_order_0 : m_wake_order_1;
+
+    for (size_t idx = 0; idx < m_max_users; ++idx)
+    {
+        auto offset = m_waiter_offset.fetch_add(1, std::memory_order_acq_rel);
+        int16_t uid = wake_order[offset % m_max_users];
+        if (uid < 0)
+        {
+            continue;
+        }
+        auto &waiter_info = m_waiter_info[uid];
+        std::unique_lock<std::mutex> lock(waiter_info.m_mutex);
+        if (waiter_info.m_waiting) {
+            waiter_info.NotifyOne(std::move(lock));
+            return;
+        }
+   }
+}
+
 /*
  * Create an IO timer object; increment the number of outstanding IOs.
  */
 XrdThrottleTimer
-XrdThrottleManager::StartIOTimer()
+XrdThrottleManager::StartIOTimer(uint16_t uid, bool &ok)
 {
-   AtomicBeg(m_compute_var);
-   int cur_counter = AtomicInc(m_io_active);
-   AtomicInc(m_io_total);
-   AtomicEnd(m_compute_var);
-   while (m_concurrency_limit >= 0 && cur_counter > m_concurrency_limit)
+   int cur_counter = m_io_active.fetch_add(1, std::memory_order_acq_rel);
+   m_io_total++;
+
+   while (m_concurrency_limit >= 0 && cur_counter >= m_concurrency_limit)
    {
-      AtomicBeg(m_compute_var);
-      AtomicInc(m_loadshed_limit_hit);
-      AtomicDec(m_io_active);
-      AtomicEnd(m_compute_var);
-      m_compute_var.Wait();
-      AtomicBeg(m_compute_var);
-      cur_counter = AtomicInc(m_io_active);
-      AtomicEnd(m_compute_var);
+      m_loadshed_limit_hit++;
+      m_io_active.fetch_sub(1, std::memory_order_acq_rel);
+      TRACE(DEBUG, "ThrottleManager (user=" << uid << "): IO concurrency limit hit; waiting for other IOs to finish.");
+      ok = m_waiter_info[uid].Wait();
+      if (!ok) {
+        TRACE(DEBUG, "ThrottleManager (user=" << uid << "): timed out waiting for other IOs to finish.");
+        return XrdThrottleTimer();
+      }
+      cur_counter = m_io_active.fetch_add(1, std::memory_order_acq_rel);
    }
-   return XrdThrottleTimer(*this);
+
+   ok = true;
+   return XrdThrottleTimer(this, uid);
 }
 
 /*
  * Finish recording an IO timer.
  */
 void
-XrdThrottleManager::StopIOTimer(struct timespec timer)
+XrdThrottleManager::StopIOTimer(std::chrono::steady_clock::duration & event_duration, uint16_t uid)
 {
-   AtomicBeg(m_compute_var);
-   AtomicDec(m_io_active);
-   AtomicAdd(m_io_wait.tv_sec, timer.tv_sec);
-   // Note this may result in tv_nsec > 1e9
-   AtomicAdd(m_io_wait.tv_nsec, timer.tv_nsec);
-   AtomicEnd(m_compute_var);
+   m_io_active_time += event_duration.count();
+   auto old_active = m_io_active.fetch_sub(1, std::memory_order_acq_rel);
+   m_waiter_info[uid].m_io_time += event_duration.count();
+   if (old_active == m_concurrency_limit)
+   {
+      NotifyOne();
+   }
 }
 
 /*
@@ -534,7 +705,7 @@ XrdThrottleManager::CheckLoadShed(const std::string &opaque)
    {
       return false;
    }
-   if (AtomicGet(m_loadshed_limit_hit) == 0)
+   if (m_loadshed_limit_hit == 0)
    {
       return false;
    }
@@ -581,3 +752,20 @@ XrdThrottleManager::PerformLoadShed(const std::string &opaque, std::string &host
    host += opaque;
    port = m_loadshed_port;
 }
+
+bool
+XrdThrottleManager::Waiter::Wait()
+{
+    auto timeout = std::chrono::steady_clock::now() + m_manager->m_max_wait_time;
+    {
+        std::unique_lock<std::mutex> lock(m_mutex);
+        m_waiting++;
+        m_cv.wait_until(lock, timeout,
+                        [&] { return m_manager->m_io_active.load(std::memory_order_acquire) < m_manager->m_concurrency_limit || std::chrono::steady_clock::now() >= timeout; });
+        m_waiting--;
+    }
+    if (std::chrono::steady_clock::now() > timeout) {
+        return false;
+    }
+    return true;
+}
diff --git a/src/XrdThrottle/XrdThrottleManager.hh b/src/XrdThrottle/XrdThrottleManager.hh
index 8d43a4123..85871a5ae 100644
--- a/src/XrdThrottle/XrdThrottleManager.hh
+++ b/src/XrdThrottle/XrdThrottleManager.hh
@@ -35,6 +35,7 @@
 #include <unordered_map>
 #include <memory>
 
+#include "XrdSys/XrdSysRAtomic.hh"
 #include "XrdSys/XrdSysPthread.hh"
 
 class XrdSysError;
@@ -69,87 +70,187 @@ void        SetMaxOpen(unsigned long max_open) {m_max_open = max_open;}
 
 void        SetMaxConns(unsigned long max_conns) {m_max_conns = max_conns;}
 
+void        SetMaxWait(unsigned long max_wait) {m_max_wait_time = std::chrono::seconds(max_wait);}
+
 void        SetMonitor(XrdXrootdGStream *gstream) {m_gstream = gstream;}
 
 //int         Stats(char *buff, int blen, int do_sync=0) {return m_pool.Stats(buff, blen, do_sync);}
 
 static
 int         GetUid(const char *username);
 
-XrdThrottleTimer StartIOTimer();
+// Notify that an I/O operation has started for a given user.
+//
+// If we are at the maximum concurrency limit then this will block;
+// if we block for too long, the second return value will return false.
+XrdThrottleTimer StartIOTimer(uint16_t uid, bool &ok);
 
 void        PrepLoadShed(const char *opaque, std::string &lsOpaque);
 
 bool        CheckLoadShed(const std::string &opaque);
 
 void        PerformLoadShed(const std::string &opaque, std::string &host, unsigned &port);
 
             XrdThrottleManager(XrdSysError *lP, XrdOucTrace *tP);
 
            ~XrdThrottleManager() {} // The buffmanager is never deleted
 
 protected:
 
-void        StopIOTimer(struct timespec);
+// Notify the manager an I/O operation has completed for a given user.
+// This is used to update the I/O wait time for the user and, potentially,
+// wake up a waiting thread.
+void        StopIOTimer(std::chrono::steady_clock::duration & event_duration, uint16_t uid);
 
 private:
 
 void        Recompute();
 
 void        RecomputeInternal();
 
 static
 void *      RecomputeBootstrap(void *pp);
 
+// Compute the order of wakeups for the existing waiters.
+void ComputeWaiterOrder();
+
+// Walk through the outstanding IO operations and compute the per-user
+// IO time.
+//
+// Meant to be done periodically as part of the Recompute interval.  Used
+// to make sure we have a better estimate of the concurrency for each user.
+void UserIOAccounting();
+
 int         WaitForShares();
 
 void        GetShares(int &shares, int &request);
 
 void        StealShares(int uid, int &reqsize, int &reqops);
 
+// Return the timer hash list ID to use for the current request.
+//
+// When on Linux, this will hash across the CPU ID; the goal is to distribute
+// the different timers across several lists to avoid mutex contention.
+static unsigned GetTimerListHash();
+
+// Notify a single waiter thread that it can proceed.
+void NotifyOne();
+
 XrdOucTrace * m_trace;
 XrdSysError * m_log;
 
 XrdSysCondVar m_compute_var;
 
 // Controls for the various rates.
 float       m_interval_length_seconds;
 float       m_bytes_per_second;
 float       m_ops_per_second;
 int         m_concurrency_limit;
 
 // Maintain the shares
-static const
-int         m_max_users;
+
+static constexpr int m_max_users = 1024; // Maximum number of users we can have; used for various fixed-size arrays.
 std::vector<int> m_primary_bytes_shares;
 std::vector<int> m_secondary_bytes_shares;
 std::vector<int> m_primary_ops_shares;
 std::vector<int> m_secondary_ops_shares;
 int         m_last_round_allocation;
 
-// Active IO counter
-int         m_io_active;
-struct timespec m_io_wait;
-unsigned    m_io_total{0};
-// Stable IO counters - must hold m_compute_var lock when reading/writing;
-int m_stable_io_active;
-int m_stable_io_total{0}; // It would take ~3 years to overflow a 32-bit unsigned integer at 100Hz of IO operations.
-struct timespec m_stable_io_wait;
+// Waiter counts for each user
+struct alignas(64) Waiter
+{
+   std::condition_variable m_cv; // Condition variable for waiters of this user.
+   std::mutex m_mutex; // Mutex for this structure
+   unsigned m_waiting{0}; // Number of waiting operations for this user.
+
+   // EWMA of the concurrency for this user.  This is used to determine how much
+   // above / below the user's concurrency share they've been recently.  This subsequently
+   // will affect the likelihood of being woken up.
+   float m_concurrency{0};
+
+   // I/O time for this user since the last recompute interval.  The value is used
+   // to compute the EWMA of the concurrency (m_concurrency).
+   XrdSys::RAtomic<std::chrono::steady_clock::duration::rep> m_io_time{0};
+
+   // Pointer to the XrdThrottleManager object that owns this waiter.
+   XrdThrottleManager *m_manager{nullptr};
+
+   // Causes the current thread to wait until it's the user's turn to wake up.
+   bool Wait();
+
+   // Wakes up one I/O operation for this user.
+   void NotifyOne(std::unique_lock<std::mutex> lock)
+   {
+      m_cv.notify_one();
+   }
+};
+std::array<Waiter, m_max_users> m_waiter_info;
+
+// Array with the wake up ordering of the waiter users.
+// Every recompute interval, we compute how much over the concurrency limit
+// each user is, quantize this to an integer number of shares and then set the
+// array value to the user ID (so if user ID 5 has two shares, then there are two
+// entries with value 5 in the array).  The array is then shuffled to randomize the
+// order of the wakeup.
+//
+// All reads and writes to the wake order array are meant to be relaxed atomics; if a thread
+// has an outdated view of the array, it simply means that a given user might get slightly
+// incorrect random probability of being woken up.  That's seen as acceptable to keep
+// the selection algorithm lock and fence-free.
+std::array<XrdSys::RAtomic<int16_t>, m_max_users> m_wake_order_0;
+std::array<XrdSys::RAtomic<int16_t>, m_max_users> m_wake_order_1; // A second wake order array; every recompute interval, we will swap the active array, avoiding locks.
+XrdSys::RAtomic<char> m_wake_order_active; // The current active wake order array; 0 or 1
+std::atomic<size_t> m_waiter_offset{0}; // Offset inside the wake order array; this is used to wake up the next potential user in line.  Cannot be relaxed atomic as offsets need to be seen in order.
+std::chrono::steady_clock::time_point m_last_waiter_recompute_time; // Last time we recomputed the wait ordering.
+
+std::atomic<uint32_t> m_io_active; // Count of in-progress IO operations: cannot be a relaxed atomic as ordering of inc/dec matters.
+XrdSys::RAtomic<std::chrono::steady_clock::duration::rep> m_io_active_time; // Total IO wait time recorded since the last recompute interval; reset to zero about every second.
+XrdSys::RAtomic<uint64_t> m_io_total{0}; // Monotonically increasing count of IO operations; reset to zero about every second.
+
+int m_stable_io_active{0}; // Number of IO operations in progress as of the last recompute interval; must hold m_compute_var lock when reading/writing.
+uint64_t m_stable_io_total{0}; // Total IO operations since startup.  Recomputed every second; must hold m_compute_var lock when reading/writing.
+
+std::chrono::steady_clock::duration m_stable_io_wait; // Total IO wait time as of the last recompute interval.
 
 // Load shed details
 std::string m_loadshed_host;
 unsigned m_loadshed_port;
 unsigned m_loadshed_frequency;
-int m_loadshed_limit_hit;
+
+// The number of times we have an I/O operation that hit the concurrency limit.
+// This is monotonically increasing and is "relaxed" because it's purely advisory;
+// ordering of the increments between threads is not important.
+XrdSys::RAtomic<int> m_loadshed_limit_hit;
 
 // Maximum number of open files
 unsigned long m_max_open{0};
 unsigned long m_max_conns{0};
 std::unordered_map<std::string, unsigned long> m_file_counters;
 std::unordered_map<std::string, unsigned long> m_conn_counters;
 std::unordered_map<std::string, std::unique_ptr<std::unordered_map<pid_t, unsigned long>>> m_active_conns;
 std::mutex m_file_mutex;
 
+// Track the ongoing I/O operations.  We have several linked lists (hashed on the
+// CPU ID) of I/O operations that are in progress.  This way, we can periodically sum
+// up the time spent in ongoing operations - which is important for operations that
+// last longer than the recompute interval.
+struct TimerList {
+   std::mutex m_mutex;
+   XrdThrottleTimer *m_first{nullptr};
+   XrdThrottleTimer *m_last{nullptr};
+};
+#if defined(__linux__)
+static constexpr size_t m_timer_list_size = 32;
+#else
+static constexpr size_t m_timer_list_size = 1;
+#endif
+std::array<TimerList, m_timer_list_size> m_timer_list; // A vector of linked lists of I/O operations.  We keep track of multiple instead of a single one to avoid a global mutex.
+
+// Maximum wait time for a user to perform an I/O operation before failing.
+// Most clients have some sort of operation timeout; after that point, if we go
+// ahead and do the work, it's wasted effort as the client has gone.
+std::chrono::steady_clock::duration m_max_wait_time{std::chrono::seconds(30)};
+
 // Monitoring handle, if configured
 XrdXrootdGStream* m_gstream{nullptr};
 
@@ -164,62 +265,79 @@ friend class XrdThrottleManager;
 
 public:
 
-void StopTimer()
-{
-   struct timespec end_timer = {0, 0};
-#if defined(__linux__) || defined(__APPLE__) || defined(__GNU__) || (defined(__FreeBSD_kernel__) && defined(__GLIBC__))
-   int retval = clock_gettime(clock_id, &end_timer);
-#else
-   int retval = -1;
-#endif
-   if (likely(retval == 0))
-   {
-      end_timer.tv_sec -= m_timer.tv_sec;
-      end_timer.tv_nsec -= m_timer.tv_nsec;
-      if (end_timer.tv_nsec < 0)
-      {
-         end_timer.tv_sec--;
-         end_timer.tv_nsec += 1000000000;
-      }
-   }
-   if (m_timer.tv_nsec != -1)
-   {
-      m_manager.StopIOTimer(end_timer);
-   }
-   m_timer.tv_sec = 0;
-   m_timer.tv_nsec = -1;
-}
-
 ~XrdThrottleTimer()
 {
-   if (!((m_timer.tv_sec == 0) && (m_timer.tv_nsec == -1)))
-   {
+   if (m_manager) {
       StopTimer();
    }
 }
 
 protected:
 
-XrdThrottleTimer(XrdThrottleManager & manager) :
-   m_manager(manager)
+XrdThrottleTimer() {}
+
+XrdThrottleTimer(XrdThrottleManager *manager, int uid) :
+   m_owner(uid),
+   m_timer_list_entry(XrdThrottleManager::GetTimerListHash()),
+   m_manager(manager),
+   m_start_time(std::chrono::steady_clock::now())
 {
-#if defined(__linux__) || defined(__APPLE__) || defined(__GNU__) || (defined(__FreeBSD_kernel__) && defined(__GLIBC__))
-   int retval = clock_gettime(clock_id, &m_timer);
-#else
-   int retval = -1;
-#endif
-   if (unlikely(retval == -1))
-   {
-      m_timer.tv_sec = 0;
-      m_timer.tv_nsec = 0;
+   if (!m_manager) {
+      return;
    }
+   auto &timerList = m_manager->m_timer_list[m_timer_list_entry];
+   std::lock_guard<std::mutex> lock(timerList.m_mutex);
+   if (timerList.m_first == nullptr) {
+      timerList.m_first = this;
+   } else {
+      m_prev = timerList.m_last;
+      m_prev->m_next = this;
+   }
+   timerList.m_last = this;
+}
+
+std::chrono::steady_clock::duration Reset() {
+   auto now = std::chrono::steady_clock::now();
+   auto last_start = m_start_time.exchange(now);
+   return now - last_start;
 }
 
 private:
-XrdThrottleManager &m_manager;
-struct timespec m_timer;
 
-static clockid_t clock_id;
+   void StopTimer()
+   {
+      if (!m_manager) return;
+
+      auto event_duration = Reset();
+      auto &timerList = m_manager->m_timer_list[m_timer_list_entry];
+      {
+         std::unique_lock<std::mutex> lock(timerList.m_mutex);
+         if (m_prev) {
+            m_prev->m_next = m_next;
+            if (m_next) {
+               m_next->m_prev = m_prev;
+            } else {
+               timerList.m_last = m_prev;
+            }
+         } else {
+            timerList.m_first = m_next;
+            if (m_next) {
+               m_next->m_prev = nullptr;
+            } else {
+               timerList.m_last = nullptr;
+            }
+         }
+      }
+      m_manager->StopIOTimer(event_duration, m_owner);
+   }
+
+   const uint16_t m_owner{0};
+   const uint16_t m_timer_list_entry{0};
+   XrdThrottleManager *m_manager{nullptr};
+   XrdThrottleTimer *m_prev{nullptr};
+   XrdThrottleTimer *m_next{nullptr};
+   XrdSys::RAtomic<std::chrono::steady_clock::time_point> m_start_time;
+
 };
 
 #endif
-- 
2.43.5

